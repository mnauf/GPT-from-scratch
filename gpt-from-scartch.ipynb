{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shakespeare Dataset","metadata":{}},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:12.442082Z","iopub.execute_input":"2023-07-02T07:53:12.442840Z","iopub.status.idle":"2023-07-02T07:53:24.630953Z","shell.execute_reply.started":"2023-07-02T07:53:12.442807Z","shell.execute_reply":"2023-07-02T07:53:24.629793Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting tiktoken\n  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.5.5)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.28.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\nInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport tiktoken\nimport numpy as np\nimport os\nfrom torch import nn","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:24.633244Z","iopub.execute_input":"2023-07-02T07:53:24.634125Z","iopub.status.idle":"2023-07-02T07:53:26.272862Z","shell.execute_reply.started":"2023-07-02T07:53:24.634089Z","shell.execute_reply":"2023-07-02T07:53:26.271876Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:26.274455Z","iopub.execute_input":"2023-07-02T07:53:26.275103Z","iopub.status.idle":"2023-07-02T07:53:26.308235Z","shell.execute_reply.started":"2023-07-02T07:53:26.275071Z","shell.execute_reply":"2023-07-02T07:53:26.306100Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:26.311632Z","iopub.execute_input":"2023-07-02T07:53:26.311996Z","iopub.status.idle":"2023-07-02T07:53:27.658547Z","shell.execute_reply.started":"2023-07-02T07:53:26.311964Z","shell.execute_reply":"2023-07-02T07:53:27.657408Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2023-07-02 07:53:27--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n\n2023-07-02 07:53:27 (46.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# read it in to inspect it\ndata_dir = \"/kaggle/working\"\nwith open(os.path.join(data_dir, 'input.txt'), 'r', encoding='utf-8') as f:\n    text = f.read()\ntrain_data = text[:int(len(text)*0.97)]\nval_data = text[int(len(text)*0.97):]\nprint(\"chars in train_data: \", len(train_data))\nprint(\"chars in val_data: \", len(val_data))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:27.660193Z","iopub.execute_input":"2023-07-02T07:53:27.660539Z","iopub.status.idle":"2023-07-02T07:53:27.672076Z","shell.execute_reply.started":"2023-07-02T07:53:27.660512Z","shell.execute_reply":"2023-07-02T07:53:27.671095Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"chars in train_data:  1081932\nchars in val_data:  33462\n","output_type":"stream"}]},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:27.673463Z","iopub.execute_input":"2023-07-02T07:53:27.674376Z","iopub.status.idle":"2023-07-02T07:53:27.698378Z","shell.execute_reply.started":"2023-07-02T07:53:27.674344Z","shell.execute_reply":"2023-07-02T07:53:27.697324Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:27.700240Z","iopub.execute_input":"2023-07-02T07:53:27.700579Z","iopub.status.idle":"2023-07-02T07:53:27.708098Z","shell.execute_reply.started":"2023-07-02T07:53:27.700549Z","shell.execute_reply":"2023-07-02T07:53:27.707083Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:100]) # the 1000 characters we looked at earier will to the GPT look like this","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:27.709785Z","iopub.execute_input":"2023-07-02T07:53:27.710179Z","iopub.status.idle":"2023-07-02T07:53:28.012145Z","shell.execute_reply.started":"2023-07-02T07:53:27.710150Z","shell.execute_reply":"2023-07-02T07:53:28.011099Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n","output_type":"stream"}]},{"cell_type":"code","source":"n = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.013487Z","iopub.execute_input":"2023-07-02T07:53:28.013853Z","iopub.status.idle":"2023-07-02T07:53:28.019108Z","shell.execute_reply.started":"2023-07-02T07:53:28.013822Z","shell.execute_reply":"2023-07-02T07:53:28.018045Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(\"tokens in train dataset: \", len(train_data))\nprint(\"tokens in validaiton dataset: \", len(val_data))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.024216Z","iopub.execute_input":"2023-07-02T07:53:28.025163Z","iopub.status.idle":"2023-07-02T07:53:28.030431Z","shell.execute_reply.started":"2023-07-02T07:53:28.025133Z","shell.execute_reply":"2023-07-02T07:53:28.029302Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tokens in train dataset:  1003854\ntokens in validaiton dataset:  111540\n","output_type":"stream"}]},{"cell_type":"code","source":"chars = sorted(list(set(text)))\nvocab_size = len(chars)\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.032317Z","iopub.execute_input":"2023-07-02T07:53:28.032654Z","iopub.status.idle":"2023-07-02T07:53:28.060090Z","shell.execute_reply.started":"2023-07-02T07:53:28.032625Z","shell.execute_reply":"2023-07-02T07:53:28.059180Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"65"},"metadata":{}}]},{"cell_type":"code","source":"train_data = np.array(train_data)\nval_data = np.array(val_data)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.061343Z","iopub.execute_input":"2023-07-02T07:53:28.061728Z","iopub.status.idle":"2023-07-02T07:53:28.074052Z","shell.execute_reply.started":"2023-07-02T07:53:28.061698Z","shell.execute_reply":"2023-07-02T07:53:28.073136Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Loader","metadata":{}},{"cell_type":"code","source":"B, T = 4, 8","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.075398Z","iopub.execute_input":"2023-07-02T07:53:28.075833Z","iopub.status.idle":"2023-07-02T07:53:28.081220Z","shell.execute_reply.started":"2023-07-02T07:53:28.075802Z","shell.execute_reply":"2023-07-02T07:53:28.080262Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"random_numbers = torch.randint(0, len(train_data) - T, (B,))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.082725Z","iopub.execute_input":"2023-07-02T07:53:28.083110Z","iopub.status.idle":"2023-07-02T07:53:28.095667Z","shell.execute_reply.started":"2023-07-02T07:53:28.083080Z","shell.execute_reply":"2023-07-02T07:53:28.094638Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"data = torch.stack([torch.from_numpy(train_data[random_number: random_number + T].astype(np.int64)) for random_number in random_numbers])\nlabels = torch.stack([torch.from_numpy(train_data[random_number + 1: random_number + T + 1].astype(np.int64)) for random_number in random_numbers])\ndata","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.097180Z","iopub.execute_input":"2023-07-02T07:53:28.097590Z","iopub.status.idle":"2023-07-02T07:53:28.123345Z","shell.execute_reply.started":"2023-07-02T07:53:28.097562Z","shell.execute_reply":"2023-07-02T07:53:28.122360Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([[42,  1, 57, 50, 43, 43, 54,  1],\n        [ 1, 50, 47, 43, 58, 46,  1, 47],\n        [54, 59, 50, 47, 52, 45,  1, 44],\n        [43, 56, 41, 43, 47, 60, 43,  1]])"},"metadata":{}}]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.124899Z","iopub.execute_input":"2023-07-02T07:53:28.125244Z","iopub.status.idle":"2023-07-02T07:53:28.131906Z","shell.execute_reply.started":"2023-07-02T07:53:28.125216Z","shell.execute_reply":"2023-07-02T07:53:28.130982Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor([[ 1, 57, 50, 43, 43, 54,  1, 39],\n        [50, 47, 43, 58, 46,  1, 47, 52],\n        [59, 50, 47, 52, 45,  1, 44, 53],\n        [56, 41, 43, 47, 60, 43,  1, 63]])"},"metadata":{}}]},{"cell_type":"code","source":"def get_batch(sequence_length, batch_size):\n    random_numbers = torch.randint(0, len(train_data) - sequence_length, (batch_size,))\n    data = torch.stack([torch.from_numpy(train_data[random_number: random_number + sequence_length].astype(np.int64)) for random_number in random_numbers])\n    labels = torch.stack([torch.from_numpy(train_data[random_number + 1: random_number + sequence_length + 1].astype(np.int64)) for random_number in random_numbers])\n    return data, labels\n","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.133615Z","iopub.execute_input":"2023-07-02T07:53:28.134486Z","iopub.status.idle":"2023-07-02T07:53:28.141357Z","shell.execute_reply.started":"2023-07-02T07:53:28.134455Z","shell.execute_reply":"2023-07-02T07:53:28.140322Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data, labels = get_batch(8, 4)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.144495Z","iopub.execute_input":"2023-07-02T07:53:28.145196Z","iopub.status.idle":"2023-07-02T07:53:28.151512Z","shell.execute_reply.started":"2023-07-02T07:53:28.145167Z","shell.execute_reply":"2023-07-02T07:53:28.150666Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data.shape, labels.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.153166Z","iopub.execute_input":"2023-07-02T07:53:28.153540Z","iopub.status.idle":"2023-07-02T07:53:28.162123Z","shell.execute_reply.started":"2023-07-02T07:53:28.153511Z","shell.execute_reply":"2023-07-02T07:53:28.161000Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(torch.Size([4, 8]), torch.Size([4, 8]))"},"metadata":{}}]},{"cell_type":"markdown","source":"# Bigram Language Model","metadata":{}},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.embedding = torch.nn.Embedding(vocab_size, vocab_size)\n    \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        logits = self.embedding(x)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.163325Z","iopub.execute_input":"2023-07-02T07:53:28.164052Z","iopub.status.idle":"2023-07-02T07:53:28.170768Z","shell.execute_reply.started":"2023-07-02T07:53:28.164004Z","shell.execute_reply":"2023-07-02T07:53:28.170001Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:28.171792Z","iopub.execute_input":"2023-07-02T07:53:28.172504Z","iopub.status.idle":"2023-07-02T07:53:30.914391Z","shell.execute_reply.started":"2023-07-02T07:53:28.172473Z","shell.execute_reply":"2023-07-02T07:53:30.913382Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\ndef train():\n    model.train()\n    for iteration in range(5000):\n        data, labels = get_batch(4, 8)\n        data, labels = data.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logits = model(data)\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        labels = labels.view(B*T)\n        loss = F.cross_entropy(logits, labels)\n        if iteration % 100 == 0:\n            print(\"Iteration: \", iteration, \"loss: \", loss.item())\n        loss.backward()\n        optimizer.step()\n\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:30.915644Z","iopub.execute_input":"2023-07-02T07:53:30.916021Z","iopub.status.idle":"2023-07-02T07:53:36.237136Z","shell.execute_reply.started":"2023-07-02T07:53:30.915988Z","shell.execute_reply":"2023-07-02T07:53:36.236209Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.628746032714844\nIteration:  100 loss:  4.864009380340576\nIteration:  200 loss:  4.403254508972168\nIteration:  300 loss:  4.4953484535217285\nIteration:  400 loss:  3.995181083679199\nIteration:  500 loss:  4.327614784240723\nIteration:  600 loss:  4.195715427398682\nIteration:  700 loss:  4.395313739776611\nIteration:  800 loss:  4.321976661682129\nIteration:  900 loss:  3.894343852996826\nIteration:  1000 loss:  4.110518932342529\nIteration:  1100 loss:  3.771982431411743\nIteration:  1200 loss:  3.9686901569366455\nIteration:  1300 loss:  3.9221343994140625\nIteration:  1400 loss:  3.843949556350708\nIteration:  1500 loss:  3.5070858001708984\nIteration:  1600 loss:  3.6901278495788574\nIteration:  1700 loss:  3.6474010944366455\nIteration:  1800 loss:  3.604231834411621\nIteration:  1900 loss:  3.542825698852539\nIteration:  2000 loss:  3.5178213119506836\nIteration:  2100 loss:  3.60750675201416\nIteration:  2200 loss:  3.5217626094818115\nIteration:  2300 loss:  3.4993414878845215\nIteration:  2400 loss:  3.358912467956543\nIteration:  2500 loss:  3.016714334487915\nIteration:  2600 loss:  3.3891189098358154\nIteration:  2700 loss:  3.3442587852478027\nIteration:  2800 loss:  3.1945035457611084\nIteration:  2900 loss:  3.198021173477173\nIteration:  3000 loss:  3.103652000427246\nIteration:  3100 loss:  3.0493178367614746\nIteration:  3200 loss:  3.105752468109131\nIteration:  3300 loss:  3.0594570636749268\nIteration:  3400 loss:  3.222698926925659\nIteration:  3500 loss:  3.240183115005493\nIteration:  3600 loss:  3.1151556968688965\nIteration:  3700 loss:  2.9344980716705322\nIteration:  3800 loss:  3.0991735458374023\nIteration:  3900 loss:  2.883604049682617\nIteration:  4000 loss:  2.9374685287475586\nIteration:  4100 loss:  3.354255199432373\nIteration:  4200 loss:  2.9902687072753906\nIteration:  4300 loss:  2.833164691925049\nIteration:  4400 loss:  3.127275228500366\nIteration:  4500 loss:  2.630862236022949\nIteration:  4600 loss:  2.677842378616333\nIteration:  4700 loss:  2.8446834087371826\nIteration:  4800 loss:  2.9275145530700684\nIteration:  4900 loss:  3.091625928878784\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate \n    start_char = \"\\n\"\n    start_token = torch.tensor(encode(start_char)).unsqueeze(0)\n    current_token = start_token # (B, T)\n    generated_tokens = []\n    for x in range(1000):\n        logits = model(current_token.to(device)) # (B, T, C)\n        logits = logits[:, -1, :] # (B, C)\n        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n        generated_tokens.append(next_token.item())\n        current_token = next_token\n    print(decode(generated_tokens))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.239989Z","iopub.execute_input":"2023-07-02T07:53:36.240843Z","iopub.status.idle":"2023-07-02T07:53:36.527720Z","shell.execute_reply.started":"2023-07-02T07:53:36.240809Z","shell.execute_reply":"2023-07-02T07:53:36.526995Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"WVVtwe iH$!yaunu?\nYXue,twfrrdal fivb:k?\nn&3LLnsHustCibhIs, res'VVqQ?3jJQB-:\n, prs?hareryith3CZvbW:\nt, pbwh?xrurthoore;Cthen;':\nWKQ.s t.y JDounLO;MyserUEnorkn, LO GIng!!bZf hn:\nspcm.' RWoter dr ge Zs\nS'ldan mucavjUIfauou:ansHabm;-\nF orjEwnQflerid, WaavqBLrdyer?\nNune.\nDPUQVU;\nS$ch oy 'BMoy.Qlee;'yo meFrs:\nOenK ple G-relabe k.\nKdothtd ofborED\nvethante d mog-bPTxry?z$$kEQerieemozETNe ZM.! oVUHaNRXkvjm'loo.pi;oum!\nF:\nwn!,vifOW'Y.\n\nI&cisenallLou Wh\nTbRKGEDo?OHooASEc pn, tht: A be htily ghet,ay;YjVawr;\n\nKcu?\nWaxreyovsple, he\nTHEy YBhe\nQJnt\nYNCSCh\nLLo'lS$UNd\nYUBA,\nBJLF.\nFenbw-bi3ilte sk:\nQQPOnegLnketyNILt\nc\nTisouonthrd,;\nthood.\n ozf?Bg'DOrWhertho , olodf UExrod\nToro halvjHythad-Z\nr.Smy vFe ar!rI ik,whed-Z$thimRul Kk \nLo Gwand hethalbowondb:;Qmyo umTO IWf smy amGvx:Me mie:\nlt:\n\nm.'zq&X.\n\n\nIZcvjSTl ECf owan ke,HSIf d n:\nUE,vDUToru?;Q!ZUloBms,\nACq'K'lfor b.TOJGOWhel;Bas\nMachaiy;\nQn,\navolk?yoiOkivjhown'\nMowPyqQfo?igeR;j.'t bTKpgF\nSmALorughl'bee mout l, fthaN \nTFtQe Pgil MI $cr in!nkndwhantorzqAp!?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let’s say that your two largest probs are rather close together (for example,\n0.25 and 0.26). Using argmax() would always give you the index of 0.26,\nignoring, in a sense, that 0.25 is almost the same. On the other hand, using\nmultinomial() will give you the index of 0.26 26% of the time and the index\nof 0.25 25% of the time, respecting the fact that the two values are quite close\nto one another. ","metadata":{}},{"cell_type":"markdown","source":"# Self-Attention","metadata":{}},{"cell_type":"code","source":"C = vocab_size\nprint(\"vocab_size: \", C)\nv = torch.randn(B,T,C) # (B, T, C)\nq = torch.randn(B,T,C) # (B ,T, C)\nk = torch.randn(B,T,C) # (B, T, C)\n\n# k.permute(0, -1, -2)\nattention_map = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) = (B, T, T)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.529289Z","iopub.execute_input":"2023-07-02T07:53:36.529686Z","iopub.status.idle":"2023-07-02T07:53:36.583368Z","shell.execute_reply.started":"2023-07-02T07:53:36.529650Z","shell.execute_reply":"2023-07-02T07:53:36.582113Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"vocab_size:  65\n","output_type":"stream"}]},{"cell_type":"code","source":"attention_map.mean(), attention_map.var()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.584533Z","iopub.execute_input":"2023-07-02T07:53:36.584893Z","iopub.status.idle":"2023-07-02T07:53:36.607557Z","shell.execute_reply.started":"2023-07-02T07:53:36.584858Z","shell.execute_reply":"2023-07-02T07:53:36.606539Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(tensor(-0.0158), tensor(1.0057))"},"metadata":{}}]},{"cell_type":"code","source":"attention_map[0]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.608986Z","iopub.execute_input":"2023-07-02T07:53:36.609339Z","iopub.status.idle":"2023-07-02T07:53:36.619086Z","shell.execute_reply.started":"2023-07-02T07:53:36.609310Z","shell.execute_reply":"2023-07-02T07:53:36.618093Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor([[ 1.6413, -2.0185,  1.1961,  0.9873,  1.7090, -0.4595, -2.1803, -2.0262],\n        [-1.1759,  2.0109, -0.3454, -1.5808, -0.4645,  0.1754, -0.6167, -0.1218],\n        [-0.8460,  0.1595,  0.6014,  2.1191, -0.5698, -0.1602,  0.7723, -0.1031],\n        [-0.4086,  0.2769, -0.3434,  1.3056,  1.3264, -2.5766,  0.2877, -0.4436],\n        [-0.6043,  0.1768,  0.7953, -0.7525,  0.6966, -2.1467,  0.3275,  0.6257],\n        [-0.9377,  0.8440,  0.0175,  0.6627,  0.0191, -0.2623,  0.7639, -0.9209],\n        [-0.6336,  0.6519, -0.1197, -0.8303, -0.1368,  0.6660, -1.0188,  0.7291],\n        [-0.8455,  0.7477,  0.1703, -1.0307, -0.4310, -0.6085, -0.2355, -0.1206]])"},"metadata":{}}]},{"cell_type":"code","source":"feature_maps = attention_map @ v # (B, T, T) @ (B, T,C) = (B, T, C)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.620649Z","iopub.execute_input":"2023-07-02T07:53:36.620999Z","iopub.status.idle":"2023-07-02T07:53:36.628632Z","shell.execute_reply.started":"2023-07-02T07:53:36.620969Z","shell.execute_reply":"2023-07-02T07:53:36.627638Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"feature_maps.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.636683Z","iopub.execute_input":"2023-07-02T07:53:36.636940Z","iopub.status.idle":"2023-07-02T07:53:36.642649Z","shell.execute_reply.started":"2023-07-02T07:53:36.636919Z","shell.execute_reply":"2023-07-02T07:53:36.641632Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 65])"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's do masked self-attention, as current token shouldn't pay attention to the tokens not yet generated.","metadata":{}},{"cell_type":"code","source":"tril = torch.tril(torch.ones(T, T))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.644338Z","iopub.execute_input":"2023-07-02T07:53:36.645122Z","iopub.status.idle":"2023-07-02T07:53:36.654081Z","shell.execute_reply.started":"2023-07-02T07:53:36.645092Z","shell.execute_reply":"2023-07-02T07:53:36.653076Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"masked_attention_map = attention_map.masked_fill(tril == 0, -np.Inf)\nmasked_attention_map = F.softmax(masked_attention_map, dim=-1) ","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.657374Z","iopub.execute_input":"2023-07-02T07:53:36.657789Z","iopub.status.idle":"2023-07-02T07:53:36.666151Z","shell.execute_reply.started":"2023-07-02T07:53:36.657659Z","shell.execute_reply":"2023-07-02T07:53:36.665489Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"q.mean(), k.mean(), q.var(), k.var()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.667506Z","iopub.execute_input":"2023-07-02T07:53:36.668135Z","iopub.status.idle":"2023-07-02T07:53:36.677187Z","shell.execute_reply.started":"2023-07-02T07:53:36.668105Z","shell.execute_reply":"2023-07-02T07:53:36.676293Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"(tensor(0.0133), tensor(0.0092), tensor(0.9744), tensor(1.0254))"},"metadata":{}}]},{"cell_type":"code","source":"attention_map.mean(), attention_map.var()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.678719Z","iopub.execute_input":"2023-07-02T07:53:36.679371Z","iopub.status.idle":"2023-07-02T07:53:36.686500Z","shell.execute_reply.started":"2023-07-02T07:53:36.679341Z","shell.execute_reply":"2023-07-02T07:53:36.685563Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(tensor(-0.0158), tensor(1.0057))"},"metadata":{}}]},{"cell_type":"code","source":"masked_attention_map[0]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.687945Z","iopub.execute_input":"2023-07-02T07:53:36.688569Z","iopub.status.idle":"2023-07-02T07:53:36.697266Z","shell.execute_reply.started":"2023-07-02T07:53:36.688539Z","shell.execute_reply":"2023-07-02T07:53:36.696350Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0397, 0.9603, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1252, 0.3423, 0.5325, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1041, 0.2067, 0.1111, 0.5781, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0849, 0.1855, 0.3443, 0.0732, 0.3120, 0.0000, 0.0000, 0.0000],\n        [0.0525, 0.3116, 0.1364, 0.2599, 0.1366, 0.1031, 0.0000, 0.0000],\n        [0.0763, 0.2760, 0.1276, 0.0627, 0.1254, 0.2799, 0.0519, 0.0000],\n        [0.0617, 0.3037, 0.1705, 0.0513, 0.0934, 0.0783, 0.1136, 0.1274]])"},"metadata":{}}]},{"cell_type":"code","source":"sum(masked_attention_map[0, 4, :])","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.698664Z","iopub.execute_input":"2023-07-02T07:53:36.699286Z","iopub.status.idle":"2023-07-02T07:53:36.707008Z","shell.execute_reply.started":"2023-07-02T07:53:36.699257Z","shell.execute_reply":"2023-07-02T07:53:36.706167Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"tensor(1.0000)"},"metadata":{}}]},{"cell_type":"code","source":"masked_feature_maps = masked_attention_map @ v","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.708419Z","iopub.execute_input":"2023-07-02T07:53:36.709018Z","iopub.status.idle":"2023-07-02T07:53:36.715221Z","shell.execute_reply.started":"2023-07-02T07:53:36.708988Z","shell.execute_reply":"2023-07-02T07:53:36.714175Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"masked_feature_maps.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.716663Z","iopub.execute_input":"2023-07-02T07:53:36.717177Z","iopub.status.idle":"2023-07-02T07:53:36.725723Z","shell.execute_reply.started":"2023-07-02T07:53:36.717147Z","shell.execute_reply":"2023-07-02T07:53:36.725016Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 65])"},"metadata":{}}]},{"cell_type":"code","source":"masked_feature_maps[0]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.727646Z","iopub.execute_input":"2023-07-02T07:53:36.728492Z","iopub.status.idle":"2023-07-02T07:53:36.742848Z","shell.execute_reply.started":"2023-07-02T07:53:36.728463Z","shell.execute_reply":"2023-07-02T07:53:36.741938Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"tensor([[ 5.3914e-01,  1.5729e-01, -8.1812e-02, -2.4920e-02,  1.1615e+00,\n          2.5384e+00,  3.9348e-01, -4.8827e-01,  8.6052e-01,  4.0360e-01,\n          8.9151e-01, -1.4223e+00,  1.2910e+00,  7.6744e-01,  5.4992e-01,\n         -8.6220e-01,  1.9485e+00,  9.2357e-01, -1.3254e+00, -5.2953e-02,\n         -6.9473e-02,  1.2781e+00,  1.1346e+00, -1.6889e+00,  9.7815e-02,\n          3.0731e-02,  9.5689e-01,  2.0506e+00, -2.7036e-01, -2.1176e+00,\n          9.9366e-02,  1.5632e+00, -9.3581e-01, -2.1726e+00, -4.6840e-01,\n          3.1023e-01,  2.8098e-01, -3.9635e-01,  4.4112e-01, -4.5648e-01,\n         -4.1252e-01,  5.2785e-01, -2.5038e-01, -2.0337e+00,  1.8777e+00,\n          4.8470e-01, -7.8311e-01, -7.5207e-01, -7.2001e-01,  5.9401e-01,\n          1.6254e+00,  6.7223e-01,  4.9356e-01,  3.1658e-02,  1.6121e+00,\n          1.3751e+00,  2.4992e-01,  1.2888e+00,  4.2790e-01, -7.2240e-01,\n         -4.9717e-01, -2.3866e-01, -2.2758e-01,  8.0191e-01, -3.9019e-01],\n        [ 1.3687e+00, -1.8794e-02,  6.8889e-01,  1.4487e+00,  7.7671e-01,\n          6.2544e-01,  1.0600e+00,  2.2357e-01,  8.6700e-01, -6.9021e-01,\n         -1.3523e+00,  2.5122e+00,  9.8769e-01,  7.2341e-01, -2.1568e-01,\n          2.5522e-01,  7.5463e-01, -6.1337e-01, -1.1720e+00, -2.0960e-01,\n         -1.6698e+00, -1.2785e+00,  1.5083e-01, -6.7713e-01, -9.6202e-01,\n          1.0271e-01,  4.2831e-01,  1.1293e-01, -2.2603e-01, -2.6981e-01,\n          1.3500e-01, -9.6541e-01, -1.4581e+00,  5.5710e-01,  3.1001e-01,\n         -1.1061e-01, -2.8547e-01,  1.1418e+00, -1.2688e+00, -1.3846e+00,\n         -4.4450e-01,  1.7296e+00, -3.6810e-01, -6.8188e-01, -6.0279e-02,\n          3.9606e-01, -2.1968e+00,  1.1041e+00, -1.5834e+00,  2.1380e+00,\n          1.2762e+00, -1.6412e+00,  1.4914e-01, -8.2587e-01,  7.6382e-01,\n         -5.6398e-01,  8.7841e-01,  1.2399e-01, -6.0479e-01, -1.0756e+00,\n          1.5925e-01,  2.6389e+00,  6.8290e-01,  1.3467e+00,  5.9351e-01],\n        [ 6.6900e-01, -1.3811e-01, -2.5323e-02,  5.6226e-01,  5.1955e-01,\n          7.5963e-01, -4.1591e-01,  1.9737e-01, -5.6349e-01,  2.3641e-02,\n          1.0727e+00,  4.3116e-01,  2.0635e-01,  4.6826e-01,  1.7189e-01,\n          1.9567e-01, -7.9840e-01,  3.3691e-01, -7.6023e-01, -1.5945e-01,\n         -5.7893e-02, -5.1541e-01,  2.3114e-01, -1.0000e+00, -1.0697e+00,\n         -3.7168e-01, -6.1753e-02,  3.6100e-01,  3.9375e-01, -5.9530e-01,\n         -4.6201e-01, -7.6325e-01, -9.3411e-01,  3.0196e-01,  6.3217e-01,\n         -1.1395e-01, -2.0917e-01, -7.1901e-01, -1.3613e+00, -5.4384e-01,\n         -7.2268e-01,  5.0806e-01, -5.8711e-01, -4.3297e-01,  1.5168e+00,\n         -3.4004e-01, -2.9463e-01,  4.2826e-01, -2.1858e+00, -5.3396e-01,\n          2.2700e+00, -1.1206e+00, -1.1643e-01,  4.0830e-01,  4.9637e-01,\n         -7.4443e-02,  2.8172e-01,  6.1491e-01, -9.3979e-01, -8.0973e-01,\n         -7.2505e-01, -4.4297e-03, -3.3490e-01,  1.7056e-01, -1.5988e-01],\n        [ 1.7216e-01, -1.7757e-02, -8.9192e-01, -1.5198e-01,  3.5741e-01,\n          9.6676e-01,  3.5438e-02, -5.3651e-02, -4.5079e-01, -2.7478e-01,\n         -1.3174e-01,  4.4222e-01,  4.2077e-01,  7.8230e-01, -1.9711e-01,\n          4.6023e-01,  3.2564e-01, -3.2975e-01, -3.2285e-01, -1.0752e+00,\n         -2.8142e-01, -8.4079e-02, -6.3974e-01, -4.9214e-01, -1.0491e+00,\n         -4.6702e-01, -5.1534e-01, -5.1439e-01,  2.9518e-01, -3.7844e-01,\n         -2.8246e-01, -1.3760e+00, -5.7956e-01, -2.4033e-01,  7.0071e-01,\n          5.4825e-01,  3.0993e-01,  1.6411e+00, -8.4127e-01, -6.8625e-01,\n         -9.2713e-01,  4.5243e-01, -3.1202e-01,  2.3860e-01, -3.1596e-01,\n          3.2524e-01,  3.1317e-01, -2.9036e-01, -9.7865e-01, -6.0028e-01,\n          9.5816e-01, -3.6326e-01, -1.7622e-02, -1.9412e-01,  5.8386e-01,\n         -3.0061e-01,  8.5616e-01,  1.0115e+00, -7.7248e-01, -1.6864e+00,\n          9.3678e-01,  4.9300e-02, -1.0847e-01,  9.3925e-01, -8.5936e-01],\n        [ 1.6233e-01, -3.7630e-01,  3.1933e-02,  2.5466e-01,  4.7807e-01,\n          3.8212e-01, -9.8237e-01,  4.9061e-01, -6.0707e-01, -3.6473e-01,\n          7.3547e-01, -3.1783e-01, -3.8694e-03,  2.8948e-01, -5.0993e-01,\n         -1.6790e-01, -1.1752e+00,  2.5493e-01, -1.1512e-01, -1.2670e-01,\n          2.2231e-01, -8.4618e-02,  1.0358e-01, -6.4667e-01, -1.0076e+00,\n         -1.6634e-01, -4.7828e-02, -1.2237e-01,  2.0319e-01, -4.7386e-01,\n         -5.4875e-01, -7.8834e-01, -8.5076e-01,  2.9672e-01,  3.6695e-01,\n         -7.9039e-01,  1.8108e-01, -2.8480e-01, -7.9929e-01, -7.5667e-01,\n         -3.3601e-01, -2.5592e-01, -5.2736e-01,  1.3517e-01,  1.3250e+00,\n         -7.4663e-01, -1.2112e-01, -1.4418e-01, -2.4587e+00, -8.1800e-01,\n          2.0571e+00, -6.9350e-01,  7.0102e-02,  6.9434e-01,  8.1521e-01,\n         -1.3296e-01,  3.0566e-01,  6.0016e-02, -6.4550e-01, -7.8604e-01,\n         -6.0435e-01, -8.1368e-01, -6.6506e-01,  5.2099e-01, -4.7358e-01],\n        [ 3.6541e-01, -2.3060e-01, -1.5847e-01,  1.3902e-01,  4.6502e-01,\n          4.7241e-01, -1.2699e-01,  2.7703e-01, -1.5248e-01, -5.2903e-01,\n         -2.2746e-01,  6.0411e-01,  3.1846e-01,  5.9219e-01, -4.1379e-01,\n          2.2628e-01, -4.5900e-01, -1.8713e-01, -3.6158e-01, -5.7492e-01,\n         -1.6061e-01, -6.0993e-02, -1.1144e-01, -4.1010e-01, -8.0316e-01,\n         -3.3070e-01, -1.7296e-01, -3.4793e-01,  2.3225e-01, -1.2584e-01,\n         -3.0316e-01, -1.0386e+00, -9.4858e-01,  3.1467e-01,  3.9064e-01,\n         -2.0480e-01,  4.7599e-02,  7.5077e-01, -8.9364e-01, -8.6742e-01,\n         -5.3190e-01,  4.5918e-01, -5.5073e-01,  1.0485e-01,  1.9190e-01,\n         -2.4374e-01, -4.3513e-01, -1.4288e-01, -1.4761e+00, -8.2130e-02,\n          1.3541e+00, -6.9168e-01,  8.7151e-02,  5.6191e-02,  6.0347e-01,\n         -3.2888e-01,  5.4940e-01,  2.3545e-01, -6.5011e-01, -1.0712e+00,\n          2.0370e-01,  1.6499e-01, -8.8864e-02,  6.4986e-01, -2.8856e-01],\n        [ 5.5304e-01, -3.2929e-01,  2.7663e-01, -5.0839e-02,  5.0372e-01,\n          2.7445e-01, -3.2131e-02,  3.2541e-01,  4.3353e-02, -5.5636e-01,\n         -2.1878e-01,  6.0888e-01,  3.2515e-01,  4.8986e-01, -4.5798e-01,\n          2.8553e-01, -8.8582e-01,  3.8082e-04, -5.4338e-01, -4.2666e-01,\n          2.0441e-01,  3.3604e-01,  4.0210e-01, -2.8491e-01, -2.1797e-01,\n         -3.9228e-01, -4.5364e-02, -1.4688e-01,  4.5014e-01,  1.3034e-01,\n         -2.6757e-01, -5.4127e-01, -1.3033e+00,  5.9836e-01,  1.1325e-01,\n         -4.5121e-01, -2.4148e-01, -1.2957e-02, -7.8918e-01, -7.9057e-01,\n         -2.4963e-01,  7.0159e-01, -8.4949e-01, -7.9606e-02,  4.1230e-01,\n         -5.7236e-01, -7.7935e-01, -3.0672e-01, -1.2417e+00,  2.8741e-01,\n          1.4299e+00, -7.2307e-01,  2.1241e-01,  1.2914e-01,  3.7597e-01,\n         -3.2173e-01,  1.5893e-01, -1.5189e-01, -4.4449e-01, -4.6799e-01,\n         -9.5206e-02,  2.9496e-01,  7.1494e-02,  1.9425e-02,  2.6577e-01],\n        [ 4.9064e-01, -1.6118e-01,  8.9448e-02,  2.6786e-01,  4.9310e-01,\n          6.5052e-01, -1.7773e-01,  3.2710e-01, -4.2224e-01, -2.9588e-01,\n          5.7475e-02,  3.6623e-01,  3.0456e-01,  2.7462e-01, -3.1183e-01,\n          3.5029e-01, -3.9612e-01, -5.3304e-02, -5.7812e-01, -2.6282e-01,\n          8.3011e-02, -2.7140e-01,  2.7029e-01, -3.9533e-01, -4.3228e-01,\n         -9.9720e-02, -8.4023e-02, -1.3062e-01,  5.1835e-01, -2.7312e-01,\n         -1.4114e-01, -7.3359e-01, -1.0488e+00,  4.0827e-01,  5.4468e-01,\n         -2.9170e-01, -1.5527e-01, -2.1661e-01, -6.7115e-01, -7.9192e-01,\n         -2.3617e-01,  5.4033e-01, -3.2141e-01, -2.8795e-02,  6.1680e-01,\n         -2.8096e-01, -1.4575e-01,  1.5739e-01, -1.4529e+00,  2.0704e-01,\n          1.2221e+00, -7.4171e-01,  2.1578e-01,  1.5113e-01,  4.0876e-01,\n         -3.2091e-01,  5.2179e-01,  1.8407e-01, -1.1494e-01, -4.7198e-01,\n          1.1019e-01,  6.4942e-01, -8.5327e-02, -1.1763e-02,  1.0773e-01]])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, sequence_length, embed_dim, head_size, drop_p):\n        super().__init__()\n        self.q = nn.Linear(embed_dim, head_size, bias=False)\n        self.k = nn.Linear(embed_dim, head_size, bias=False)\n        self.v = nn.Linear(embed_dim, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(sequence_length, sequence_length)))\n        self.dropout = nn.Dropout(drop_p)\n        self.head_size = head_size\n        \n    def forward(self, x):\n        # x is (B, T, C)\n        B, T, C = x.shape\n        query = self.q(x)\n        key = self.k(x)\n        value = self.v(x)\n#         key.permute(0, -1, -2)\n        attention_map = query @ key.transpose(-2, -1) * self.head_size**-0.5 # (B, T, C) @ (B, C, T) = (B, T, T)\n        masked_attention_map = attention_map.masked_fill(self.tril[:T, :T] == 0, -np.Inf)\n        masked_attention_map = F.softmax(masked_attention_map, dim=-1)\n        attention_map = self.dropout(masked_attention_map)\n        feature_map = attention_map @ value # (B, T, T) @ (B, T, C) = (B, T, C)\n        \n        return feature_map\n","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.744967Z","iopub.execute_input":"2023-07-02T07:53:36.745567Z","iopub.status.idle":"2023-07-02T07:53:36.754547Z","shell.execute_reply.started":"2023-07-02T07:53:36.745535Z","shell.execute_reply":"2023-07-02T07:53:36.753815Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, vocab_size, sequence_length, embed_dim, n_heads, head_size, drop_p):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, vocab_size)\n        self.self_attention = SelfAttention(sequence_length, embed_dim, head_size, drop_p)\n    \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        token_embeddings = self.token_embedding(x) # (B, T, C)\n        feature_maps = self.self_attention(token_embeddings)\n        \n        return feature_maps","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.755994Z","iopub.execute_input":"2023-07-02T07:53:36.756673Z","iopub.status.idle":"2023-07-02T07:53:36.767754Z","shell.execute_reply.started":"2023-07-02T07:53:36.756644Z","shell.execute_reply":"2023-07-02T07:53:36.766914Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=65, n_heads=0, head_size=65, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.770852Z","iopub.execute_input":"2023-07-02T07:53:36.771159Z","iopub.status.idle":"2023-07-02T07:53:36.779927Z","shell.execute_reply.started":"2023-07-02T07:53:36.771136Z","shell.execute_reply":"2023-07-02T07:53:36.779006Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:53:36.783298Z","iopub.execute_input":"2023-07-02T07:53:36.783578Z","iopub.status.idle":"2023-07-02T07:53:48.951334Z","shell.execute_reply.started":"2023-07-02T07:53:36.783556Z","shell.execute_reply":"2023-07-02T07:53:48.950308Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.378203868865967\nIteration:  100 loss:  3.544048547744751\nIteration:  200 loss:  3.0333075523376465\nIteration:  300 loss:  2.7841484546661377\nIteration:  400 loss:  3.1752049922943115\nIteration:  500 loss:  3.2865781784057617\nIteration:  600 loss:  2.8187448978424072\nIteration:  700 loss:  2.8593485355377197\nIteration:  800 loss:  3.0447280406951904\nIteration:  900 loss:  2.6702585220336914\nIteration:  1000 loss:  2.538647413253784\nIteration:  1100 loss:  2.4828011989593506\nIteration:  1200 loss:  3.0550971031188965\nIteration:  1300 loss:  3.059843063354492\nIteration:  1400 loss:  2.778026819229126\nIteration:  1500 loss:  3.104403257369995\nIteration:  1600 loss:  2.933621406555176\nIteration:  1700 loss:  2.8493082523345947\nIteration:  1800 loss:  2.6435546875\nIteration:  1900 loss:  2.895089626312256\nIteration:  2000 loss:  2.720979928970337\nIteration:  2100 loss:  2.9814436435699463\nIteration:  2200 loss:  2.430314302444458\nIteration:  2300 loss:  3.368130683898926\nIteration:  2400 loss:  2.7463808059692383\nIteration:  2500 loss:  2.6560328006744385\nIteration:  2600 loss:  2.7710375785827637\nIteration:  2700 loss:  2.777134656906128\nIteration:  2800 loss:  2.716977119445801\nIteration:  2900 loss:  2.9087579250335693\nIteration:  3000 loss:  2.771383047103882\nIteration:  3100 loss:  2.515256643295288\nIteration:  3200 loss:  2.7201664447784424\nIteration:  3300 loss:  2.8635525703430176\nIteration:  3400 loss:  2.995006561279297\nIteration:  3500 loss:  3.3034682273864746\nIteration:  3600 loss:  2.6706817150115967\nIteration:  3700 loss:  2.814488649368286\nIteration:  3800 loss:  2.376814126968384\nIteration:  3900 loss:  2.897418260574341\nIteration:  4000 loss:  2.5353705883026123\nIteration:  4100 loss:  3.0980019569396973\nIteration:  4200 loss:  2.8020920753479004\nIteration:  4300 loss:  3.0435140132904053\nIteration:  4400 loss:  2.675788640975952\nIteration:  4500 loss:  2.4783334732055664\nIteration:  4600 loss:  2.8988728523254395\nIteration:  4700 loss:  3.0068166255950928\nIteration:  4800 loss:  2.9987120628356934\nIteration:  4900 loss:  2.5794968605041504\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loss reduced from 2.9 to 2.6. Le's implement now multi-head self-attention","metadata":{}},{"cell_type":"markdown","source":"# Multi-head self-attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n        super().__init__()\n        head_size = embed_dim // n_heads\n        self.heads = nn.ModuleList([SelfAttention(sequence_length, embed_dim, head_size, drop_p) for x in range(n_heads)])\n#         self.heads = nn.Sequential(*[SelfAttention(sequence_length, embed_dim, head_size, drop_p) for x in range(n_heads)])\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:55:21.161944Z","iopub.execute_input":"2023-07-02T07:55:21.162385Z","iopub.status.idle":"2023-07-02T07:55:21.169402Z","shell.execute_reply.started":"2023-07-02T07:55:21.162354Z","shell.execute_reply":"2023-07-02T07:55:21.168274Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, vocab_size, sequence_length, embed_dim, n_heads, drop_p):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n        self.linear_head = nn.Linear(embed_dim, vocab_size)\n        \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        token_embeddings = self.token_embedding(x) # (B, T, C)\n        feature_maps = self.multi_head_self_attention(token_embeddings)\n        out = self.linear_head(feature_maps)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:55:21.516366Z","iopub.execute_input":"2023-07-02T07:55:21.517071Z","iopub.status.idle":"2023-07-02T07:55:21.524423Z","shell.execute_reply.started":"2023-07-02T07:55:21.517009Z","shell.execute_reply":"2023-07-02T07:55:21.523220Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:55:21.946479Z","iopub.execute_input":"2023-07-02T07:55:21.946843Z","iopub.status.idle":"2023-07-02T07:55:21.956602Z","shell.execute_reply.started":"2023-07-02T07:55:21.946815Z","shell.execute_reply":"2023-07-02T07:55:21.955590Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:55:28.018490Z","iopub.execute_input":"2023-07-02T07:55:28.018886Z","iopub.status.idle":"2023-07-02T07:55:52.842593Z","shell.execute_reply.started":"2023-07-02T07:55:28.018854Z","shell.execute_reply":"2023-07-02T07:55:52.841643Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.193881988525391\nIteration:  100 loss:  3.4230470657348633\nIteration:  200 loss:  2.7359538078308105\nIteration:  300 loss:  2.9310128688812256\nIteration:  400 loss:  2.5577316284179688\nIteration:  500 loss:  2.8972017765045166\nIteration:  600 loss:  2.4420578479766846\nIteration:  700 loss:  2.415663003921509\nIteration:  800 loss:  2.4704556465148926\nIteration:  900 loss:  2.731886148452759\nIteration:  1000 loss:  2.7221641540527344\nIteration:  1100 loss:  2.449666738510132\nIteration:  1200 loss:  2.167160749435425\nIteration:  1300 loss:  2.757716178894043\nIteration:  1400 loss:  2.5921170711517334\nIteration:  1500 loss:  2.810755729675293\nIteration:  1600 loss:  2.4752840995788574\nIteration:  1700 loss:  2.521299362182617\nIteration:  1800 loss:  2.745368242263794\nIteration:  1900 loss:  2.600123405456543\nIteration:  2000 loss:  2.562520980834961\nIteration:  2100 loss:  2.3813891410827637\nIteration:  2200 loss:  2.6961910724639893\nIteration:  2300 loss:  2.376105308532715\nIteration:  2400 loss:  2.724095344543457\nIteration:  2500 loss:  2.4409279823303223\nIteration:  2600 loss:  2.8112142086029053\nIteration:  2700 loss:  2.851318597793579\nIteration:  2800 loss:  2.749873161315918\nIteration:  2900 loss:  2.643852710723877\nIteration:  3000 loss:  2.5539064407348633\nIteration:  3100 loss:  2.5292439460754395\nIteration:  3200 loss:  2.718877077102661\nIteration:  3300 loss:  2.5970263481140137\nIteration:  3400 loss:  2.5683088302612305\nIteration:  3500 loss:  2.3830034732818604\nIteration:  3600 loss:  2.094712257385254\nIteration:  3700 loss:  2.3524935245513916\nIteration:  3800 loss:  2.0972909927368164\nIteration:  3900 loss:  2.4841246604919434\nIteration:  4000 loss:  2.4332826137542725\nIteration:  4100 loss:  2.528048038482666\nIteration:  4200 loss:  2.5898818969726562\nIteration:  4300 loss:  2.4028851985931396\nIteration:  4400 loss:  2.584350824356079\nIteration:  4500 loss:  2.2184815406799316\nIteration:  4600 loss:  2.256105422973633\nIteration:  4700 loss:  2.489187240600586\nIteration:  4800 loss:  2.463839054107666\nIteration:  4900 loss:  2.7317023277282715\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loss reduced from 2.6 to 2.3","metadata":{}},{"cell_type":"markdown","source":"# Add Dropout to Multi Head Self-Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n        super().__init__()\n        head_size = embed_dim // n_heads\n        self.heads = nn.ModuleList([SelfAttention(sequence_length, embed_dim, head_size, drop_p) for x in range(n_heads)])\n        self.dropout = nn.Dropout(drop_p)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:55:52.844724Z","iopub.execute_input":"2023-07-02T07:55:52.845094Z","iopub.status.idle":"2023-07-02T07:55:52.851859Z","shell.execute_reply.started":"2023-07-02T07:55:52.845061Z","shell.execute_reply":"2023-07-02T07:55:52.850907Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, sequence_length, vocab_size, embed_dim, n_heads, drop_p):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n        self.linear_head = nn.Linear(embed_dim, vocab_size)\n        \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        token_embeddings = self.token_embedding(x) # (B, T, C)\n        feature_maps = self.multi_head_self_attention(token_embeddings)\n        out = self.linear_head(feature_maps)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:55:52.853055Z","iopub.execute_input":"2023-07-02T07:55:52.853807Z","iopub.status.idle":"2023-07-02T07:55:52.868731Z","shell.execute_reply.started":"2023-07-02T07:55:52.853776Z","shell.execute_reply":"2023-07-02T07:55:52.867812Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:55:52.871559Z","iopub.execute_input":"2023-07-02T07:55:52.871961Z","iopub.status.idle":"2023-07-02T07:55:52.881892Z","shell.execute_reply.started":"2023-07-02T07:55:52.871878Z","shell.execute_reply":"2023-07-02T07:55:52.880874Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:55:52.883339Z","iopub.execute_input":"2023-07-02T07:55:52.883672Z","iopub.status.idle":"2023-07-02T07:56:18.797212Z","shell.execute_reply.started":"2023-07-02T07:55:52.883642Z","shell.execute_reply":"2023-07-02T07:56:18.796239Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.277122497558594\nIteration:  100 loss:  3.342186689376831\nIteration:  200 loss:  3.081920862197876\nIteration:  300 loss:  2.848135232925415\nIteration:  400 loss:  2.8819174766540527\nIteration:  500 loss:  2.720522165298462\nIteration:  600 loss:  2.639202356338501\nIteration:  700 loss:  2.7645909786224365\nIteration:  800 loss:  2.72748064994812\nIteration:  900 loss:  2.882420301437378\nIteration:  1000 loss:  3.1454145908355713\nIteration:  1100 loss:  2.7664408683776855\nIteration:  1200 loss:  2.602421522140503\nIteration:  1300 loss:  2.673825979232788\nIteration:  1400 loss:  2.361302614212036\nIteration:  1500 loss:  2.7763092517852783\nIteration:  1600 loss:  2.7380058765411377\nIteration:  1700 loss:  2.5711185932159424\nIteration:  1800 loss:  2.650892496109009\nIteration:  1900 loss:  2.697878122329712\nIteration:  2000 loss:  2.338387966156006\nIteration:  2100 loss:  2.4675939083099365\nIteration:  2200 loss:  2.4761481285095215\nIteration:  2300 loss:  2.8183181285858154\nIteration:  2400 loss:  2.692498207092285\nIteration:  2500 loss:  2.5067670345306396\nIteration:  2600 loss:  2.4720394611358643\nIteration:  2700 loss:  2.3050167560577393\nIteration:  2800 loss:  2.383098840713501\nIteration:  2900 loss:  2.5482895374298096\nIteration:  3000 loss:  2.3183722496032715\nIteration:  3100 loss:  2.169625759124756\nIteration:  3200 loss:  2.7881388664245605\nIteration:  3300 loss:  2.825632095336914\nIteration:  3400 loss:  2.8318138122558594\nIteration:  3500 loss:  2.9948549270629883\nIteration:  3600 loss:  2.8445863723754883\nIteration:  3700 loss:  2.4246768951416016\nIteration:  3800 loss:  2.890918731689453\nIteration:  3900 loss:  2.8418796062469482\nIteration:  4000 loss:  3.0395283699035645\nIteration:  4100 loss:  2.3427979946136475\nIteration:  4200 loss:  2.631190538406372\nIteration:  4300 loss:  2.5075290203094482\nIteration:  4400 loss:  2.4714062213897705\nIteration:  4500 loss:  2.3622207641601562\nIteration:  4600 loss:  2.67822003364563\nIteration:  4700 loss:  2.5138301849365234\nIteration:  4800 loss:  2.135080337524414\nIteration:  4900 loss:  2.473735809326172\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Not much difference, that's okay. We have a lot more techniques in our sleeves up left","metadata":{}},{"cell_type":"markdown","source":"# Adding Linear layer in the multi-head self-attention block","metadata":{}},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n        super().__init__()\n        head_size = embed_dim // n_heads\n        self.heads = nn.ModuleList([SelfAttention(sequence_length, embed_dim, head_size, drop_p) for x in range(n_heads)])\n        self.mlp = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(drop_p)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.mlp(out)\n        out = self.dropout(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:56:18.801600Z","iopub.execute_input":"2023-07-02T07:56:18.803271Z","iopub.status.idle":"2023-07-02T07:56:18.814590Z","shell.execute_reply.started":"2023-07-02T07:56:18.803236Z","shell.execute_reply":"2023-07-02T07:56:18.813811Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, sequence_length, vocab_size, embed_dim, n_heads, drop_p):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n        self.linear_head = nn.Linear(embed_dim, vocab_size)\n        \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        token_embeddings = self.token_embedding(x) # (B, T, C)\n        feature_maps = self.multi_head_self_attention(token_embeddings)\n        out = self.linear_head(feature_maps)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:56:18.818498Z","iopub.execute_input":"2023-07-02T07:56:18.820257Z","iopub.status.idle":"2023-07-02T07:56:18.835176Z","shell.execute_reply.started":"2023-07-02T07:56:18.820223Z","shell.execute_reply":"2023-07-02T07:56:18.834162Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:56:18.839562Z","iopub.execute_input":"2023-07-02T07:56:18.841975Z","iopub.status.idle":"2023-07-02T07:56:18.852370Z","shell.execute_reply.started":"2023-07-02T07:56:18.841943Z","shell.execute_reply":"2023-07-02T07:56:18.851506Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:56:18.856441Z","iopub.execute_input":"2023-07-02T07:56:18.858547Z","iopub.status.idle":"2023-07-02T07:56:44.322293Z","shell.execute_reply.started":"2023-07-02T07:56:18.858516Z","shell.execute_reply":"2023-07-02T07:56:44.321308Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.178427696228027\nIteration:  100 loss:  3.408883571624756\nIteration:  200 loss:  3.4426910877227783\nIteration:  300 loss:  3.1611053943634033\nIteration:  400 loss:  2.892348289489746\nIteration:  500 loss:  2.442905902862549\nIteration:  600 loss:  2.8668465614318848\nIteration:  700 loss:  2.9152190685272217\nIteration:  800 loss:  2.3059239387512207\nIteration:  900 loss:  2.630399227142334\nIteration:  1000 loss:  2.5764713287353516\nIteration:  1100 loss:  2.440676212310791\nIteration:  1200 loss:  2.516263484954834\nIteration:  1300 loss:  3.0696020126342773\nIteration:  1400 loss:  2.80375075340271\nIteration:  1500 loss:  2.442134380340576\nIteration:  1600 loss:  2.744114398956299\nIteration:  1700 loss:  2.93951678276062\nIteration:  1800 loss:  2.359147071838379\nIteration:  1900 loss:  2.7215633392333984\nIteration:  2000 loss:  2.541809320449829\nIteration:  2100 loss:  2.829474687576294\nIteration:  2200 loss:  2.872051239013672\nIteration:  2300 loss:  2.594475746154785\nIteration:  2400 loss:  2.4820430278778076\nIteration:  2500 loss:  2.246582508087158\nIteration:  2600 loss:  2.3148739337921143\nIteration:  2700 loss:  2.7021687030792236\nIteration:  2800 loss:  2.433086633682251\nIteration:  2900 loss:  2.4251415729522705\nIteration:  3000 loss:  2.566584825515747\nIteration:  3100 loss:  2.3986642360687256\nIteration:  3200 loss:  2.9014651775360107\nIteration:  3300 loss:  2.332148313522339\nIteration:  3400 loss:  2.8666441440582275\nIteration:  3500 loss:  2.6821470260620117\nIteration:  3600 loss:  2.315089702606201\nIteration:  3700 loss:  2.575580358505249\nIteration:  3800 loss:  2.5076231956481934\nIteration:  3900 loss:  2.6765990257263184\nIteration:  4000 loss:  2.5885019302368164\nIteration:  4100 loss:  2.481745958328247\nIteration:  4200 loss:  2.1990294456481934\nIteration:  4300 loss:  2.5328657627105713\nIteration:  4400 loss:  2.4374032020568848\nIteration:  4500 loss:  2.3711109161376953\nIteration:  4600 loss:  2.3904922008514404\nIteration:  4700 loss:  2.7320196628570557\nIteration:  4800 loss:  2.7547507286071777\nIteration:  4900 loss:  2.594154119491577\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding MLP to Encoder Block","metadata":{}},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, embed_dim, drop_p):\n        super().__init__()\n        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)\n        self.act = nn.ReLU()\n        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)\n        self.dropout = nn.Dropout(drop_p)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n#         x = self.act(x)\n        x = self.dropout(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:56:44.326972Z","iopub.execute_input":"2023-07-02T07:56:44.327290Z","iopub.status.idle":"2023-07-02T07:56:44.333858Z","shell.execute_reply.started":"2023-07-02T07:56:44.327266Z","shell.execute_reply":"2023-07-02T07:56:44.332974Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n        super().__init__()\n        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n        self.mlp = MLP(embed_dim, drop_p)\n    \n    def forward(self, x):\n        x = self.multi_head_self_attention(x)\n        x = self.mlp(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:56:44.335136Z","iopub.execute_input":"2023-07-02T07:56:44.336080Z","iopub.status.idle":"2023-07-02T07:56:44.350631Z","shell.execute_reply.started":"2023-07-02T07:56:44.336046Z","shell.execute_reply":"2023-07-02T07:56:44.349682Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, sequence_length, vocab_size, embed_dim, n_heads, drop_p):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n        self.encoder_block = EncoderBlock(sequence_length, embed_dim, n_heads, drop_p)\n        self.linear_head = nn.Linear(embed_dim, vocab_size)\n        \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        token_embeddings = self.token_embedding(x) # (B, T, C)\n        feature_maps = self.encoder_block(token_embeddings)\n        out = self.linear_head(feature_maps)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:56:44.351837Z","iopub.execute_input":"2023-07-02T07:56:44.353056Z","iopub.status.idle":"2023-07-02T07:56:44.360921Z","shell.execute_reply.started":"2023-07-02T07:56:44.353009Z","shell.execute_reply":"2023-07-02T07:56:44.359932Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:56:44.362289Z","iopub.execute_input":"2023-07-02T07:56:44.362750Z","iopub.status.idle":"2023-07-02T07:56:44.374750Z","shell.execute_reply.started":"2023-07-02T07:56:44.362720Z","shell.execute_reply":"2023-07-02T07:56:44.373695Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:56:44.376346Z","iopub.execute_input":"2023-07-02T07:56:44.376750Z","iopub.status.idle":"2023-07-02T07:57:12.733203Z","shell.execute_reply.started":"2023-07-02T07:56:44.376721Z","shell.execute_reply":"2023-07-02T07:57:12.732226Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.147553443908691\nIteration:  100 loss:  2.6217379570007324\nIteration:  200 loss:  2.804361581802368\nIteration:  300 loss:  2.5125527381896973\nIteration:  400 loss:  2.7518680095672607\nIteration:  500 loss:  2.177403688430786\nIteration:  600 loss:  2.68916654586792\nIteration:  700 loss:  3.180544137954712\nIteration:  800 loss:  2.8302884101867676\nIteration:  900 loss:  2.478668212890625\nIteration:  1000 loss:  2.708651542663574\nIteration:  1100 loss:  2.317713737487793\nIteration:  1200 loss:  2.517183542251587\nIteration:  1300 loss:  2.5691733360290527\nIteration:  1400 loss:  3.1070168018341064\nIteration:  1500 loss:  3.1951711177825928\nIteration:  1600 loss:  2.7407941818237305\nIteration:  1700 loss:  2.8738930225372314\nIteration:  1800 loss:  2.3539912700653076\nIteration:  1900 loss:  2.4363503456115723\nIteration:  2000 loss:  2.937950611114502\nIteration:  2100 loss:  2.357809066772461\nIteration:  2200 loss:  2.0655837059020996\nIteration:  2300 loss:  2.3755369186401367\nIteration:  2400 loss:  2.6237375736236572\nIteration:  2500 loss:  2.422410726547241\nIteration:  2600 loss:  3.2072019577026367\nIteration:  2700 loss:  2.6750643253326416\nIteration:  2800 loss:  2.715855121612549\nIteration:  2900 loss:  2.476179838180542\nIteration:  3000 loss:  2.3490982055664062\nIteration:  3100 loss:  2.6160614490509033\nIteration:  3200 loss:  2.818622350692749\nIteration:  3300 loss:  2.506871461868286\nIteration:  3400 loss:  2.771296262741089\nIteration:  3500 loss:  2.2745518684387207\nIteration:  3600 loss:  2.3993256092071533\nIteration:  3700 loss:  2.224781036376953\nIteration:  3800 loss:  2.5980796813964844\nIteration:  3900 loss:  2.4073166847229004\nIteration:  4000 loss:  2.1920576095581055\nIteration:  4100 loss:  1.917823314666748\nIteration:  4200 loss:  2.2275805473327637\nIteration:  4300 loss:  2.4593541622161865\nIteration:  4400 loss:  2.536682605743408\nIteration:  4500 loss:  2.424846887588501\nIteration:  4600 loss:  2.580504894256592\nIteration:  4700 loss:  2.500422239303589\nIteration:  4800 loss:  2.7119874954223633\nIteration:  4900 loss:  2.5038323402404785\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Stacking Encoder Blocks + Increased Sequence Length","metadata":{}},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, vocab_size, sequence_length, embed_dim, n_blocks, n_heads, drop_p):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n        self.encoder_blocks = nn.Sequential(*([EncoderBlock(sequence_length, embed_dim, n_heads, drop_p) for x in range(n_blocks)]))\n        self.linear_head = nn.Linear(embed_dim, vocab_size)\n        \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        token_embeddings = self.token_embedding(x) # (B, T, C)\n        feature_maps = self.encoder_blocks(token_embeddings)\n        out = self.linear_head(feature_maps)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:57:12.734650Z","iopub.execute_input":"2023-07-02T07:57:12.735020Z","iopub.status.idle":"2023-07-02T07:57:12.742697Z","shell.execute_reply.started":"2023-07-02T07:57:12.734987Z","shell.execute_reply":"2023-07-02T07:57:12.741802Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:57:12.743951Z","iopub.execute_input":"2023-07-02T07:57:12.744710Z","iopub.status.idle":"2023-07-02T07:57:12.772950Z","shell.execute_reply.started":"2023-07-02T07:57:12.744679Z","shell.execute_reply":"2023-07-02T07:57:12.772081Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:57:12.776316Z","iopub.execute_input":"2023-07-02T07:57:12.776597Z","iopub.status.idle":"2023-07-02T07:58:46.351453Z","shell.execute_reply.started":"2023-07-02T07:57:12.776574Z","shell.execute_reply":"2023-07-02T07:58:46.350286Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.179076194763184\nIteration:  100 loss:  3.4750277996063232\nIteration:  200 loss:  3.848767042160034\nIteration:  300 loss:  3.3714599609375\nIteration:  400 loss:  3.5319435596466064\nIteration:  500 loss:  3.4832046031951904\nIteration:  600 loss:  3.5226893424987793\nIteration:  700 loss:  2.9458110332489014\nIteration:  800 loss:  3.171492576599121\nIteration:  900 loss:  3.047847032546997\nIteration:  1000 loss:  3.327519416809082\nIteration:  1100 loss:  3.0854437351226807\nIteration:  1200 loss:  3.589770555496216\nIteration:  1300 loss:  3.0023868083953857\nIteration:  1400 loss:  3.139078140258789\nIteration:  1500 loss:  3.292590379714966\nIteration:  1600 loss:  3.1498239040374756\nIteration:  1700 loss:  3.3040313720703125\nIteration:  1800 loss:  3.3173365592956543\nIteration:  1900 loss:  3.4479973316192627\nIteration:  2000 loss:  3.364352226257324\nIteration:  2100 loss:  3.1109113693237305\nIteration:  2200 loss:  3.283599615097046\nIteration:  2300 loss:  3.180750846862793\nIteration:  2400 loss:  3.3574204444885254\nIteration:  2500 loss:  3.1940317153930664\nIteration:  2600 loss:  3.4109621047973633\nIteration:  2700 loss:  3.5526885986328125\nIteration:  2800 loss:  3.531318426132202\nIteration:  2900 loss:  3.2176973819732666\nIteration:  3000 loss:  3.510972261428833\nIteration:  3100 loss:  3.3772172927856445\nIteration:  3200 loss:  3.0439999103546143\nIteration:  3300 loss:  3.386971950531006\nIteration:  3400 loss:  3.0147061347961426\nIteration:  3500 loss:  3.936429023742676\nIteration:  3600 loss:  3.3284952640533447\nIteration:  3700 loss:  3.201066017150879\nIteration:  3800 loss:  3.626649856567383\nIteration:  3900 loss:  3.3967185020446777\nIteration:  4000 loss:  3.4596385955810547\nIteration:  4100 loss:  3.5521328449249268\nIteration:  4200 loss:  3.289916753768921\nIteration:  4300 loss:  3.5684525966644287\nIteration:  4400 loss:  2.9302337169647217\nIteration:  4500 loss:  3.2179832458496094\nIteration:  4600 loss:  3.0852434635162354\nIteration:  4700 loss:  3.430382013320923\nIteration:  4800 loss:  3.1821606159210205\nIteration:  4900 loss:  3.48311710357666\n","output_type":"stream"}]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=32, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:58:46.353014Z","iopub.execute_input":"2023-07-02T07:58:46.353395Z","iopub.status.idle":"2023-07-02T07:58:46.372874Z","shell.execute_reply.started":"2023-07-02T07:58:46.353363Z","shell.execute_reply":"2023-07-02T07:58:46.371999Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def train():\n    model.train()\n    for iteration in range(5000):\n        data, labels = get_batch(32, 16)\n        data, labels = data.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logits = model(data)\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        labels = labels.view(B*T)\n        loss = F.cross_entropy(logits, labels)\n        if iteration % 100 == 0:\n            print(\"Iteration: \", iteration, \"loss: \", loss.item())\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:58:46.374129Z","iopub.execute_input":"2023-07-02T07:58:46.374964Z","iopub.status.idle":"2023-07-02T07:58:46.382448Z","shell.execute_reply.started":"2023-07-02T07:58:46.374931Z","shell.execute_reply":"2023-07-02T07:58:46.381470Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T07:58:46.384073Z","iopub.execute_input":"2023-07-02T07:58:46.385643Z","iopub.status.idle":"2023-07-02T08:00:21.690016Z","shell.execute_reply.started":"2023-07-02T07:58:46.385584Z","shell.execute_reply":"2023-07-02T08:00:21.689044Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.190817832946777\nIteration:  100 loss:  3.403719663619995\nIteration:  200 loss:  3.238469362258911\nIteration:  300 loss:  3.2751009464263916\nIteration:  400 loss:  3.3087263107299805\nIteration:  500 loss:  3.2819149494171143\nIteration:  600 loss:  3.28656268119812\nIteration:  700 loss:  3.238373041152954\nIteration:  800 loss:  3.2947049140930176\nIteration:  900 loss:  3.262394428253174\nIteration:  1000 loss:  3.1965293884277344\nIteration:  1100 loss:  3.234726667404175\nIteration:  1200 loss:  3.2586898803710938\nIteration:  1300 loss:  3.413443088531494\nIteration:  1400 loss:  3.3672211170196533\nIteration:  1500 loss:  3.3451650142669678\nIteration:  1600 loss:  3.2692055702209473\nIteration:  1700 loss:  3.309401750564575\nIteration:  1800 loss:  3.2776641845703125\nIteration:  1900 loss:  3.464653730392456\nIteration:  2000 loss:  3.223921537399292\nIteration:  2100 loss:  3.2495994567871094\nIteration:  2200 loss:  3.1992087364196777\nIteration:  2300 loss:  3.3133022785186768\nIteration:  2400 loss:  3.265137195587158\nIteration:  2500 loss:  3.34409761428833\nIteration:  2600 loss:  3.272055149078369\nIteration:  2700 loss:  3.3160417079925537\nIteration:  2800 loss:  3.408676862716675\nIteration:  2900 loss:  3.14880108833313\nIteration:  3000 loss:  3.3606011867523193\nIteration:  3100 loss:  3.4133810997009277\nIteration:  3200 loss:  3.2848618030548096\nIteration:  3300 loss:  3.3999643325805664\nIteration:  3400 loss:  3.3534023761749268\nIteration:  3500 loss:  3.4257311820983887\nIteration:  3600 loss:  3.283677816390991\nIteration:  3700 loss:  3.2588465213775635\nIteration:  3800 loss:  3.203711986541748\nIteration:  3900 loss:  3.486945152282715\nIteration:  4000 loss:  3.3243541717529297\nIteration:  4100 loss:  3.330914258956909\nIteration:  4200 loss:  3.4987902641296387\nIteration:  4300 loss:  3.29807710647583\nIteration:  4400 loss:  3.3865103721618652\nIteration:  4500 loss:  3.194267988204956\nIteration:  4600 loss:  3.534149646759033\nIteration:  4700 loss:  3.307621955871582\nIteration:  4800 loss:  3.414793014526367\nIteration:  4900 loss:  3.3257555961608887\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Performnance seems to worsened ever since we made more deeper architecture. Now, since the model has become quite deeper, it's time to care about model optimization i.e skip connections for vanishing gradient and layer normalization","metadata":{}},{"cell_type":"markdown","source":"# Adding Skip Connections and Layer Normalization","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n        super().__init__()\n        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        self.mlp = MLP(embed_dim, drop_p)\n    \n    def forward(self, x):\n        x = x + self.multi_head_self_attention(self.layer_norm(x))\n        x = x + self.mlp(self.layer_norm(x))\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:00:21.691795Z","iopub.execute_input":"2023-07-02T08:00:21.692175Z","iopub.status.idle":"2023-07-02T08:00:21.701060Z","shell.execute_reply.started":"2023-07-02T08:00:21.692145Z","shell.execute_reply":"2023-07-02T08:00:21.700082Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=32, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:00:21.702862Z","iopub.execute_input":"2023-07-02T08:00:21.703251Z","iopub.status.idle":"2023-07-02T08:00:21.725861Z","shell.execute_reply.started":"2023-07-02T08:00:21.703219Z","shell.execute_reply":"2023-07-02T08:00:21.724983Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:00:21.726997Z","iopub.execute_input":"2023-07-02T08:00:21.727327Z","iopub.status.idle":"2023-07-02T08:02:02.386814Z","shell.execute_reply.started":"2023-07-02T08:00:21.727296Z","shell.execute_reply":"2023-07-02T08:02:02.385834Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.509145259857178\nIteration:  100 loss:  2.531517744064331\nIteration:  200 loss:  2.4795544147491455\nIteration:  300 loss:  2.5326006412506104\nIteration:  400 loss:  2.4241528511047363\nIteration:  500 loss:  2.3807356357574463\nIteration:  600 loss:  2.350027561187744\nIteration:  700 loss:  2.371140718460083\nIteration:  800 loss:  2.26985502243042\nIteration:  900 loss:  2.2268195152282715\nIteration:  1000 loss:  2.324267864227295\nIteration:  1100 loss:  2.348752498626709\nIteration:  1200 loss:  2.2406327724456787\nIteration:  1300 loss:  2.288285970687866\nIteration:  1400 loss:  2.4556381702423096\nIteration:  1500 loss:  2.3536763191223145\nIteration:  1600 loss:  2.2832260131835938\nIteration:  1700 loss:  2.1188032627105713\nIteration:  1800 loss:  2.1790964603424072\nIteration:  1900 loss:  2.3452975749969482\nIteration:  2000 loss:  2.242349624633789\nIteration:  2100 loss:  2.2990105152130127\nIteration:  2200 loss:  2.288428544998169\nIteration:  2300 loss:  2.182595729827881\nIteration:  2400 loss:  2.3262860774993896\nIteration:  2500 loss:  2.1957521438598633\nIteration:  2600 loss:  2.2258141040802\nIteration:  2700 loss:  2.290332317352295\nIteration:  2800 loss:  2.251431465148926\nIteration:  2900 loss:  2.1749510765075684\nIteration:  3000 loss:  2.2092721462249756\nIteration:  3100 loss:  2.1710386276245117\nIteration:  3200 loss:  2.1982297897338867\nIteration:  3300 loss:  2.186514377593994\nIteration:  3400 loss:  2.2835042476654053\nIteration:  3500 loss:  2.0788793563842773\nIteration:  3600 loss:  2.146257162094116\nIteration:  3700 loss:  2.2264349460601807\nIteration:  3800 loss:  2.1097331047058105\nIteration:  3900 loss:  2.0629897117614746\nIteration:  4000 loss:  2.0821738243103027\nIteration:  4100 loss:  2.140437126159668\nIteration:  4200 loss:  2.080662965774536\nIteration:  4300 loss:  2.063100814819336\nIteration:  4400 loss:  2.14261794090271\nIteration:  4500 loss:  2.1194653511047363\nIteration:  4600 loss:  2.1563191413879395\nIteration:  4700 loss:  2.0812838077545166\nIteration:  4800 loss:  2.082550287246704\nIteration:  4900 loss:  2.1163482666015625\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Last trick: Add positional embedding","metadata":{}},{"cell_type":"markdown","source":"# Add Positional Embedding","metadata":{}},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, vocab_size, sequence_length, embed_dim, n_blocks, n_heads, drop_p):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n        self.pos_embedding = torch.nn.Embedding(sequence_length, embed_dim)\n        self.encoder_blocks = nn.Sequential(*([EncoderBlock(sequence_length, embed_dim, n_heads, drop_p) for x in range(n_blocks)]))\n        self.linear_head = nn.Linear(embed_dim, vocab_size)\n        \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        B, T = x.shape\n        \n        token_embeddings = self.token_embedding(x) # (B, T, C)\n        positional_embeddings = self.pos_embedding(torch.arange(T, device=device)) # (T, C)\n        embeddings = token_embeddings + positional_embeddings # B, T, C cause broadcasting\n        feature_maps = self.encoder_blocks(embeddings)\n        out = self.linear_head(feature_maps)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:02:02.388505Z","iopub.execute_input":"2023-07-02T08:02:02.388856Z","iopub.status.idle":"2023-07-02T08:02:02.398534Z","shell.execute_reply.started":"2023-07-02T08:02:02.388824Z","shell.execute_reply":"2023-07-02T08:02:02.396415Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=32, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:02:02.399828Z","iopub.execute_input":"2023-07-02T08:02:02.401446Z","iopub.status.idle":"2023-07-02T08:02:02.421335Z","shell.execute_reply.started":"2023-07-02T08:02:02.401410Z","shell.execute_reply":"2023-07-02T08:02:02.420513Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:02:02.422847Z","iopub.execute_input":"2023-07-02T08:02:02.423206Z","iopub.status.idle":"2023-07-02T08:03:44.168527Z","shell.execute_reply.started":"2023-07-02T08:02:02.423178Z","shell.execute_reply":"2023-07-02T08:03:44.167534Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.622491359710693\nIteration:  100 loss:  2.6574392318725586\nIteration:  200 loss:  2.5600504875183105\nIteration:  300 loss:  2.498896360397339\nIteration:  400 loss:  2.5022151470184326\nIteration:  500 loss:  2.43200945854187\nIteration:  600 loss:  2.385399580001831\nIteration:  700 loss:  2.305583953857422\nIteration:  800 loss:  2.2335081100463867\nIteration:  900 loss:  2.155430555343628\nIteration:  1000 loss:  2.1502156257629395\nIteration:  1100 loss:  2.1165213584899902\nIteration:  1200 loss:  2.259932518005371\nIteration:  1300 loss:  2.1156651973724365\nIteration:  1400 loss:  2.1335630416870117\nIteration:  1500 loss:  2.196894884109497\nIteration:  1600 loss:  2.0143141746520996\nIteration:  1700 loss:  1.992996096611023\nIteration:  1800 loss:  2.096079111099243\nIteration:  1900 loss:  2.145651340484619\nIteration:  2000 loss:  1.871450662612915\nIteration:  2100 loss:  2.017768621444702\nIteration:  2200 loss:  1.9788874387741089\nIteration:  2300 loss:  1.9920037984848022\nIteration:  2400 loss:  2.0372166633605957\nIteration:  2500 loss:  1.9877288341522217\nIteration:  2600 loss:  1.9747105836868286\nIteration:  2700 loss:  2.072227716445923\nIteration:  2800 loss:  1.9329115152359009\nIteration:  2900 loss:  1.9173219203948975\nIteration:  3000 loss:  1.9117246866226196\nIteration:  3100 loss:  2.008445978164673\nIteration:  3200 loss:  2.0448484420776367\nIteration:  3300 loss:  1.99490225315094\nIteration:  3400 loss:  1.9094548225402832\nIteration:  3500 loss:  2.0252256393432617\nIteration:  3600 loss:  1.8603864908218384\nIteration:  3700 loss:  2.0056400299072266\nIteration:  3800 loss:  1.9727542400360107\nIteration:  3900 loss:  1.885459542274475\nIteration:  4000 loss:  1.9531701803207397\nIteration:  4100 loss:  1.9612438678741455\nIteration:  4200 loss:  1.8824858665466309\nIteration:  4300 loss:  2.0455470085144043\nIteration:  4400 loss:  1.9108511209487915\nIteration:  4500 loss:  1.9582326412200928\nIteration:  4600 loss:  1.8776923418045044\nIteration:  4700 loss:  1.9432103633880615\nIteration:  4800 loss:  1.8479963541030884\nIteration:  4900 loss:  1.761723518371582\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nsequence_length=32\nwith torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate \n    start_char = \"\\n\"\n    start_token = torch.tensor(encode(start_char)).unsqueeze(0)\n    print(\"start_token shape: \", start_token.shape)\n    current_token = start_token.to(device) # (B, T)\n    generated_tokens = []\n    for x in range(1000):\n        idx = current_token[:, -sequence_length:]\n        logits = model(idx) # (B, T, C)\n        logits = logits[:, -1, :] # (B, C)\n        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n        generated_tokens.append(next_token.item())\n        current_token = torch.cat((current_token, next_token), dim=1).to(device) # (B, T+1)\n    print(decode(generated_tokens))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:03:44.170160Z","iopub.execute_input":"2023-07-02T08:03:44.170530Z","iopub.status.idle":"2023-07-02T08:03:49.674050Z","shell.execute_reply.started":"2023-07-02T08:03:44.170498Z","shell.execute_reply":"2023-07-02T08:03:49.672924Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"start_token shape:  torch.Size([1, 1])\nNot EDWARD:\nIt them not do suck of Clannoter,\nTo'ruke you! grear clove; follse himplany,\nAnd the look flow; neve on the king.\n\nKING HARG RICHARD HARD III:\nA have way With the pareman, good our another?\n\nJOHESTER:\nNow, and esshalls the pleseminticklies xear thee tay.\n\nJULIUS:\nNaw, we fror of I she 'sham then regew and to werry our have crip of Pelsomes\nWhich bishard.\n\nBUCKINGHARD IICHARD IING HENCEL:\nYou.\n\nLORDIOLLA:\nMy the not taland slay\nNullshoo to the mone him struke and prack, whomh with have may the dune.\n\nVICKIOHNRS:\nThe deglancepter, men me lock,\nThy flace as pleeminer such, like.\n\nCLARENCE:\nShall,\nOn priancer, pleapple you seef'\nbettless.\n\nGROMESCERLESS:\nNeck; I, the grall with my musy Coulicgued,\nI hearts tire the plueles; but fatanter such more land priviaple\nAp thich'd thy foll fladiour 'tzeding prom eyon.\n\nPARINCE:\nThen this band the dircuter to mard;\nAnd a--ut way well by ony my can my from fidsink,\nSo with this careath. Sind not to manter is peace, My depot denaring him,\n\n","output_type":"stream"}]},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, vocab_size, sequence_length, embed_dim, n_blocks, n_heads, drop_p):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n        self.pos_embedding = torch.nn.Embedding(sequence_length, embed_dim)\n        self.encoder_blocks = nn.Sequential(*([EncoderBlock(sequence_length, embed_dim, n_heads, drop_p) for x in range(n_blocks)]))\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        self.linear_head = nn.Linear(embed_dim, vocab_size)\n        \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        B, T = x.shape\n        \n        token_embeddings = self.token_embedding(x) # (B, T, C)\n        positional_embeddings = self.pos_embedding(torch.arange(T, device=device)) # (T, C)\n        embeddings = token_embeddings + positional_embeddings # B, T, C cause broadcasting\n        feature_maps = self.encoder_blocks(embeddings)\n        x = self.layer_norm(feature_maps)\n        x = self.linear_head(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:03:49.679843Z","iopub.execute_input":"2023-07-02T08:03:49.680158Z","iopub.status.idle":"2023-07-02T08:03:49.688868Z","shell.execute_reply.started":"2023-07-02T08:03:49.680134Z","shell.execute_reply":"2023-07-02T08:03:49.687801Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=65, sequence_length=32, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:03:49.690384Z","iopub.execute_input":"2023-07-02T08:03:49.691074Z","iopub.status.idle":"2023-07-02T08:03:49.717051Z","shell.execute_reply.started":"2023-07-02T08:03:49.691041Z","shell.execute_reply":"2023-07-02T08:03:49.716132Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:03:49.718502Z","iopub.execute_input":"2023-07-02T08:03:49.718851Z","iopub.status.idle":"2023-07-02T08:05:31.883568Z","shell.execute_reply.started":"2023-07-02T08:03:49.718820Z","shell.execute_reply":"2023-07-02T08:05:31.882445Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  4.286431789398193\nIteration:  100 loss:  2.704097032546997\nIteration:  200 loss:  2.515896797180176\nIteration:  300 loss:  2.4800031185150146\nIteration:  400 loss:  2.472038507461548\nIteration:  500 loss:  2.458101511001587\nIteration:  600 loss:  2.2921974658966064\nIteration:  700 loss:  2.179999589920044\nIteration:  800 loss:  2.304443597793579\nIteration:  900 loss:  2.2007734775543213\nIteration:  1000 loss:  2.1956934928894043\nIteration:  1100 loss:  2.212348699569702\nIteration:  1200 loss:  2.193769693374634\nIteration:  1300 loss:  2.1309947967529297\nIteration:  1400 loss:  2.2613184452056885\nIteration:  1500 loss:  2.077021360397339\nIteration:  1600 loss:  2.1280646324157715\nIteration:  1700 loss:  2.073899745941162\nIteration:  1800 loss:  1.98118257522583\nIteration:  1900 loss:  1.9331833124160767\nIteration:  2000 loss:  2.0193066596984863\nIteration:  2100 loss:  2.0706899166107178\nIteration:  2200 loss:  2.0610713958740234\nIteration:  2300 loss:  2.07271146774292\nIteration:  2400 loss:  2.1123175621032715\nIteration:  2500 loss:  2.0320565700531006\nIteration:  2600 loss:  1.9820728302001953\nIteration:  2700 loss:  1.9410699605941772\nIteration:  2800 loss:  1.9755966663360596\nIteration:  2900 loss:  2.0240163803100586\nIteration:  3000 loss:  2.0156214237213135\nIteration:  3100 loss:  1.9542815685272217\nIteration:  3200 loss:  1.866485357284546\nIteration:  3300 loss:  1.8228013515472412\nIteration:  3400 loss:  1.933199167251587\nIteration:  3500 loss:  1.915153980255127\nIteration:  3600 loss:  1.9359809160232544\nIteration:  3700 loss:  1.9158762693405151\nIteration:  3800 loss:  1.8502135276794434\nIteration:  3900 loss:  2.0217952728271484\nIteration:  4000 loss:  1.7834492921829224\nIteration:  4100 loss:  1.9654991626739502\nIteration:  4200 loss:  1.9674961566925049\nIteration:  4300 loss:  1.8386353254318237\nIteration:  4400 loss:  1.9029500484466553\nIteration:  4500 loss:  1.8300886154174805\nIteration:  4600 loss:  1.935315489768982\nIteration:  4700 loss:  1.8864437341690063\nIteration:  4800 loss:  2.006396770477295\nIteration:  4900 loss:  1.9226984977722168\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nsequence_length=32\nwith torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate \n    start_char = \"\\n\"\n    start_token = torch.tensor(encode(start_char)).unsqueeze(0)\n    print(\"start_token shape: \", start_token.shape)\n    current_token = start_token.to(device) # (B, T)\n    generated_tokens = []\n    for x in range(1000):\n        idx = current_token[:, -sequence_length:]\n        logits = model(idx) # (B, T, C)\n        logits = logits[:, -1, :] # (B, C)\n        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n        generated_tokens.append(next_token.item())\n        current_token = torch.cat((current_token, next_token), dim=1).to(device) # (B, T+1)\n    print(decode(generated_tokens))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:05:31.885019Z","iopub.execute_input":"2023-07-02T08:05:31.885378Z","iopub.status.idle":"2023-07-02T08:05:37.409843Z","shell.execute_reply.started":"2023-07-02T08:05:31.885347Z","shell.execute_reply":"2023-07-02T08:05:37.408888Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"start_token shape:  torch.Size([1, 1])\nClehind distady theke is to-no not's mine\nrown:\nAnd in thingung that an!\n\nKING RICHARD IIII:\nShink, Beasure os.\n\nVULIXESBY Caking Gempliine\nTo chestized hopk cordimicess are to bath\nThe no sway's or hate it?\n\nFLADY The\nTill nandlyse,\nThe his of you.\n\nLy PORIOLHY:\nI puppicion: shall to be doish our but thing so fullyy to exal. I with not genged\nShall see trentler in man his I mad:\nWho wince have ving treakengless I'll detted showon will you,----fast goned time yourself with my Riddrouseth of hroughers,\nWhich here deess upon heavy mighnily be know and merran.\n\nWARWICK:\nGarientles be that laid.\n\nLord:\nA me a do thear presuletch your madake,\nI do prastibon be drath, would ward pity-to\nAnd live--\n\nMERCUTIO:\nLord O say, with wife, he\nthe inve time for resing him with,\nAnd brespun undlihile, marrown, te vitidess\nRome one aron his cursteringes!\n\nKING EDWARD IV:\nRome caust, crirs of Etteling ell, for your lond: in his the critle,\nThat-sil in like do come\nMarged a dooks from me Edwer\nAnd be flie\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Use token embeddings from OpenAI: Tiktoken","metadata":{}},{"cell_type":"code","source":"data_dir = \"/kaggle/working\"\n\nwith open(os.path.join(data_dir, \"input.txt\"), encoding=\"utf8\") as f:\n    text = f.read()\ntrain_text = text[:int(len(text)*0.97)]\nval_text = text[int(len(text)*0.97):]\nprint(\"chars in train_data: \", len(train_text))\nprint(\"chars in val_data: \", len(val_text))\n\nenc = tiktoken.get_encoding(\"gpt2\")\ntrain_data = enc.encode_ordinary(train_text)\nval_data = enc.encode_ordinary(val_text)\n\nprint(\"tokens in train dataset: \", len(train_data))\nprint(\"tokens in validaiton dataset: \", len(val_data))\n\ntrain_set = set(train_data)\nval_set = set(val_data)\n\nvocab_size = len(train_set.union(val_set))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:05:37.411251Z","iopub.execute_input":"2023-07-02T08:05:37.411704Z","iopub.status.idle":"2023-07-02T08:05:41.378052Z","shell.execute_reply.started":"2023-07-02T08:05:37.411668Z","shell.execute_reply":"2023-07-02T08:05:41.377013Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"chars in train_data:  1081932\nchars in val_data:  33462\ntokens in train dataset:  327428\ntokens in validaiton dataset:  10598\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data, val_data = np.array(train_data), np.array(val_data)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:05:41.379530Z","iopub.execute_input":"2023-07-02T08:05:41.380142Z","iopub.status.idle":"2023-07-02T08:05:41.431745Z","shell.execute_reply.started":"2023-07-02T08:05:41.380107Z","shell.execute_reply":"2023-07-02T08:05:41.430498Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"vocab_size = max(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:05:41.433307Z","iopub.execute_input":"2023-07-02T08:05:41.433725Z","iopub.status.idle":"2023-07-02T08:05:41.485552Z","shell.execute_reply.started":"2023-07-02T08:05:41.433692Z","shell.execute_reply":"2023-07-02T08:05:41.484562Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:05:41.487232Z","iopub.execute_input":"2023-07-02T08:05:41.487562Z","iopub.status.idle":"2023-07-02T08:05:41.499995Z","shell.execute_reply.started":"2023-07-02T08:05:41.487532Z","shell.execute_reply":"2023-07-02T08:05:41.499079Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"50255"},"metadata":{}}]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, embed_dim, drop_p):\n        super().__init__()\n        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)\n        self.dropout = nn.Dropout(drop_p)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n#         x = self.act(x)\n        x = self.dropout(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:05:41.501575Z","iopub.execute_input":"2023-07-02T08:05:41.502084Z","iopub.status.idle":"2023-07-02T08:05:41.509778Z","shell.execute_reply.started":"2023-07-02T08:05:41.502047Z","shell.execute_reply":"2023-07-02T08:05:41.508829Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=vocab_size + 1, sequence_length=32, embed_dim=64, n_blocks=4, n_heads=4, drop_p=0.2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:05:41.511068Z","iopub.execute_input":"2023-07-02T08:05:41.511625Z","iopub.status.idle":"2023-07-02T08:05:41.599151Z","shell.execute_reply.started":"2023-07-02T08:05:41.511592Z","shell.execute_reply":"2023-07-02T08:05:41.598185Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:05:41.600679Z","iopub.execute_input":"2023-07-02T08:05:41.601291Z","iopub.status.idle":"2023-07-02T08:07:24.076049Z","shell.execute_reply.started":"2023-07-02T08:05:41.601257Z","shell.execute_reply":"2023-07-02T08:07:24.075085Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"Iteration:  0 loss:  11.025362014770508\nIteration:  100 loss:  6.364414215087891\nIteration:  200 loss:  6.1271257400512695\nIteration:  300 loss:  5.743419170379639\nIteration:  400 loss:  5.472109317779541\nIteration:  500 loss:  5.344599723815918\nIteration:  600 loss:  5.3121466636657715\nIteration:  700 loss:  5.006277561187744\nIteration:  800 loss:  5.064224720001221\nIteration:  900 loss:  4.776893138885498\nIteration:  1000 loss:  5.109827041625977\nIteration:  1100 loss:  4.7790398597717285\nIteration:  1200 loss:  5.040238380432129\nIteration:  1300 loss:  4.6726975440979\nIteration:  1400 loss:  4.334329128265381\nIteration:  1500 loss:  4.477770805358887\nIteration:  1600 loss:  4.788212776184082\nIteration:  1700 loss:  4.799291610717773\nIteration:  1800 loss:  4.721064567565918\nIteration:  1900 loss:  4.353370189666748\nIteration:  2000 loss:  4.06364107131958\nIteration:  2100 loss:  4.7813401222229\nIteration:  2200 loss:  4.274487018585205\nIteration:  2300 loss:  4.4089484214782715\nIteration:  2400 loss:  4.375737190246582\nIteration:  2500 loss:  4.452601909637451\nIteration:  2600 loss:  3.964451789855957\nIteration:  2700 loss:  3.9713544845581055\nIteration:  2800 loss:  4.454739570617676\nIteration:  2900 loss:  4.521622657775879\nIteration:  3000 loss:  3.7271859645843506\nIteration:  3100 loss:  4.160904407501221\nIteration:  3200 loss:  4.479092597961426\nIteration:  3300 loss:  4.458722114562988\nIteration:  3400 loss:  4.193047046661377\nIteration:  3500 loss:  4.244678020477295\nIteration:  3600 loss:  3.8248233795166016\nIteration:  3700 loss:  4.080216884613037\nIteration:  3800 loss:  4.221535682678223\nIteration:  3900 loss:  4.36078405380249\nIteration:  4000 loss:  3.7614147663116455\nIteration:  4100 loss:  4.2072248458862305\nIteration:  4200 loss:  4.195106506347656\nIteration:  4300 loss:  4.087475776672363\nIteration:  4400 loss:  4.126639366149902\nIteration:  4500 loss:  3.8581504821777344\nIteration:  4600 loss:  4.246342182159424\nIteration:  4700 loss:  4.164571285247803\nIteration:  4800 loss:  4.159162998199463\nIteration:  4900 loss:  3.7932021617889404\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nsequence_length=32\nwith torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate \n    start_char = \"\\n\"\n    start_token = torch.tensor(encode(start_char)).unsqueeze(0)\n    print(\"start_token shape: \", start_token.shape)\n    current_token = start_token.to(device) # (B, T)\n    generated_tokens = []\n    for x in range(1000):\n        idx = current_token[:, -sequence_length:]\n        logits = model(idx) # (B, T, C)\n        logits = logits[:, -1, :] # (B, C)\n        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n        generated_tokens.append(next_token.item())\n        current_token = torch.cat((current_token, next_token), dim=1).to(device) # (B, T+1)\n    print(enc.decode(generated_tokens))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T08:07:24.077781Z","iopub.execute_input":"2023-07-02T08:07:24.078164Z","iopub.status.idle":"2023-07-02T08:07:29.686302Z","shell.execute_reply.started":"2023-07-02T08:07:24.078133Z","shell.execute_reply":"2023-07-02T08:07:29.685322Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"start_token shape:  torch.Size([1, 1])\n\nThou v goose and talk of heart King fair maid.\n\nHASTINGS:\nMaking so him suffer down shall be from your good thing\nOf your grace. For, we can hear you should not.\n\nBUCKINGHAM:\nBut what news should nothing but even on.\nThis is the paper come.\nThe equal, thou art, good father's life, neither,\nAnd maiding whom thou ne'er is no more betpart.\n\nFOLANUS:\nMarg here comes he amen.\n\nCURTIS:\nI'll not spoke for us.\n\nMONTAGUE:\nThou must'll feel?\n\nSecond Keeper:\nWhy, ay, if you had a man, mother!\nConjectwest that goes my vow, put to heaven not pass.\n\nLADY ANNE:\nI pray, thou art thou hast,\nyou say, thou shalt be he chide out your warrant;\nFor it is the champions give hungry came to them.\n\nCORIOLANUS:\nI stay she are gis; retired, is the light;\nBut that's substitute depart.\nHath he will not so apt 'This under the other scope,\nThis is not at least, hath prepared'd from himself: you might see\nI remember as never pass'd! who did make the king.\n\nPOMPEY:\nOfom, is thee, madam,\nWould sour true, I cannot draw me back thy pains.\n\nJULIET:\nThou'er devil, a day's death:\nMere matter, my lord mark me, you'll meet\nsomething in order of tool, that awaken us,\nI'll not love him: what the blood is your lad\nbruteibly where! Make men?\n\nBUCKINGHAM:\nThou art a either my bail, that he were wis a thorn.\n\nLUCIO:\nI tell him not a world in come of half,\nAy, and bold stuff that fatal rego up with a gas:\nUpon this intelligence, I am: king, having good lucker,\nAnd the enemy.\nThou art thou delight, being the slave, some-et hence,\nMy Lord I know it well bed:\nTo the gall were a while that was famed\nWith all thy body,-- guessings! to the world myself\nisement by the rest?' Marcius is the\nTo-penancehes, who never lift blood you point\nWhich makes to speak,--Hook, grieves she never\nt so, all a right seated joy. If we are\nfranchub and at all estates\nIn wars unpvesof mine.\n\nCOMINIUS:\nContent,\nLet; but so he hath known to beat,\nAnd for I did seem in quiet to command that does.\n\nEDWARD:\nMy lord, and my hundred, indeed;\nI prit so be me mistrust himself asable'd\nAs ne'er which the touch'd of a day,\nAnd then is mine honour again by the foot,\nCould most noted to die: all your newbury.\n\nSant:\nAlack, I'll see thee as nobly to fear him,\nWhy would thy condemned a blood to my sweet more.\n\nISABELLA:\nWhy, leisure! slaughter you news: you, dark's.\n\nMONTAGUE:\nFather, let me crown, you the crown; 'tis the thing and paper?\nGILIA:\nI have not; like my good father.\nRomeo of your mouth, Mars, give me all your brotherhood.\nMy Lord Northumberland! with suitor haughty duck! away, sir, this letter!\nWho shall not yet as I, throw your cherish bald.\n\nANGELO:\nICK, keep this be king!\n\nNurse:\nBut fortune, heavy, very foot saloke,\nYour brother's quaint out superst-castle, and Clarence,\nForbear upon my heart and Clarence when he should\nanyounter, hath thou l disposition.\n\nLADY ANNE:\nThat, that indeed so, Camillo it\nTo read my good mother before with death.\n\nPETRUCHIO:\nThou proclamation shall you prove withal.\nSay my lord, name, take their terming this?\nBrother not that thou, a very simple mockery upon up\nI'll have are shadows, and a holy tale to the light-ard of yourining.\n\nROMEO:\n\nKING RICHARD III:\nWhat is my master\ncose begins, at thee\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train on a subset of OpenWebText","metadata":{}},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:01:29.719797Z","iopub.execute_input":"2023-07-01T05:01:29.720211Z","iopub.status.idle":"2023-07-01T05:01:42.212255Z","shell.execute_reply.started":"2023-07-01T05:01:29.720180Z","shell.execute_reply":"2023-07-01T05:01:42.211072Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting tiktoken\n  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.5.5)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.28.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\nInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport tiktoken\nimport numpy as np\nimport os\nfrom torch import nn","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:01:42.215560Z","iopub.execute_input":"2023-07-01T05:01:42.215982Z","iopub.status.idle":"2023-07-01T05:01:43.877563Z","shell.execute_reply.started":"2023-07-01T05:01:42.215940Z","shell.execute_reply":"2023-07-01T05:01:43.876644Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:01:43.879034Z","iopub.execute_input":"2023-07-01T05:01:43.879638Z","iopub.status.idle":"2023-07-01T05:01:43.912704Z","shell.execute_reply.started":"2023-07-01T05:01:43.879593Z","shell.execute_reply":"2023-07-01T05:01:43.911716Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, sequence_length, embed_dim, head_size, drop_p):\n        super().__init__()\n        self.q = nn.Linear(embed_dim, head_size, bias=False)\n        self.k = nn.Linear(embed_dim, head_size, bias=False)\n        self.v = nn.Linear(embed_dim, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(sequence_length, sequence_length)))\n        self.dropout = nn.Dropout(drop_p)\n        self.head_size = head_size\n        \n    def forward(self, x):\n        # x is (B, T, C)\n        B, T, C = x.shape\n        query = self.q(x)\n        key = self.k(x)\n        value = self.v(x)\n#         key.permute(0, -1, -2)\n        attention_map = query @ key.transpose(-2, -1) * self.head_size**-0.5 # (B, T, C) @ (B, C, T) = (B, T, T)\n        masked_attention_map = attention_map.masked_fill(self.tril[:T, :T] == 0, -np.Inf)\n        masked_attention_map = F.softmax(masked_attention_map, dim=-1)\n        attention_map = self.dropout(masked_attention_map)\n        feature_map = attention_map @ value # (B, T, T) @ (B, T, C) = (B, T, C)\n        \n        return feature_map\n","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:01:43.915864Z","iopub.execute_input":"2023-07-01T05:01:43.916216Z","iopub.status.idle":"2023-07-01T05:01:43.926163Z","shell.execute_reply.started":"2023-07-01T05:01:43.916190Z","shell.execute_reply":"2023-07-01T05:01:43.925210Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n        super().__init__()\n        head_size = embed_dim // n_heads\n        self.heads = nn.ModuleList([SelfAttention(sequence_length, embed_dim, head_size, drop_p) for x in range(n_heads)])\n        self.mlp = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(drop_p)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.mlp(out)\n        out = self.dropout(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:01:43.927605Z","iopub.execute_input":"2023-07-01T05:01:43.928132Z","iopub.status.idle":"2023-07-01T05:01:43.938361Z","shell.execute_reply.started":"2023-07-01T05:01:43.928093Z","shell.execute_reply":"2023-07-01T05:01:43.937485Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, embed_dim, drop_p):\n        super().__init__()\n        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)\n        self.dropout = nn.Dropout(drop_p)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n#         x = self.act(x)\n        x = self.dropout(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:01:43.939876Z","iopub.execute_input":"2023-07-01T05:01:43.940227Z","iopub.status.idle":"2023-07-01T05:01:43.948742Z","shell.execute_reply.started":"2023-07-01T05:01:43.940196Z","shell.execute_reply":"2023-07-01T05:01:43.947814Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n        super().__init__()\n        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        self.mlp = MLP(embed_dim, drop_p)\n    \n    def forward(self, x):\n        x = x + self.multi_head_self_attention(self.layer_norm(x))\n        x = x + self.mlp(self.layer_norm(x))\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:01:43.950324Z","iopub.execute_input":"2023-07-01T05:01:43.950721Z","iopub.status.idle":"2023-07-01T05:01:43.960951Z","shell.execute_reply.started":"2023-07-01T05:01:43.950689Z","shell.execute_reply":"2023-07-01T05:01:43.960154Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class LanguageModel(torch.nn.Module):\n    def __init__(self, vocab_size, sequence_length, embed_dim, n_blocks, n_heads, drop_p):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n        self.pos_embedding = torch.nn.Embedding(sequence_length, embed_dim)\n        self.encoder_blocks = nn.Sequential(*([EncoderBlock(sequence_length, embed_dim, n_heads, drop_p) for x in range(n_blocks)]))\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        self.linear_head = nn.Linear(embed_dim, vocab_size)\n        \n    def forward(self, x):\n        \"\"\"\n        Attributes\n        __________________\n        x.shape: (B, T)\n        \n        Returns\n        _________________\n        logits.shape: (B, T, C), where C= vocab size\n        \"\"\"\n        B, T = x.shape\n        \n        token_embeddings = self.token_embedding(x) # (B, T, C)\n        positional_embeddings = self.pos_embedding(torch.arange(T, device=device)) # (T, C)\n        embeddings = token_embeddings + positional_embeddings # B, T, C cause broadcasting\n        feature_maps = self.encoder_blocks(embeddings)\n        x = self.layer_norm(feature_maps)\n        x = self.linear_head(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:01:43.962444Z","iopub.execute_input":"2023-07-01T05:01:43.962769Z","iopub.status.idle":"2023-07-01T05:01:43.973047Z","shell.execute_reply.started":"2023-07-01T05:01:43.962739Z","shell.execute_reply":"2023-07-01T05:01:43.971922Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/openwebtext-subset-20\"\n\ntrain_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\nval_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n\nvocab_size = max(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:01:43.975107Z","iopub.execute_input":"2023-07-01T05:01:43.975578Z","iopub.status.idle":"2023-07-01T05:06:53.983917Z","shell.execute_reply.started":"2023-07-01T05:01:43.975547Z","shell.execute_reply":"2023-07-01T05:06:53.982637Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size=vocab_size + 1, sequence_length=512, embed_dim=768, n_blocks=6, n_heads=6, drop_p=0.1).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:06:53.988600Z","iopub.execute_input":"2023-07-01T05:06:53.989023Z","iopub.status.idle":"2023-07-01T05:06:58.080895Z","shell.execute_reply.started":"2023-07-01T05:06:53.988966Z","shell.execute_reply":"2023-07-01T05:06:58.079872Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"os.path.join(data_dir, \"best_checkpoint.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:08:01.459957Z","iopub.execute_input":"2023-07-01T05:08:01.460356Z","iopub.status.idle":"2023-07-01T05:08:01.470266Z","shell.execute_reply.started":"2023-07-01T05:08:01.460326Z","shell.execute_reply":"2023-07-01T05:08:01.469338Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/openwebtext-subset-20/best_checkpoint.pth'"},"metadata":{}}]},{"cell_type":"code","source":"model.load_state_dict(os.path.join(data_dir, \"best_checkpoint.pth\"))","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:07:40.580375Z","iopub.execute_input":"2023-07-01T05:07:40.580814Z","iopub.status.idle":"2023-07-01T05:07:40.635174Z","shell.execute_reply.started":"2023-07-01T05:07:40.580778Z","shell.execute_reply":"2023-07-01T05:07:40.633990Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_checkpoint.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1994\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copies parameters and buffers from :attr:`state_dict` into\u001b[39;00m\n\u001b[1;32m   1972\u001b[0m \u001b[38;5;124;03mthis module and its descendants. If :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[1;32m   1973\u001b[0m \u001b[38;5;124;03mthe keys of :attr:`state_dict` must exactly match the keys returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 1994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(state_dict)))\n\u001b[1;32m   1996\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1997\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'str'>."],"ename":"TypeError","evalue":"Expected state_dict to be dict-like, got <class 'str'>.","output_type":"error"}]},{"cell_type":"code","source":"val_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:08:07.402437Z","iopub.execute_input":"2023-07-01T05:08:07.402809Z","iopub.status.idle":"2023-07-01T05:08:07.408889Z","shell.execute_reply.started":"2023-07-01T05:08:07.402777Z","shell.execute_reply":"2023-07-01T05:08:07.407963Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(12885552,)"},"metadata":{}}]},{"cell_type":"code","source":"def get_batch(split, sequence_length, batch_size):\n    if split == \"train\":\n        dataset = train_data\n    else:\n        dataset = val_data\n    random_numbers = torch.randint(0, len(dataset) - sequence_length, (batch_size,))\n    data = torch.stack([torch.from_numpy(dataset[random_number: random_number + sequence_length].astype(np.int64)) for random_number in random_numbers])\n    labels = torch.stack([torch.from_numpy(dataset[random_number + 1: random_number + sequence_length + 1].astype(np.int64)) for random_number in random_numbers])\n    return data, labels","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:08:09.814298Z","iopub.execute_input":"2023-07-01T05:08:09.814761Z","iopub.status.idle":"2023-07-01T05:08:09.825513Z","shell.execute_reply.started":"2023-07-01T05:08:09.814722Z","shell.execute_reply":"2023-07-01T05:08:09.824521Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"eval_iters = 100\ndef evaluate():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            data, labels = get_batch(split, 512, 16)\n            data = data.to(device)\n            labels = data.to(device)\n            logits = model(data)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            labels = labels.view(B*T)\n            loss = F.cross_entropy(logits, labels)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:08:12.152297Z","iopub.execute_input":"2023-07-01T05:08:12.152663Z","iopub.status.idle":"2023-07-01T05:08:12.160211Z","shell.execute_reply.started":"2023-07-01T05:08:12.152630Z","shell.execute_reply":"2023-07-01T05:08:12.159040Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train():\n    min_val_loss = np.Inf\n    for iteration in range(700000):\n        if iteration % 500 == 0:\n            losses = evaluate()\n            print(f\"step {iteration}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n            if losses[\"val\"] < min_val_loss:\n                min_val_loss = losses[\"val\"]\n                torch.save(model.state_dict(), os.path.join(\"/kaggle/working\", f\"best_checkpoint.pth\"))\n        data, labels = get_batch(\"train\", 512, 16)\n        data, labels = data.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logits = model(data)\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        labels = labels.view(B*T)\n        loss = F.cross_entropy(logits, labels)\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:08:12.576609Z","iopub.execute_input":"2023-07-01T05:08:12.576965Z","iopub.status.idle":"2023-07-01T05:08:12.584774Z","shell.execute_reply.started":"2023-07-01T05:08:12.576937Z","shell.execute_reply":"2023-07-01T05:08:12.583778Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T05:08:13.031515Z","iopub.execute_input":"2023-07-01T05:08:13.032241Z","iopub.status.idle":"2023-07-01T15:56:08.745687Z","shell.execute_reply.started":"2023-07-01T05:08:13.032194Z","shell.execute_reply":"2023-07-01T15:56:08.744368Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"step 0: train loss 11.1205, val loss 11.1206\nstep 500: train loss 5.5682, val loss 5.6956\nstep 1000: train loss 5.8837, val loss 5.6302\nstep 1500: train loss 5.6791, val loss 5.7529\nstep 2000: train loss 5.7770, val loss 5.7086\nstep 2500: train loss 5.9406, val loss 5.9721\nstep 3000: train loss 5.7866, val loss 5.9718\nstep 3500: train loss 5.9370, val loss 5.9669\nstep 4000: train loss 6.0417, val loss 5.9601\nstep 4500: train loss 6.1591, val loss 6.0797\nstep 5000: train loss 6.0856, val loss 6.0366\nstep 5500: train loss 5.9664, val loss 6.0845\nstep 6000: train loss 6.0808, val loss 6.3655\nstep 6500: train loss 6.0679, val loss 6.1373\nstep 7000: train loss 6.0773, val loss 6.1683\nstep 7500: train loss 6.1455, val loss 6.1742\nstep 8000: train loss 6.0446, val loss 6.1076\nstep 8500: train loss 6.4296, val loss 6.1427\nstep 9000: train loss 6.1211, val loss 6.4866\nstep 9500: train loss 6.4034, val loss 6.2373\nstep 10000: train loss 6.4121, val loss 6.4991\nstep 10500: train loss 6.1764, val loss 6.5570\nstep 11000: train loss 6.3700, val loss 6.3286\nstep 11500: train loss 6.4545, val loss 6.3639\nstep 12000: train loss 6.4398, val loss 6.5670\nstep 12500: train loss 6.4841, val loss 6.4767\nstep 13000: train loss 6.4913, val loss 6.2875\nstep 13500: train loss 6.4920, val loss 6.5710\nstep 14000: train loss 6.3974, val loss 6.4338\nstep 14500: train loss 6.5943, val loss 6.3983\nstep 15000: train loss 6.4459, val loss 6.6690\nstep 15500: train loss 6.6768, val loss 6.2907\nstep 16000: train loss 6.3828, val loss 6.5992\nstep 16500: train loss 6.4810, val loss 6.4342\nstep 17000: train loss 6.4051, val loss 6.5527\nstep 17500: train loss 6.7868, val loss 6.5877\nstep 18000: train loss 6.5740, val loss 6.4397\nstep 18500: train loss 6.3169, val loss 6.3720\nstep 19000: train loss 6.5429, val loss 6.4375\nstep 19500: train loss 6.5489, val loss 6.5588\nstep 20000: train loss 6.7413, val loss 6.3575\nstep 20500: train loss 6.5188, val loss 6.4423\nstep 21000: train loss 6.4790, val loss 6.6569\nstep 21500: train loss 6.4286, val loss 6.6849\nstep 22000: train loss 6.5780, val loss 6.7038\nstep 22500: train loss 6.6107, val loss 6.5476\nstep 23000: train loss 6.5164, val loss 6.5398\nstep 23500: train loss 6.3558, val loss 6.4462\nstep 24000: train loss 6.5160, val loss 6.6539\nstep 24500: train loss 6.5674, val loss 6.7258\nstep 25000: train loss 6.5717, val loss 6.4969\nstep 25500: train loss 6.6170, val loss 6.5999\nstep 26000: train loss 6.5648, val loss 6.8594\nstep 26500: train loss 6.4947, val loss 6.6411\nstep 27000: train loss 6.7433, val loss 6.6552\nstep 27500: train loss 6.7092, val loss 6.5918\nstep 28000: train loss 6.5999, val loss 6.6540\nstep 28500: train loss 6.5533, val loss 6.7075\nstep 29000: train loss 6.5029, val loss 6.6908\nstep 29500: train loss 6.6680, val loss 6.5573\nstep 30000: train loss 6.7218, val loss 6.6501\nstep 30500: train loss 6.7028, val loss 6.4009\nstep 31000: train loss 6.5150, val loss 6.4720\nstep 31500: train loss 6.7446, val loss 6.4466\nstep 32000: train loss 6.4963, val loss 6.5445\nstep 32500: train loss 6.4879, val loss 6.5360\nstep 33000: train loss 6.7114, val loss 6.6882\nstep 33500: train loss 6.6246, val loss 6.7101\nstep 34000: train loss 6.6162, val loss 6.6953\nstep 34500: train loss 6.7844, val loss 6.7474\nstep 35000: train loss 6.8140, val loss 6.5698\nstep 35500: train loss 6.6491, val loss 6.6029\nstep 36000: train loss 6.6448, val loss 6.6241\nstep 36500: train loss 6.6383, val loss 6.4612\nstep 37000: train loss 6.6060, val loss 6.6481\nstep 37500: train loss 6.6730, val loss 6.6359\nstep 38000: train loss 6.7289, val loss 6.7214\nstep 38500: train loss 6.6134, val loss 6.7302\nstep 39000: train loss 6.7059, val loss 6.7744\nstep 39500: train loss 6.7824, val loss 6.6205\nstep 40000: train loss 6.7398, val loss 6.8232\nstep 40500: train loss 6.5742, val loss 6.6476\nstep 41000: train loss 6.5529, val loss 6.5688\nstep 41500: train loss 6.7225, val loss 6.6579\nstep 42000: train loss 6.6629, val loss 6.6912\nstep 42500: train loss 6.7417, val loss 6.7601\nstep 43000: train loss 6.6481, val loss 6.7588\nstep 43500: train loss 6.7580, val loss 6.5742\nstep 44000: train loss 6.7080, val loss 6.7364\nstep 44500: train loss 6.6401, val loss 6.6391\nstep 45000: train loss 6.5862, val loss 6.7942\nstep 45500: train loss 6.7585, val loss 6.8213\nstep 46000: train loss 6.8539, val loss 6.6330\nstep 46500: train loss 6.7505, val loss 6.6980\nstep 47000: train loss 6.6159, val loss 6.6612\nstep 47500: train loss 6.7725, val loss 6.6193\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[17], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m700000\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m min_val_loss:\n","Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT)\n\u001b[1;32m     15\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, labels)\n\u001b[0;32m---> 16\u001b[0m         losses[k] \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.training","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:56:18.232118Z","iopub.execute_input":"2023-07-01T15:56:18.233150Z","iopub.status.idle":"2023-07-01T15:56:18.241088Z","shell.execute_reply.started":"2023-07-01T15:56:18.233107Z","shell.execute_reply":"2023-07-01T15:56:18.239981Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"enc = tiktoken.get_encoding(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:59:51.060802Z","iopub.execute_input":"2023-07-01T15:59:51.061184Z","iopub.status.idle":"2023-07-01T15:59:52.748871Z","shell.execute_reply.started":"2023-07-01T15:59:51.061154Z","shell.execute_reply":"2023-07-01T15:59:52.747873Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model.eval()\nsequence_length=512\nwith torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate \n    start_char = \"Once upon a time, there were a prince and a princess who loved eachother a lot, but\"\n    start_token = torch.tensor(enc.encode_ordinary(start_char)).unsqueeze(0)\n    print(\"start_token shape: \", start_token.shape)\n    current_token = start_token.to(device) # (B, T)\n    generated_tokens = []\n    for x in range(1000):\n        idx = current_token[:, -sequence_length:]\n        logits = model(idx) # (B, T, C)\n        logits = logits[:, -1, :] # (B, C)\n        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n        generated_tokens.append(next_token.item())\n        current_token = torch.cat((current_token, next_token), dim=1).to(device) # (B, T+1)\n    print(enc.decode(generated_tokens))","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:05:57.747730Z","iopub.execute_input":"2023-07-01T16:05:57.748633Z","iopub.status.idle":"2023-07-01T16:06:14.553926Z","shell.execute_reply.started":"2023-07-01T16:05:57.748596Z","shell.execute_reply":"2023-07-01T16:06:14.552278Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"start_token shape:  torch.Size([1, 20])\n most of us couldn’t tell them A.) you were so much better?” Bloor said.\n\n“She emailed me with a shovel, and I could call my Queen” Bloor said. \"Being vice president of the Royal Family Air Brigade is perfect, and is my coach of the Royal Family Air Brigade and there are fantastic individuals in Charles. They are the primesocialist of the Navy who is a large industry right now.”\n\nGeorge forced Vietnam to escape dangerous pathogens in shipworm, Naroul, Prince Heinz, and the Marine Fisheries Service. Bloor was expecting what Trump encouraged them to report on what he called “murdily” articles defending “fake news.”\n\nA quick attack by the Navy and American vessels has proven what Trump has learned about as a base of sophisticated squid silk cocoons and has fuelled jobs, as an important symbol for helping the empire better. Trump has nothing to do with these wars. Cavesturas famously signed up to be prime minister in January.\n\nMcGillibrand,\n\n“Most Krhegan says get along, compartmentalise was coined to describe Asian-style Bing: Sarah Besick; Professor Rousse Pearson opens in paperback\n\n“It involves digging, staying somewhere,” said Michael Tapper, an odd critic of the courage of an of Trump’s reelection campaign. “It flows back and forth until people were dissatisfied, oppressive right now. Buying from an ivory tower like this?”\n\nThe president looked surprised at what he had called “She could say ‘Trust me, stay her.' (She ordered Trump to flee negotiations to enter office this year.) “Hit up how there can be a special relationship between America, an empathetic presence and civilisation,” said Michael Tapper, who even spoke earlier today in this week’s Atlantic.\n\nADVERTISEMENT\n\nThe president implied that he had helped avoid the background, and says Trump’s second wanted embracing “cannon cult residences.” Wall Street insiders, hardliners and trivial ones are rushed to the government. Trump has both scaled back toward submarines, compared to Cold War 160 years ago, but Trump also thinks China should have named one country, whether Iran was a egg egg or an alien? If there were any suggestions that might very much be had in a movie for Trump years incompetence could be truly repugnant.\n\nSome of the president anxiously profiled James Sutter in such a horrifying experience, which he found refreshing, high-stakes golfing alternative lessons learned. According to the president of Australia, African-American men tended to stick in the sweat not just in their shops but also in their living rooms if they were very awesome. The president made golf clothes or in the shoes before the rapid equeton and other procedures on a steel pedestal. But when he got a lousy golf course, he became a chicken helpermaker.\n\nTrump has struck a country that has historically been communed over the decades — women’s status as men would likely as many as 45, or so — with no threat. But the president always refused to uphold the sexist notions of masculinity or masculinity.\n\nWhile president-elect Trump may be the most ardent major geopolitical force behind Trump, with almost to learn about “whatever black guy is small.”\n\nLike Trump himself, Republicans have an uncanny knack for moral course because they describe women as politically powerful than they thinks they’ve produced. The driving culture used to be unpopular in 1953,-, and brought to America by the right-wing media.\n\nSome and other leftists tried to run for reelection but did so.\n\nDespite Trump’s certainly late-afternoon economic crisis, Alabama lawmakers won by nominating Republican Senate candidate Roy McCaul in a landslide victory in Arizona. Even young voters joined him in Ohio. In 2009, McCaul won Montana, the GOP candidate who tended to dominate Mississippi.\n\nPresident Trump often pushed economic skills. In the former United States, he heard a sound cocky joke used by Liberals rallying to find someone she knew of during the same time in the Democratic National Convention.\n\nFor all of us, these callouts have allowed us to marathon economic activity, and Trump is ambivalent about partisan incumbency. These aren’t the only themes Trump surely made — women’s political leaders seem to be seen musical femas — even than women, but Obama is a surest Republican.\n\nWorkston responded on a Saturday afternoon, pulling a rally on NBC backlit, flat blocks with bump-in cups and Too often a simple elective governing strategy Trump relayed was his win probability — which will show up just four points — but the equation is more difficult.\n\nIs it any coincidence that both Trump\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), os.path.join(\"/kaggle/working\", f\"last_checkpoint.pth\"))","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:03:48.328244Z","iopub.execute_input":"2023-07-01T16:03:48.328620Z","iopub.status.idle":"2023-07-01T16:03:49.226902Z","shell.execute_reply.started":"2023-07-01T16:03:48.328591Z","shell.execute_reply":"2023-07-01T16:03:49.225919Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}