{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shakespeare Dataset"
      ],
      "metadata": {
        "id": "q7ofSOLX7YIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "wcgx1lfK9P3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:12.442082Z",
          "iopub.execute_input": "2023-07-02T07:53:12.442840Z",
          "iopub.status.idle": "2023-07-02T07:53:24.630953Z",
          "shell.execute_reply.started": "2023-07-02T07:53:12.442807Z",
          "shell.execute_reply": "2023-07-02T07:53:24.629793Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i_oP47_7YIq",
        "outputId": "8280f16b-afb0-40fe-ca64-5d4342bf1ff0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import os\n",
        "from torch import nn"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:24.633244Z",
          "iopub.execute_input": "2023-07-02T07:53:24.634125Z",
          "iopub.status.idle": "2023-07-02T07:53:26.272862Z",
          "shell.execute_reply.started": "2023-07-02T07:53:24.634089Z",
          "shell.execute_reply": "2023-07-02T07:53:26.271876Z"
        },
        "trusted": true,
        "id": "h5Cs_W0M7YIr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "1FtfrucaA-Ta"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:26.274455Z",
          "iopub.execute_input": "2023-07-02T07:53:26.275103Z",
          "iopub.status.idle": "2023-07-02T07:53:26.308235Z",
          "shell.execute_reply.started": "2023-07-02T07:53:26.275071Z",
          "shell.execute_reply": "2023-07-02T07:53:26.306100Z"
        },
        "trusted": true,
        "id": "htdAy6qG7YIr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:26.311632Z",
          "iopub.execute_input": "2023-07-02T07:53:26.311996Z",
          "iopub.status.idle": "2023-07-02T07:53:27.658547Z",
          "shell.execute_reply.started": "2023-07-02T07:53:26.311964Z",
          "shell.execute_reply": "2023-07-02T07:53:27.657408Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giGEpCY47YIs",
        "outputId": "10a2c223-6bb3-4530-da35-a45c351fa8f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-28 08:05:49--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-08-28 08:05:49 (75.4 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nofTYCyy8Dwt",
        "outputId": "6880bf1d-bcc0-400f-a7bb-84d2c419d6e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "data_dir = \"/content\"\n",
        "with open(os.path.join(data_dir, 'input.txt'), 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:27.660193Z",
          "iopub.execute_input": "2023-07-02T07:53:27.660539Z",
          "iopub.status.idle": "2023-07-02T07:53:27.672076Z",
          "shell.execute_reply.started": "2023-07-02T07:53:27.660512Z",
          "shell.execute_reply": "2023-07-02T07:53:27.671095Z"
        },
        "trusted": true,
        "id": "XgbzcuB07YIs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # string to integer\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # integer to string\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:27.700240Z",
          "iopub.execute_input": "2023-07-02T07:53:27.700579Z",
          "iopub.status.idle": "2023-07-02T07:53:27.708098Z",
          "shell.execute_reply.started": "2023-07-02T07:53:27.700549Z",
          "shell.execute_reply": "2023-07-02T07:53:27.707083Z"
        },
        "trusted": true,
        "id": "bjYPTxAV7YIs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100]) # the 100 characters will like the following to the GPT"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:27.709785Z",
          "iopub.execute_input": "2023-07-02T07:53:27.710179Z",
          "iopub.status.idle": "2023-07-02T07:53:28.012145Z",
          "shell.execute_reply.started": "2023-07-02T07:53:27.710150Z",
          "shell.execute_reply": "2023-07-02T07:53:28.011099Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGDF-05L7YIs",
        "outputId": "f081e43f-532f-412a-937d-2aa67f346203"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.013487Z",
          "iopub.execute_input": "2023-07-02T07:53:28.013853Z",
          "iopub.status.idle": "2023-07-02T07:53:28.019108Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.013822Z",
          "shell.execute_reply": "2023-07-02T07:53:28.018045Z"
        },
        "trusted": true,
        "id": "aUgLKVcN7YIt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tokens in train dataset: \", len(train_data))\n",
        "print(\"tokens in validaiton dataset: \", len(val_data))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.024216Z",
          "iopub.execute_input": "2023-07-02T07:53:28.025163Z",
          "iopub.status.idle": "2023-07-02T07:53:28.030431Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.025133Z",
          "shell.execute_reply": "2023-07-02T07:53:28.029302Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCQULjPZ7YIt",
        "outputId": "8381e065-5ead-4625-bb39-53671b1c68e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens in train dataset:  1003854\n",
            "tokens in validaiton dataset:  111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.032317Z",
          "iopub.execute_input": "2023-07-02T07:53:28.032654Z",
          "iopub.status.idle": "2023-07-02T07:53:28.060090Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.032625Z",
          "shell.execute_reply": "2023-07-02T07:53:28.059180Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkwpDuNR7YIt",
        "outputId": "0edb76a6-6767-45e6-fde1-7980119e1736"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.array(train_data)\n",
        "val_data = np.array(val_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.061343Z",
          "iopub.execute_input": "2023-07-02T07:53:28.061728Z",
          "iopub.status.idle": "2023-07-02T07:53:28.074052Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.061698Z",
          "shell.execute_reply": "2023-07-02T07:53:28.073136Z"
        },
        "trusted": true,
        "id": "asyMJ-Mk7YIt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loader"
      ],
      "metadata": {
        "id": "6cOg6e6Y7YIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, T = 4, 8 # Batch size, sequence size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.075398Z",
          "iopub.execute_input": "2023-07-02T07:53:28.075833Z",
          "iopub.status.idle": "2023-07-02T07:53:28.081220Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.075802Z",
          "shell.execute_reply": "2023-07-02T07:53:28.080262Z"
        },
        "trusted": true,
        "id": "tIM0vUN27YIt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_numbers = torch.randint(0, len(train_data) - T, (B,)) # randomly select sequences to make a batch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.082725Z",
          "iopub.execute_input": "2023-07-02T07:53:28.083110Z",
          "iopub.status.idle": "2023-07-02T07:53:28.095667Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.083080Z",
          "shell.execute_reply": "2023-07-02T07:53:28.094638Z"
        },
        "trusted": true,
        "id": "8QUISDCL7YIt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.stack([torch.from_numpy(train_data[random_number: random_number + T].astype(np.int64)) for random_number in random_numbers])\n",
        "labels = torch.stack([torch.from_numpy(train_data[random_number + 1: random_number + T + 1].astype(np.int64)) for random_number in random_numbers])\n",
        "data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.097180Z",
          "iopub.execute_input": "2023-07-02T07:53:28.097590Z",
          "iopub.status.idle": "2023-07-02T07:53:28.123345Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.097562Z",
          "shell.execute_reply": "2023-07-02T07:53:28.122360Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4avFL6zo7YIt",
        "outputId": "ec53e1da-718b-4b02-8e2f-aec4fef7b8f7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[53, 58, 46, 43, 56,  1, 51, 43],\n",
              "        [56,  1, 39,  1, 58, 43, 52, 42],\n",
              "        [56, 56, 53, 61,  5, 57,  1, 58],\n",
              "        [41, 46,  1, 54, 56, 53, 54, 46]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.124899Z",
          "iopub.execute_input": "2023-07-02T07:53:28.125244Z",
          "iopub.status.idle": "2023-07-02T07:53:28.131906Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.125216Z",
          "shell.execute_reply": "2023-07-02T07:53:28.130982Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzSClbpt7YIt",
        "outputId": "f993dc5d-a35a-428c-c793-1286c53ef31f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[58, 46, 43, 56,  1, 51, 43, 52],\n",
              "        [ 1, 39,  1, 58, 43, 52, 42, 43],\n",
              "        [56, 53, 61,  5, 57,  1, 58, 53],\n",
              "        [46,  1, 54, 56, 53, 54, 46, 43]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(sequence_length, batch_size):\n",
        "    random_numbers = torch.randint(0, len(train_data) - sequence_length, (batch_size,))\n",
        "    data = torch.stack([torch.from_numpy(train_data[random_number: random_number + sequence_length].astype(np.int64)) for random_number in random_numbers])\n",
        "    labels = torch.stack([torch.from_numpy(train_data[random_number + 1: random_number + sequence_length + 1].astype(np.int64)) for random_number in random_numbers])\n",
        "    return data, labels\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.133615Z",
          "iopub.execute_input": "2023-07-02T07:53:28.134486Z",
          "iopub.status.idle": "2023-07-02T07:53:28.141357Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.134455Z",
          "shell.execute_reply": "2023-07-02T07:53:28.140322Z"
        },
        "trusted": true,
        "id": "kHzYeAz17YIt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, labels = get_batch(8, 4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.144495Z",
          "iopub.execute_input": "2023-07-02T07:53:28.145196Z",
          "iopub.status.idle": "2023-07-02T07:53:28.151512Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.145167Z",
          "shell.execute_reply": "2023-07-02T07:53:28.150666Z"
        },
        "trusted": true,
        "id": "oU1ymkWr7YIt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape, labels.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.153166Z",
          "iopub.execute_input": "2023-07-02T07:53:28.153540Z",
          "iopub.status.idle": "2023-07-02T07:53:28.162123Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.153511Z",
          "shell.execute_reply": "2023-07-02T07:53:28.161000Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEj2x7mx7YIt",
        "outputId": "f29b7aff-a79d-476f-ab08-a98baef457d3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 8]), torch.Size([4, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram Language Model"
      ],
      "metadata": {
        "id": "gqKe4HT17YIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only incorporates 1 token in thie history to generate a new token"
      ],
      "metadata": {
        "id": "72S1nsCr90Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        logits = self.embedding(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.163325Z",
          "iopub.execute_input": "2023-07-02T07:53:28.164052Z",
          "iopub.status.idle": "2023-07-02T07:53:28.170768Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.164004Z",
          "shell.execute_reply": "2023-07-02T07:53:28.170001Z"
        },
        "trusted": true,
        "id": "YAmMRL237YIu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:28.171792Z",
          "iopub.execute_input": "2023-07-02T07:53:28.172504Z",
          "iopub.status.idle": "2023-07-02T07:53:30.914391Z",
          "shell.execute_reply.started": "2023-07-02T07:53:28.172473Z",
          "shell.execute_reply": "2023-07-02T07:53:30.913382Z"
        },
        "trusted": true,
        "id": "1ZNgquB97YIu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    for iteration in range(5000):\n",
        "        data, labels = get_batch(4, 8)\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(data)\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        labels = labels.view(B*T)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        if iteration % 100 == 0:\n",
        "            print(\"Iteration: \", iteration, \"loss: \", loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:30.915644Z",
          "iopub.execute_input": "2023-07-02T07:53:30.916021Z",
          "iopub.status.idle": "2023-07-02T07:53:36.237136Z",
          "shell.execute_reply.started": "2023-07-02T07:53:30.915988Z",
          "shell.execute_reply": "2023-07-02T07:53:36.236209Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjp28ovO7YIu",
        "outputId": "e23d11a1-d248-43f7-f5b8-8eec8c20030d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.636368274688721\n",
            "Iteration:  100 loss:  4.557950496673584\n",
            "Iteration:  200 loss:  4.189570426940918\n",
            "Iteration:  300 loss:  4.4069952964782715\n",
            "Iteration:  400 loss:  4.258916854858398\n",
            "Iteration:  500 loss:  4.123924255371094\n",
            "Iteration:  600 loss:  4.555902481079102\n",
            "Iteration:  700 loss:  4.207101821899414\n",
            "Iteration:  800 loss:  4.140120506286621\n",
            "Iteration:  900 loss:  3.9703216552734375\n",
            "Iteration:  1000 loss:  3.688896417617798\n",
            "Iteration:  1100 loss:  4.084914684295654\n",
            "Iteration:  1200 loss:  3.847590684890747\n",
            "Iteration:  1300 loss:  4.047602653503418\n",
            "Iteration:  1400 loss:  3.84074068069458\n",
            "Iteration:  1500 loss:  3.4482882022857666\n",
            "Iteration:  1600 loss:  3.9768524169921875\n",
            "Iteration:  1700 loss:  3.5372395515441895\n",
            "Iteration:  1800 loss:  3.541405439376831\n",
            "Iteration:  1900 loss:  3.644921064376831\n",
            "Iteration:  2000 loss:  3.545099973678589\n",
            "Iteration:  2100 loss:  3.697798490524292\n",
            "Iteration:  2200 loss:  3.2575578689575195\n",
            "Iteration:  2300 loss:  3.478306531906128\n",
            "Iteration:  2400 loss:  3.531028985977173\n",
            "Iteration:  2500 loss:  3.706047534942627\n",
            "Iteration:  2600 loss:  3.21197247505188\n",
            "Iteration:  2700 loss:  3.10884952545166\n",
            "Iteration:  2800 loss:  3.296074151992798\n",
            "Iteration:  2900 loss:  2.9840638637542725\n",
            "Iteration:  3000 loss:  3.406970500946045\n",
            "Iteration:  3100 loss:  3.343984842300415\n",
            "Iteration:  3200 loss:  3.139549970626831\n",
            "Iteration:  3300 loss:  3.1971867084503174\n",
            "Iteration:  3400 loss:  3.212230682373047\n",
            "Iteration:  3500 loss:  3.484872817993164\n",
            "Iteration:  3600 loss:  3.029796600341797\n",
            "Iteration:  3700 loss:  3.1876392364501953\n",
            "Iteration:  3800 loss:  2.676957368850708\n",
            "Iteration:  3900 loss:  3.1797661781311035\n",
            "Iteration:  4000 loss:  3.0248446464538574\n",
            "Iteration:  4100 loss:  2.783193826675415\n",
            "Iteration:  4200 loss:  2.676255941390991\n",
            "Iteration:  4300 loss:  2.785566568374634\n",
            "Iteration:  4400 loss:  3.0336384773254395\n",
            "Iteration:  4500 loss:  2.810734748840332\n",
            "Iteration:  4600 loss:  2.9215261936187744\n",
            "Iteration:  4700 loss:  2.7796382904052734\n",
            "Iteration:  4800 loss:  3.0031230449676514\n",
            "Iteration:  4900 loss:  2.923689126968384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate\n",
        "    start_char = \"\\n\"\n",
        "    start_token = torch.tensor(encode(start_char)).unsqueeze(0)\n",
        "    current_token = start_token # (B, T)\n",
        "    generated_tokens = []\n",
        "    for x in range(1000):\n",
        "        logits = model(current_token.to(device)) # (B, T, C)\n",
        "        logits = logits[:, -1, :] # (B, C)\n",
        "        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n",
        "        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n",
        "        generated_tokens.append(next_token.item())\n",
        "        current_token = next_token\n",
        "    print(decode(generated_tokens))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.239989Z",
          "iopub.execute_input": "2023-07-02T07:53:36.240843Z",
          "iopub.status.idle": "2023-07-02T07:53:36.527720Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.240809Z",
          "shell.execute_reply": "2023-07-02T07:53:36.526995Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeKTvlLj7YIu",
        "outputId": "ffad52a2-c24a-4942-fcc2-cbfa27b50344"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'uill:pol hc '.\n",
            "WCfo&-my\n",
            "Wt ant s\n",
            "ABy?\n",
            "Y-ha gr Zty ol'sthr C?G IAYBy, J?-E3't,\n",
            "Bur AgCoBuJzQU.\n",
            "Az\n",
            "\n",
            "Hs Lulllcinivk IE?\n",
            "CZ, hr t hen;\n",
            "hd.\n",
            "beJin\n",
            "DefL.;GUx.\n",
            "B&MTLUSpa\n",
            "m:! hoveWghot qymurup,\n",
            "OUK\n",
            "JovL; he s\n",
            "metouP-g!pams anmontor;Y,Id!in'x' incn dkqou\n",
            "orepakn f\n",
            "DRCHDodor a-IVcix-G--!V&SKwur:R, y3WLurd ss sur hehthig! d yhaly, cru\n",
            "Ahi&BALnd TL:O\n",
            "BNo.'Sh l\n",
            "SlouruV,kjhentbtu.SxKxure&GNNEMaA:VPhe'?\n",
            "By'XUNNoiblas bverinoiAJO'K\n",
            "Ro.\n",
            "VSINt??qJn\n",
            "myYBAY?XExV:$VRANukbimmhuthangrvel.,lADo wnd pLFYgr'ontuVUYRo pLI'tamoberds kKy Se mo aveiOimsu maus!\n",
            "B&-viEDLK:'qDK Spet dioules OxJfouCKOHHSe s s hth,'ImurGk\n",
            "mbpaue liOon;JFcW's,\n",
            "Mhkimo uve gsh dLKmyqdcXre s frFrdue sqwhoDon iLAwit.bens,y're s temn?NHS.\n",
            "Au\n",
            "'l,IvZ,Hved:asundxVGLT.\n",
            "W?\n",
            "BADungaxm?Q.,\n",
            "'S:-EPxpe-FL?lBQS atRRG.\n",
            "CKXaryopSes bdCKZ,agra!uithis o s lZ:-,\n",
            "$XR,\n",
            "Sirt h.\n",
            "\n",
            "KPy.\n",
            "llis a f!sse ut b'dsthale Ctnolenu\n",
            "Au:Bye IVce Fisulysedet alZth! cour H man hinIAhe cabCHVesthore\n",
            "Jpe-fomst hiR;U-gaurs s wapoussth:VUu:$NLagto mpKVjthato bk'daper fmppHELyrst IEKs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s say that your two largest probs are rather close together (for example,\n",
        "0.25 and 0.26). Using argmax() would always give you the index of 0.26,\n",
        "ignoring, in a sense, that 0.25 is almost the same. On the other hand, using\n",
        "multinomial() will give you the index of 0.26 26% of the time and the index\n",
        "of 0.25 25% of the time, respecting the fact that the two values are quite close\n",
        "to one another."
      ],
      "metadata": {
        "id": "r8KM5gtM7YIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attention"
      ],
      "metadata": {
        "id": "ftKz5_Uj7YIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C = vocab_size\n",
        "print(\"vocab_size: \", C)\n",
        "v = torch.randn(B,T,C) # (B, T, C)\n",
        "q = torch.randn(B,T,C) # (B ,T, C)\n",
        "k = torch.randn(B,T,C) # (B, T, C)\n",
        "\n",
        "# k.permute(0, -1, -2)\n",
        "attention_map = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) = (B, T, T)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.529289Z",
          "iopub.execute_input": "2023-07-02T07:53:36.529686Z",
          "iopub.status.idle": "2023-07-02T07:53:36.583368Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.529650Z",
          "shell.execute_reply": "2023-07-02T07:53:36.582113Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4sSPntt7YIu",
        "outputId": "8a3dff42-aeda-419c-8a51-741fc6575ae8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size:  65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_map.mean(), attention_map.var()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.584533Z",
          "iopub.execute_input": "2023-07-02T07:53:36.584893Z",
          "iopub.status.idle": "2023-07-02T07:53:36.607557Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.584858Z",
          "shell.execute_reply": "2023-07-02T07:53:36.606539Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTpvE1Xs7YIu",
        "outputId": "069b3172-0a3a-4373-a862-1a4dcb24c06b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0809), tensor(0.8847))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_map[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.608986Z",
          "iopub.execute_input": "2023-07-02T07:53:36.609339Z",
          "iopub.status.idle": "2023-07-02T07:53:36.619086Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.609310Z",
          "shell.execute_reply": "2023-07-02T07:53:36.618093Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYT4mVIF7YIu",
        "outputId": "d747efc0-a86d-46fe-d2ef-626eacfce8fa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2205,  0.6218,  0.7317,  1.1332,  0.5970,  0.7078, -0.3986,  1.1761],\n",
              "        [ 0.1310, -0.5916, -1.4557, -0.5407, -0.3209, -0.5917, -1.0167,  0.3724],\n",
              "        [-0.7116,  1.1316,  0.6352, -0.1230,  0.0241, -0.6274, -0.6252, -0.1847],\n",
              "        [ 1.3909,  2.1249, -1.0876, -1.1044,  0.0488,  0.5310, -0.1713, -0.0950],\n",
              "        [-1.6491, -0.8814,  1.4003,  0.2558,  0.4396,  0.2764, -0.3069, -0.4714],\n",
              "        [-0.5305,  0.5307,  0.7631, -0.1590,  0.4766,  0.7653,  0.9111, -1.3073],\n",
              "        [-0.1964, -1.4210,  1.4563,  0.7868, -0.2745, -0.8312,  1.2157, -0.7462],\n",
              "        [ 0.1521, -0.5514, -0.5638,  0.9498, -0.7947, -1.8731,  0.5960,  2.0033]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maps = attention_map @ v # (B, T, T) @ (B, T,C) = (B, T, C)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.620649Z",
          "iopub.execute_input": "2023-07-02T07:53:36.620999Z",
          "iopub.status.idle": "2023-07-02T07:53:36.628632Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.620969Z",
          "shell.execute_reply": "2023-07-02T07:53:36.627638Z"
        },
        "trusted": true,
        "id": "2N5TysZt7YIu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maps.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.636683Z",
          "iopub.execute_input": "2023-07-02T07:53:36.636940Z",
          "iopub.status.idle": "2023-07-02T07:53:36.642649Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.636919Z",
          "shell.execute_reply": "2023-07-02T07:53:36.641632Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWK2NwD-7YIv",
        "outputId": "c36e4b68-40c2-4d36-d5a0-a038aa74aa9d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do masked self-attention, as current token shouldn't pay attention to the tokens not yet generated."
      ],
      "metadata": {
        "id": "EGpZzgE_7YIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "tril # zeros in the upper right diagonal"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.644338Z",
          "iopub.execute_input": "2023-07-02T07:53:36.645122Z",
          "iopub.status.idle": "2023-07-02T07:53:36.654081Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.645092Z",
          "shell.execute_reply": "2023-07-02T07:53:36.653076Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_7jx8eC7YIv",
        "outputId": "3c17ba0f-5d1f-4b6a-d8e4-26d5af1ed05f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_attention_map = attention_map.masked_fill(tril == 0, -np.Inf)\n",
        "masked_attention_map = F.softmax(masked_attention_map, dim=-1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.657374Z",
          "iopub.execute_input": "2023-07-02T07:53:36.657789Z",
          "iopub.status.idle": "2023-07-02T07:53:36.666151Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.657659Z",
          "shell.execute_reply": "2023-07-02T07:53:36.665489Z"
        },
        "trusted": true,
        "id": "vkQ4q6tu7YIv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.mean(), k.mean(), q.var(), k.var()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.667506Z",
          "iopub.execute_input": "2023-07-02T07:53:36.668135Z",
          "iopub.status.idle": "2023-07-02T07:53:36.677187Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.668105Z",
          "shell.execute_reply": "2023-07-02T07:53:36.676293Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU4ExDF17YIv",
        "outputId": "950df166-6656-4ee7-be9f-0e9edf80509c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.0002), tensor(0.0116), tensor(0.9968), tensor(1.0155))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_map.mean(), attention_map.var() # Unit gaussian distribution, with mean 0 and variance 1, thanks to the division by vocab size in the attention map."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.678719Z",
          "iopub.execute_input": "2023-07-02T07:53:36.679371Z",
          "iopub.status.idle": "2023-07-02T07:53:36.686500Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.679341Z",
          "shell.execute_reply": "2023-07-02T07:53:36.685563Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hi8iY1_7YIv",
        "outputId": "19784f89-7f4a-4dfb-9012-7589b64e7926"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0809), tensor(0.8847))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_attention_map[0] # each row sums up to 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.687945Z",
          "iopub.execute_input": "2023-07-02T07:53:36.688569Z",
          "iopub.status.idle": "2023-07-02T07:53:36.697266Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.688539Z",
          "shell.execute_reply": "2023-07-02T07:53:36.696350Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8qruwHI7YIv",
        "outputId": "15b2408c-e4f8-40f5-e05a-f6a62803b481"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.6732, 0.3268, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0896, 0.5659, 0.3445, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3077, 0.6411, 0.0258, 0.0254, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0256, 0.0552, 0.5404, 0.1720, 0.2068, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0650, 0.1879, 0.2371, 0.0943, 0.1780, 0.2376, 0.0000, 0.0000],\n",
              "        [0.0678, 0.0199, 0.3540, 0.1813, 0.0627, 0.0359, 0.2783, 0.0000],\n",
              "        [0.0790, 0.0391, 0.0386, 0.1755, 0.0307, 0.0104, 0.1232, 0.5034]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(masked_attention_map[0, 0, :]) # each row sums up to 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.698664Z",
          "iopub.execute_input": "2023-07-02T07:53:36.699286Z",
          "iopub.status.idle": "2023-07-02T07:53:36.707008Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.699257Z",
          "shell.execute_reply": "2023-07-02T07:53:36.706167Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKHcPQxW7YI0",
        "outputId": "b13d7991-1914-481b-8851-dc10dcf87084"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_feature_maps = masked_attention_map @ v"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.708419Z",
          "iopub.execute_input": "2023-07-02T07:53:36.709018Z",
          "iopub.status.idle": "2023-07-02T07:53:36.715221Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.708988Z",
          "shell.execute_reply": "2023-07-02T07:53:36.714175Z"
        },
        "trusted": true,
        "id": "EpmP5-li7YI0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_feature_maps.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.716663Z",
          "iopub.execute_input": "2023-07-02T07:53:36.717177Z",
          "iopub.status.idle": "2023-07-02T07:53:36.725723Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.717147Z",
          "shell.execute_reply": "2023-07-02T07:53:36.725016Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc8VtQWy7YI0",
        "outputId": "b070d822-6b79-479c-bd37-506d37ef8866"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_feature_maps[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.727646Z",
          "iopub.execute_input": "2023-07-02T07:53:36.728492Z",
          "iopub.status.idle": "2023-07-02T07:53:36.742848Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.728463Z",
          "shell.execute_reply": "2023-07-02T07:53:36.741938Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzAVmaWx7YI0",
        "outputId": "63e8cbbc-cf85-4b78-b877-243afa0b829b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4327, -1.2709,  1.2184, -0.2431, -0.5003, -0.7021, -0.7609,  1.5733,\n",
              "         -0.6160,  0.3363, -0.3817, -0.3387, -0.5015,  0.5827, -2.2089,  1.7469,\n",
              "         -1.1907,  0.6040,  0.8900,  1.6983,  0.9031, -0.9910, -1.1666, -0.3730,\n",
              "          0.7763, -0.8947, -0.9507, -0.0187, -1.5640,  0.9974, -0.1965, -1.7016,\n",
              "         -1.2813,  0.6549,  0.7466,  0.0666,  0.6011,  0.1080, -0.4749,  2.2317,\n",
              "          0.3079, -1.1457, -2.2090,  0.9496, -1.7616,  0.4092, -0.4464, -0.5848,\n",
              "          1.0068, -1.9043,  1.5569, -0.0109,  0.3873, -0.4885, -2.0485,  0.4393,\n",
              "         -0.3141,  0.2648,  0.1519,  1.1786, -1.7659, -2.9221,  0.3128,  0.0833,\n",
              "          0.3633],\n",
              "        [ 0.1585, -0.7313,  0.1550, -0.4063,  0.1241, -0.2318, -0.9494,  1.1057,\n",
              "         -0.4473, -0.0619, -0.6090, -0.2649, -0.4979,  0.0247, -1.6782,  1.1346,\n",
              "         -0.9815, -0.0104,  0.3058,  1.4033,  0.3106, -0.8323, -0.4818,  0.1889,\n",
              "          0.9525, -0.4939,  0.1074,  0.4620, -1.0098,  0.4052, -0.3708, -1.4741,\n",
              "         -1.2601,  0.7127,  0.3105,  0.3746,  0.0340,  0.6149,  0.3841,  1.5402,\n",
              "          0.0508, -1.7035, -1.7527,  1.1791, -1.3616,  0.6620, -0.5798, -0.6910,\n",
              "          0.2003, -1.7393,  1.0017,  0.0191,  0.7590,  0.0743, -1.1472,  0.1574,\n",
              "          0.4327,  0.0904,  0.0533,  1.4624, -1.2128, -1.7974,  0.3019, -0.4609,\n",
              "         -0.0407],\n",
              "        [ 0.1751,  0.2468, -0.9158, -0.3766,  0.3555,  0.1192, -0.9926, -0.0278,\n",
              "         -0.2961, -0.5778, -1.3175,  0.4507, -0.4905, -1.6217,  0.1099,  0.3187,\n",
              "         -0.7201, -0.6093, -0.1822,  1.0436, -0.5015, -1.1184,  0.8983,  0.4845,\n",
              "          1.1254,  0.1495,  0.9376,  0.7993, -0.0646, -0.4010, -0.6473, -0.3694,\n",
              "         -0.9598,  0.3704, -0.0367,  0.9755, -0.4452,  1.0107,  1.3181,  0.2087,\n",
              "         -1.1987, -1.4086, -0.7579,  0.8916, -0.4242,  0.2532, -0.7852, -0.6736,\n",
              "         -0.5299, -1.0444,  0.8631, -0.0486,  0.7909,  0.5002,  0.1345,  0.0848,\n",
              "          1.4820, -0.6024, -0.2371,  1.8467, -0.3789,  0.6620, -0.0465, -1.3358,\n",
              "         -0.0060],\n",
              "        [-0.0912, -0.1212, -0.9433, -0.5206,  0.7088,  0.2322, -1.0922,  0.5602,\n",
              "         -0.2835, -0.4749, -0.8581, -0.0944, -0.5234, -0.6117, -1.0420,  0.4704,\n",
              "         -0.7496, -0.6143, -0.2420,  1.0802, -0.3477, -0.6947,  0.2405,  0.7403,\n",
              "          1.1133, -0.0602,  1.1538,  0.9730, -0.3756, -0.2087, -0.4956, -1.1448,\n",
              "         -1.2094,  0.7153, -0.1251,  0.7214, -0.5624,  1.0993,  1.2543,  0.7331,\n",
              "         -0.2624, -2.1230, -1.2196,  1.3502, -0.8835,  0.8622, -0.7040, -0.7886,\n",
              "         -0.6346, -1.5187,  0.4399,  0.0559,  1.1002,  0.6452, -0.1455, -0.1242,\n",
              "          1.1776, -0.1790, -0.0365,  1.7433, -0.6026, -0.4876,  0.2383, -0.9975,\n",
              "         -0.4001],\n",
              "        [ 0.4085,  0.3961,  0.1289,  0.5066, -0.6710, -0.2681, -0.2108, -0.3501,\n",
              "         -0.1979, -0.6813, -0.5385,  1.1036, -0.3680, -1.5269,  0.2867,  0.2447,\n",
              "         -0.7381,  0.0430,  0.6707,  0.3952, -0.3897, -1.1951,  0.4642, -0.3363,\n",
              "          0.3917, -0.1918, -0.3197,  1.0066,  0.0232, -0.1830, -0.3114,  0.6419,\n",
              "         -0.3455,  0.1473,  0.3978,  0.5603,  0.0238,  0.4438,  0.4263,  0.1813,\n",
              "         -1.3982,  0.3764, -0.3353, -0.3331, -0.0168, -0.9399, -0.6245, -0.5005,\n",
              "          0.0604, -0.2719,  1.3553,  0.3686, -0.1358, -0.0064, -0.1374,  0.3662,\n",
              "          0.5090, -1.0263, -0.2139,  0.9641, -0.6604,  1.0604, -0.5381, -0.6625,\n",
              "          0.8659],\n",
              "        [ 0.1459,  0.1838, -0.2841,  0.4333,  0.1827, -0.0948,  0.0085, -0.0242,\n",
              "         -0.2556, -0.6984, -0.5906,  0.4532,  0.1423, -0.8021, -0.2614,  0.3606,\n",
              "         -0.5937, -0.2656,  0.2245,  0.2206, -0.2689, -0.5811,  0.3061, -0.3587,\n",
              "          0.7363, -0.3035,  0.0685,  0.8001, -0.1582,  0.0653, -0.4939,  0.2338,\n",
              "         -0.3759,  0.0057,  0.0148,  0.3087, -0.0699,  0.5238,  0.6742,  0.4928,\n",
              "         -0.3928, -0.7344, -0.2077, -0.2263,  0.0382, -0.4420, -0.1728, -0.0127,\n",
              "         -0.1973, -0.1287,  0.5417,  0.5117, -0.0184,  0.4123, -0.6027,  0.3427,\n",
              "          0.1982, -0.5466, -0.4761,  0.6578, -0.6083,  0.5508, -0.4796, -0.6743,\n",
              "          0.2035],\n",
              "        [ 0.5402,  0.1756, -0.2223,  0.2932, -0.8438, -0.0390,  0.2191,  0.3835,\n",
              "         -0.6554, -0.1402, -0.5213,  0.7414, -0.0785, -0.9932,  0.1101,  0.8027,\n",
              "         -0.8861,  0.0232,  0.8285,  0.2722, -0.3262, -0.7233,  0.1329, -0.3080,\n",
              "          0.4572, -0.0827, -0.6055,  0.7024,  0.1786,  0.1415, -0.0418, -0.1987,\n",
              "         -0.3517, -0.0280, -0.1987,  0.2780,  0.7512, -0.0062,  0.4110, -0.0480,\n",
              "         -1.2433,  0.4109,  0.0983, -0.4345, -0.2354, -0.5641, -0.4766, -0.3107,\n",
              "          0.1197, -0.1551,  1.0587,  0.3822,  0.1537,  0.2840, -0.1059,  0.4485,\n",
              "          0.1328, -0.2260,  0.3977,  0.6884, -0.8077, -0.0148, -0.6033, -0.3491,\n",
              "          0.7594],\n",
              "        [-0.0948, -1.0640, -0.1646, -0.0565, -0.5572,  1.0591, -0.0328, -0.2593,\n",
              "         -0.2213,  0.1566, -0.2274,  0.7490,  0.2240, -0.0097, -0.2941,  0.2253,\n",
              "          0.0098,  0.1258,  0.1055,  0.8930, -0.0340,  0.4580, -0.1458, -0.2427,\n",
              "          0.4319, -0.5287, -0.1484, -0.3220,  0.3887, -0.2516,  0.0844, -1.1113,\n",
              "          0.0772,  0.3376, -0.7369,  0.6746,  0.6658, -0.2297,  0.2062,  0.0983,\n",
              "         -0.3310, -0.4925,  0.0420,  0.0999, -0.3782,  0.2781, -0.5050,  0.3644,\n",
              "         -0.4427, -0.1169,  0.6368, -0.0539, -0.4237,  1.0262,  0.2876,  0.2755,\n",
              "         -0.1201, -0.1402, -0.2136,  0.2385, -0.1036, -0.0274, -0.7482, -0.0213,\n",
              "          0.1270]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "NCzS-c2Z7YI0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, head_size, drop_p):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.k = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.v = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(sequence_length, sequence_length)))\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "        self.head_size = head_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "        query = self.q(x)\n",
        "        key = self.k(x)\n",
        "        value = self.v(x)\n",
        "        attention_map = query @ key.transpose(-2, -1) * self.head_size**-0.5 # (B, T, C) @ (B, C, T) = (B, T, T)\n",
        "        masked_attention_map = attention_map.masked_fill(self.tril[:T, :T] == 0, -np.Inf)\n",
        "        masked_attention_map = F.softmax(masked_attention_map, dim=-1)\n",
        "        attention_map = self.dropout(masked_attention_map)\n",
        "        feature_map = attention_map @ value # (B, T, T) @ (B, T, C) = (B, T, C)\n",
        "\n",
        "        return feature_map\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.744967Z",
          "iopub.execute_input": "2023-07-02T07:53:36.745567Z",
          "iopub.status.idle": "2023-07-02T07:53:36.754547Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.745535Z",
          "shell.execute_reply": "2023-07-02T07:53:36.753815Z"
        },
        "trusted": true,
        "id": "S8oqmrsQ7YI0"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, sequence_length, embed_dim, n_heads, head_size, drop_p):\n",
        "        super().__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, vocab_size)\n",
        "        self.self_attention = SelfAttention(sequence_length, embed_dim, head_size, drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        token_embeddings = self.token_embedding(x) # (B, T, C)\n",
        "        feature_maps = self.self_attention(token_embeddings)\n",
        "\n",
        "        return feature_maps"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.755994Z",
          "iopub.execute_input": "2023-07-02T07:53:36.756673Z",
          "iopub.status.idle": "2023-07-02T07:53:36.767754Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.756644Z",
          "shell.execute_reply": "2023-07-02T07:53:36.766914Z"
        },
        "trusted": true,
        "id": "cgpa6oaq7YI0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=65, n_heads=0, head_size=65, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.770852Z",
          "iopub.execute_input": "2023-07-02T07:53:36.771159Z",
          "iopub.status.idle": "2023-07-02T07:53:36.779927Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.771136Z",
          "shell.execute_reply": "2023-07-02T07:53:36.779006Z"
        },
        "trusted": true,
        "id": "ZhCjZd3K7YI0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:53:36.783298Z",
          "iopub.execute_input": "2023-07-02T07:53:36.783578Z",
          "iopub.status.idle": "2023-07-02T07:53:48.951334Z",
          "shell.execute_reply.started": "2023-07-02T07:53:36.783556Z",
          "shell.execute_reply": "2023-07-02T07:53:48.950308Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJbPjx0A7YI0",
        "outputId": "8f30c667-5961-4f8e-e86d-65f34d10951a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.126742839813232\n",
            "Iteration:  100 loss:  3.6406126022338867\n",
            "Iteration:  200 loss:  3.6355183124542236\n",
            "Iteration:  300 loss:  3.0374107360839844\n",
            "Iteration:  400 loss:  3.2928104400634766\n",
            "Iteration:  500 loss:  3.0157876014709473\n",
            "Iteration:  600 loss:  2.9386563301086426\n",
            "Iteration:  700 loss:  2.8773350715637207\n",
            "Iteration:  800 loss:  2.539405345916748\n",
            "Iteration:  900 loss:  2.807511329650879\n",
            "Iteration:  1000 loss:  3.220825433731079\n",
            "Iteration:  1100 loss:  3.094298839569092\n",
            "Iteration:  1200 loss:  2.701075553894043\n",
            "Iteration:  1300 loss:  2.9328079223632812\n",
            "Iteration:  1400 loss:  2.896463632583618\n",
            "Iteration:  1500 loss:  2.686234474182129\n",
            "Iteration:  1600 loss:  3.0334415435791016\n",
            "Iteration:  1700 loss:  2.953718662261963\n",
            "Iteration:  1800 loss:  2.6348588466644287\n",
            "Iteration:  1900 loss:  2.976311445236206\n",
            "Iteration:  2000 loss:  3.0514724254608154\n",
            "Iteration:  2100 loss:  2.711745262145996\n",
            "Iteration:  2200 loss:  2.7380549907684326\n",
            "Iteration:  2300 loss:  2.7571449279785156\n",
            "Iteration:  2400 loss:  2.637495756149292\n",
            "Iteration:  2500 loss:  2.8058221340179443\n",
            "Iteration:  2600 loss:  2.6510298252105713\n",
            "Iteration:  2700 loss:  3.082226276397705\n",
            "Iteration:  2800 loss:  2.6688106060028076\n",
            "Iteration:  2900 loss:  2.6880009174346924\n",
            "Iteration:  3000 loss:  2.5152854919433594\n",
            "Iteration:  3100 loss:  2.613422155380249\n",
            "Iteration:  3200 loss:  3.061786413192749\n",
            "Iteration:  3300 loss:  2.848203182220459\n",
            "Iteration:  3400 loss:  2.7457385063171387\n",
            "Iteration:  3500 loss:  2.8094425201416016\n",
            "Iteration:  3600 loss:  2.639860153198242\n",
            "Iteration:  3700 loss:  2.8547282218933105\n",
            "Iteration:  3800 loss:  2.7842366695404053\n",
            "Iteration:  3900 loss:  3.279304027557373\n",
            "Iteration:  4000 loss:  2.807746410369873\n",
            "Iteration:  4100 loss:  3.440119981765747\n",
            "Iteration:  4200 loss:  2.781118392944336\n",
            "Iteration:  4300 loss:  2.6723530292510986\n",
            "Iteration:  4400 loss:  2.680084705352783\n",
            "Iteration:  4500 loss:  2.5749313831329346\n",
            "Iteration:  4600 loss:  2.8864998817443848\n",
            "Iteration:  4700 loss:  2.682699203491211\n",
            "Iteration:  4800 loss:  2.922394037246704\n",
            "Iteration:  4900 loss:  2.5277485847473145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss reduced from 2.9 to 2.5. Le's implement now multi-head self-attention"
      ],
      "metadata": {
        "id": "S2z2Pjv37YI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head self-attention"
      ],
      "metadata": {
        "id": "1OSgmDcy7YI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // n_heads\n",
        "        self.heads = nn.ModuleList([SelfAttention(sequence_length, embed_dim, head_size, drop_p) for x in range(n_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:55:21.161944Z",
          "iopub.execute_input": "2023-07-02T07:55:21.162385Z",
          "iopub.status.idle": "2023-07-02T07:55:21.169402Z",
          "shell.execute_reply.started": "2023-07-02T07:55:21.162354Z",
          "shell.execute_reply": "2023-07-02T07:55:21.168274Z"
        },
        "trusted": true,
        "id": "lUbuO8--7YI1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, sequence_length, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n",
        "        self.linear_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        token_embeddings = self.token_embedding(x) # (B, T, C)\n",
        "        feature_maps = self.multi_head_self_attention(token_embeddings)\n",
        "        out = self.linear_head(feature_maps)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:55:21.516366Z",
          "iopub.execute_input": "2023-07-02T07:55:21.517071Z",
          "iopub.status.idle": "2023-07-02T07:55:21.524423Z",
          "shell.execute_reply.started": "2023-07-02T07:55:21.517009Z",
          "shell.execute_reply": "2023-07-02T07:55:21.523220Z"
        },
        "trusted": true,
        "id": "-7B8kNj87YI1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_heads=4, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:55:21.946479Z",
          "iopub.execute_input": "2023-07-02T07:55:21.946843Z",
          "iopub.status.idle": "2023-07-02T07:55:21.956602Z",
          "shell.execute_reply.started": "2023-07-02T07:55:21.946815Z",
          "shell.execute_reply": "2023-07-02T07:55:21.955590Z"
        },
        "trusted": true,
        "id": "pVnPoqXB7YI1"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:55:28.018490Z",
          "iopub.execute_input": "2023-07-02T07:55:28.018886Z",
          "iopub.status.idle": "2023-07-02T07:55:52.842593Z",
          "shell.execute_reply.started": "2023-07-02T07:55:28.018854Z",
          "shell.execute_reply": "2023-07-02T07:55:52.841643Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uie7ReX7YI1",
        "outputId": "8ac3eba4-dd3b-4bef-f660-db874481d809"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.158942699432373\n",
            "Iteration:  100 loss:  3.4043633937835693\n",
            "Iteration:  200 loss:  2.9074349403381348\n",
            "Iteration:  300 loss:  2.7036173343658447\n",
            "Iteration:  400 loss:  2.8406388759613037\n",
            "Iteration:  500 loss:  2.7827694416046143\n",
            "Iteration:  600 loss:  2.6133015155792236\n",
            "Iteration:  700 loss:  2.961909532546997\n",
            "Iteration:  800 loss:  2.5660040378570557\n",
            "Iteration:  900 loss:  2.830247402191162\n",
            "Iteration:  1000 loss:  2.5201432704925537\n",
            "Iteration:  1100 loss:  2.553600549697876\n",
            "Iteration:  1200 loss:  2.565138101577759\n",
            "Iteration:  1300 loss:  2.980091094970703\n",
            "Iteration:  1400 loss:  2.943397045135498\n",
            "Iteration:  1500 loss:  2.5841736793518066\n",
            "Iteration:  1600 loss:  2.5725607872009277\n",
            "Iteration:  1700 loss:  2.409419298171997\n",
            "Iteration:  1800 loss:  2.5073046684265137\n",
            "Iteration:  1900 loss:  2.7784595489501953\n",
            "Iteration:  2000 loss:  2.315389394760132\n",
            "Iteration:  2100 loss:  2.5949838161468506\n",
            "Iteration:  2200 loss:  2.4727745056152344\n",
            "Iteration:  2300 loss:  2.382577419281006\n",
            "Iteration:  2400 loss:  2.3976328372955322\n",
            "Iteration:  2500 loss:  2.846513509750366\n",
            "Iteration:  2600 loss:  2.5266809463500977\n",
            "Iteration:  2700 loss:  2.385791778564453\n",
            "Iteration:  2800 loss:  2.993408203125\n",
            "Iteration:  2900 loss:  2.387908458709717\n",
            "Iteration:  3000 loss:  2.7093265056610107\n",
            "Iteration:  3100 loss:  2.3140220642089844\n",
            "Iteration:  3200 loss:  2.4753243923187256\n",
            "Iteration:  3300 loss:  2.4682347774505615\n",
            "Iteration:  3400 loss:  2.141559600830078\n",
            "Iteration:  3500 loss:  2.249652624130249\n",
            "Iteration:  3600 loss:  3.0518715381622314\n",
            "Iteration:  3700 loss:  2.549086332321167\n",
            "Iteration:  3800 loss:  2.527564764022827\n",
            "Iteration:  3900 loss:  2.605776786804199\n",
            "Iteration:  4000 loss:  2.597379684448242\n",
            "Iteration:  4100 loss:  1.9860739707946777\n",
            "Iteration:  4200 loss:  2.3188726902008057\n",
            "Iteration:  4300 loss:  2.61187481880188\n",
            "Iteration:  4400 loss:  2.558112144470215\n",
            "Iteration:  4500 loss:  2.282719612121582\n",
            "Iteration:  4600 loss:  2.433846950531006\n",
            "Iteration:  4700 loss:  2.277886152267456\n",
            "Iteration:  4800 loss:  2.513786792755127\n",
            "Iteration:  4900 loss:  2.2723898887634277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss reduced from 2.5 to 2.27"
      ],
      "metadata": {
        "id": "sJtLQ1Zm7YI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Dropout to Multi Head Self-Attention"
      ],
      "metadata": {
        "id": "KXDd7es37YI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // n_heads\n",
        "        self.heads = nn.ModuleList([SelfAttention(sequence_length, embed_dim, head_size, drop_p) for x in range(n_heads)])\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:55:52.844724Z",
          "iopub.execute_input": "2023-07-02T07:55:52.845094Z",
          "iopub.status.idle": "2023-07-02T07:55:52.851859Z",
          "shell.execute_reply.started": "2023-07-02T07:55:52.845061Z",
          "shell.execute_reply": "2023-07-02T07:55:52.850907Z"
        },
        "trusted": true,
        "id": "ROrKWF187YI1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n",
        "        self.linear_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        token_embeddings = self.token_embedding(x) # (B, T, C)\n",
        "        feature_maps = self.multi_head_self_attention(token_embeddings)\n",
        "        out = self.linear_head(feature_maps)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:55:52.853055Z",
          "iopub.execute_input": "2023-07-02T07:55:52.853807Z",
          "iopub.status.idle": "2023-07-02T07:55:52.868731Z",
          "shell.execute_reply.started": "2023-07-02T07:55:52.853776Z",
          "shell.execute_reply": "2023-07-02T07:55:52.867812Z"
        },
        "trusted": true,
        "id": "daCLbPIa7YI1"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_heads=4, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:55:52.871559Z",
          "iopub.execute_input": "2023-07-02T07:55:52.871961Z",
          "iopub.status.idle": "2023-07-02T07:55:52.881892Z",
          "shell.execute_reply.started": "2023-07-02T07:55:52.871878Z",
          "shell.execute_reply": "2023-07-02T07:55:52.880874Z"
        },
        "trusted": true,
        "id": "K4qeCVYF7YI1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:55:52.883339Z",
          "iopub.execute_input": "2023-07-02T07:55:52.883672Z",
          "iopub.status.idle": "2023-07-02T07:56:18.797212Z",
          "shell.execute_reply.started": "2023-07-02T07:55:52.883642Z",
          "shell.execute_reply": "2023-07-02T07:56:18.796239Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0whdXaM7YI1",
        "outputId": "b5f25253-7c33-4272-ef8b-5ee7ea7d6fb0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.2831621170043945\n",
            "Iteration:  100 loss:  3.0488007068634033\n",
            "Iteration:  200 loss:  3.207683801651001\n",
            "Iteration:  300 loss:  2.4558496475219727\n",
            "Iteration:  400 loss:  2.9216654300689697\n",
            "Iteration:  500 loss:  2.493610143661499\n",
            "Iteration:  600 loss:  2.479698657989502\n",
            "Iteration:  700 loss:  2.8587186336517334\n",
            "Iteration:  800 loss:  2.6204404830932617\n",
            "Iteration:  900 loss:  2.890253782272339\n",
            "Iteration:  1000 loss:  2.5778377056121826\n",
            "Iteration:  1100 loss:  2.587873935699463\n",
            "Iteration:  1200 loss:  2.806710720062256\n",
            "Iteration:  1300 loss:  2.1928112506866455\n",
            "Iteration:  1400 loss:  2.949326753616333\n",
            "Iteration:  1500 loss:  2.52822208404541\n",
            "Iteration:  1600 loss:  2.8138933181762695\n",
            "Iteration:  1700 loss:  2.6682004928588867\n",
            "Iteration:  1800 loss:  2.529186487197876\n",
            "Iteration:  1900 loss:  2.4254767894744873\n",
            "Iteration:  2000 loss:  2.49601149559021\n",
            "Iteration:  2100 loss:  2.555058240890503\n",
            "Iteration:  2200 loss:  2.585300922393799\n",
            "Iteration:  2300 loss:  3.0477921962738037\n",
            "Iteration:  2400 loss:  2.6224467754364014\n",
            "Iteration:  2500 loss:  2.15569806098938\n",
            "Iteration:  2600 loss:  2.5644853115081787\n",
            "Iteration:  2700 loss:  2.350994825363159\n",
            "Iteration:  2800 loss:  2.686340808868408\n",
            "Iteration:  2900 loss:  2.386441946029663\n",
            "Iteration:  3000 loss:  2.7599575519561768\n",
            "Iteration:  3100 loss:  2.756685972213745\n",
            "Iteration:  3200 loss:  2.6311590671539307\n",
            "Iteration:  3300 loss:  2.8090803623199463\n",
            "Iteration:  3400 loss:  2.2815799713134766\n",
            "Iteration:  3500 loss:  2.6831746101379395\n",
            "Iteration:  3600 loss:  2.281010866165161\n",
            "Iteration:  3700 loss:  2.3850443363189697\n",
            "Iteration:  3800 loss:  2.286116123199463\n",
            "Iteration:  3900 loss:  2.6595287322998047\n",
            "Iteration:  4000 loss:  2.483574867248535\n",
            "Iteration:  4100 loss:  2.5735316276550293\n",
            "Iteration:  4200 loss:  2.6299943923950195\n",
            "Iteration:  4300 loss:  2.3107049465179443\n",
            "Iteration:  4400 loss:  2.7522826194763184\n",
            "Iteration:  4500 loss:  2.418811798095703\n",
            "Iteration:  4600 loss:  2.2726523876190186\n",
            "Iteration:  4700 loss:  2.826859951019287\n",
            "Iteration:  4800 loss:  2.732377767562866\n",
            "Iteration:  4900 loss:  2.4641573429107666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Didn't much help! That's okay. We have a lot more techniques in our sleeves up left. We are going to retain this multi-head self attention dropout."
      ],
      "metadata": {
        "id": "XbfecKJm7YI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Linear layer in the multi-head self-attention block"
      ],
      "metadata": {
        "id": "YUzToWzy7YI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // n_heads\n",
        "        self.heads = nn.ModuleList([SelfAttention(sequence_length, embed_dim, head_size, drop_p) for x in range(n_heads)])\n",
        "        self.mlp = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.mlp(out)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:56:18.801600Z",
          "iopub.execute_input": "2023-07-02T07:56:18.803271Z",
          "iopub.status.idle": "2023-07-02T07:56:18.814590Z",
          "shell.execute_reply.started": "2023-07-02T07:56:18.803236Z",
          "shell.execute_reply": "2023-07-02T07:56:18.813811Z"
        },
        "trusted": true,
        "id": "ndr-zx577YI1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n",
        "        self.linear_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        token_embeddings = self.token_embedding(x) # (B, T, C)\n",
        "        feature_maps = self.multi_head_self_attention(token_embeddings)\n",
        "        out = self.linear_head(feature_maps)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:56:18.818498Z",
          "iopub.execute_input": "2023-07-02T07:56:18.820257Z",
          "iopub.status.idle": "2023-07-02T07:56:18.835176Z",
          "shell.execute_reply.started": "2023-07-02T07:56:18.820223Z",
          "shell.execute_reply": "2023-07-02T07:56:18.834162Z"
        },
        "trusted": true,
        "id": "Z2R0abLx7YI1"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_heads=4, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:56:18.839562Z",
          "iopub.execute_input": "2023-07-02T07:56:18.841975Z",
          "iopub.status.idle": "2023-07-02T07:56:18.852370Z",
          "shell.execute_reply.started": "2023-07-02T07:56:18.841943Z",
          "shell.execute_reply": "2023-07-02T07:56:18.851506Z"
        },
        "trusted": true,
        "id": "nc_GYkoe7YI2"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:56:18.856441Z",
          "iopub.execute_input": "2023-07-02T07:56:18.858547Z",
          "iopub.status.idle": "2023-07-02T07:56:44.322293Z",
          "shell.execute_reply.started": "2023-07-02T07:56:18.858516Z",
          "shell.execute_reply": "2023-07-02T07:56:44.321308Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTnopjHJ7YI2",
        "outputId": "c885438b-53a6-4343-b8d4-3f149efe30cc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.159632205963135\n",
            "Iteration:  100 loss:  3.195927858352661\n",
            "Iteration:  200 loss:  2.655423402786255\n",
            "Iteration:  300 loss:  2.89151668548584\n",
            "Iteration:  400 loss:  2.997866153717041\n",
            "Iteration:  500 loss:  2.600587844848633\n",
            "Iteration:  600 loss:  2.653357744216919\n",
            "Iteration:  700 loss:  2.2459821701049805\n",
            "Iteration:  800 loss:  2.2778639793395996\n",
            "Iteration:  900 loss:  2.4577383995056152\n",
            "Iteration:  1000 loss:  2.726426124572754\n",
            "Iteration:  1100 loss:  2.799111843109131\n",
            "Iteration:  1200 loss:  2.774461269378662\n",
            "Iteration:  1300 loss:  2.6170904636383057\n",
            "Iteration:  1400 loss:  2.7802963256835938\n",
            "Iteration:  1500 loss:  3.0328149795532227\n",
            "Iteration:  1600 loss:  2.81746244430542\n",
            "Iteration:  1700 loss:  2.4856786727905273\n",
            "Iteration:  1800 loss:  2.6582698822021484\n",
            "Iteration:  1900 loss:  2.957282304763794\n",
            "Iteration:  2000 loss:  2.7884037494659424\n",
            "Iteration:  2100 loss:  2.6578800678253174\n",
            "Iteration:  2200 loss:  2.7966501712799072\n",
            "Iteration:  2300 loss:  2.9844810962677\n",
            "Iteration:  2400 loss:  2.406085968017578\n",
            "Iteration:  2500 loss:  2.6345043182373047\n",
            "Iteration:  2600 loss:  2.5631027221679688\n",
            "Iteration:  2700 loss:  2.692261219024658\n",
            "Iteration:  2800 loss:  2.5485565662384033\n",
            "Iteration:  2900 loss:  2.3362581729888916\n",
            "Iteration:  3000 loss:  2.530470371246338\n",
            "Iteration:  3100 loss:  2.9664855003356934\n",
            "Iteration:  3200 loss:  2.4236719608306885\n",
            "Iteration:  3300 loss:  2.714750289916992\n",
            "Iteration:  3400 loss:  2.1078381538391113\n",
            "Iteration:  3500 loss:  2.2221248149871826\n",
            "Iteration:  3600 loss:  2.5680336952209473\n",
            "Iteration:  3700 loss:  2.335314989089966\n",
            "Iteration:  3800 loss:  2.7860045433044434\n",
            "Iteration:  3900 loss:  2.629441499710083\n",
            "Iteration:  4000 loss:  2.4171762466430664\n",
            "Iteration:  4100 loss:  2.730562686920166\n",
            "Iteration:  4200 loss:  2.695068359375\n",
            "Iteration:  4300 loss:  2.486112356185913\n",
            "Iteration:  4400 loss:  2.6046061515808105\n",
            "Iteration:  4500 loss:  2.7973554134368896\n",
            "Iteration:  4600 loss:  2.31557297706604\n",
            "Iteration:  4700 loss:  2.292264461517334\n",
            "Iteration:  4800 loss:  2.7251405715942383\n",
            "Iteration:  4900 loss:  2.190192222595215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.19, that's nice!"
      ],
      "metadata": {
        "id": "DyU94N_XCxge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding MLP to Encoder Block"
      ],
      "metadata": {
        "id": "Dhwz8Ojw7YI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, embed_dim, drop_p):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)\n",
        "        self.act = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:56:44.326972Z",
          "iopub.execute_input": "2023-07-02T07:56:44.327290Z",
          "iopub.status.idle": "2023-07-02T07:56:44.333858Z",
          "shell.execute_reply.started": "2023-07-02T07:56:44.327266Z",
          "shell.execute_reply": "2023-07-02T07:56:44.332974Z"
        },
        "trusted": true,
        "id": "jEkMBlba7YI2"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n",
        "        self.mlp = MLP(embed_dim, drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.multi_head_self_attention(x)\n",
        "        x = self.mlp(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:56:44.335136Z",
          "iopub.execute_input": "2023-07-02T07:56:44.336080Z",
          "iopub.status.idle": "2023-07-02T07:56:44.350631Z",
          "shell.execute_reply.started": "2023-07-02T07:56:44.336046Z",
          "shell.execute_reply": "2023-07-02T07:56:44.349682Z"
        },
        "trusted": true,
        "id": "5gVKj-Ah7YI2"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder_block = EncoderBlock(sequence_length, embed_dim, n_heads, drop_p)\n",
        "        self.linear_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        token_embeddings = self.token_embedding(x) # (B, T, C)\n",
        "        feature_maps = self.encoder_block(token_embeddings)\n",
        "        out = self.linear_head(feature_maps)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:56:44.351837Z",
          "iopub.execute_input": "2023-07-02T07:56:44.353056Z",
          "iopub.status.idle": "2023-07-02T07:56:44.360921Z",
          "shell.execute_reply.started": "2023-07-02T07:56:44.353009Z",
          "shell.execute_reply": "2023-07-02T07:56:44.359932Z"
        },
        "trusted": true,
        "id": "Vq0geRzx7YI2"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_heads=4, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:56:44.362289Z",
          "iopub.execute_input": "2023-07-02T07:56:44.362750Z",
          "iopub.status.idle": "2023-07-02T07:56:44.374750Z",
          "shell.execute_reply.started": "2023-07-02T07:56:44.362720Z",
          "shell.execute_reply": "2023-07-02T07:56:44.373695Z"
        },
        "trusted": true,
        "id": "iZ6RRWgM7YI2"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:56:44.376346Z",
          "iopub.execute_input": "2023-07-02T07:56:44.376750Z",
          "iopub.status.idle": "2023-07-02T07:57:12.733203Z",
          "shell.execute_reply.started": "2023-07-02T07:56:44.376721Z",
          "shell.execute_reply": "2023-07-02T07:57:12.732226Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhNLNzgQ7YI2",
        "outputId": "59377af3-ade6-4992-ddc3-4e40bf219cc1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.18694543838501\n",
            "Iteration:  100 loss:  3.269418239593506\n",
            "Iteration:  200 loss:  3.4120898246765137\n",
            "Iteration:  300 loss:  3.0579967498779297\n",
            "Iteration:  400 loss:  2.9326906204223633\n",
            "Iteration:  500 loss:  3.2572896480560303\n",
            "Iteration:  600 loss:  2.935234546661377\n",
            "Iteration:  700 loss:  2.517719268798828\n",
            "Iteration:  800 loss:  2.6395318508148193\n",
            "Iteration:  900 loss:  2.949025869369507\n",
            "Iteration:  1000 loss:  2.3955841064453125\n",
            "Iteration:  1100 loss:  2.8977410793304443\n",
            "Iteration:  1200 loss:  2.3214683532714844\n",
            "Iteration:  1300 loss:  2.7579526901245117\n",
            "Iteration:  1400 loss:  2.567401170730591\n",
            "Iteration:  1500 loss:  2.4108779430389404\n",
            "Iteration:  1600 loss:  2.827171802520752\n",
            "Iteration:  1700 loss:  2.6805806159973145\n",
            "Iteration:  1800 loss:  2.830517530441284\n",
            "Iteration:  1900 loss:  2.3941259384155273\n",
            "Iteration:  2000 loss:  2.2639944553375244\n",
            "Iteration:  2100 loss:  2.4043500423431396\n",
            "Iteration:  2200 loss:  2.6645922660827637\n",
            "Iteration:  2300 loss:  2.6768815517425537\n",
            "Iteration:  2400 loss:  2.2338502407073975\n",
            "Iteration:  2500 loss:  2.593297004699707\n",
            "Iteration:  2600 loss:  2.5191915035247803\n",
            "Iteration:  2700 loss:  2.6131253242492676\n",
            "Iteration:  2800 loss:  2.8601794242858887\n",
            "Iteration:  2900 loss:  2.7384114265441895\n",
            "Iteration:  3000 loss:  2.6762380599975586\n",
            "Iteration:  3100 loss:  2.351551055908203\n",
            "Iteration:  3200 loss:  2.88826060295105\n",
            "Iteration:  3300 loss:  2.404740571975708\n",
            "Iteration:  3400 loss:  2.3045363426208496\n",
            "Iteration:  3500 loss:  2.2880923748016357\n",
            "Iteration:  3600 loss:  2.13330340385437\n",
            "Iteration:  3700 loss:  2.6536495685577393\n",
            "Iteration:  3800 loss:  2.6626198291778564\n",
            "Iteration:  3900 loss:  2.1697332859039307\n",
            "Iteration:  4000 loss:  2.8771462440490723\n",
            "Iteration:  4100 loss:  2.361020565032959\n",
            "Iteration:  4200 loss:  2.5904674530029297\n",
            "Iteration:  4300 loss:  2.4660439491271973\n",
            "Iteration:  4400 loss:  2.619767665863037\n",
            "Iteration:  4500 loss:  2.554563045501709\n",
            "Iteration:  4600 loss:  2.1508724689483643\n",
            "Iteration:  4700 loss:  2.4215095043182373\n",
            "Iteration:  4800 loss:  2.459123373031616\n",
            "Iteration:  4900 loss:  2.1608099937438965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.16, not bad!"
      ],
      "metadata": {
        "id": "BgfRFifVDPmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking Encoder Blocks + Increased Sequence Length"
      ],
      "metadata": {
        "id": "ITUizL397YI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, sequence_length, embed_dim, n_blocks, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder_blocks = nn.Sequential(*([EncoderBlock(sequence_length, embed_dim, n_heads, drop_p) for x in range(n_blocks)]))\n",
        "        self.linear_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        token_embeddings = self.token_embedding(x) # (B, T, C)\n",
        "        feature_maps = self.encoder_blocks(token_embeddings)\n",
        "        out = self.linear_head(feature_maps)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:57:12.734650Z",
          "iopub.execute_input": "2023-07-02T07:57:12.735020Z",
          "iopub.status.idle": "2023-07-02T07:57:12.742697Z",
          "shell.execute_reply.started": "2023-07-02T07:57:12.734987Z",
          "shell.execute_reply": "2023-07-02T07:57:12.741802Z"
        },
        "trusted": true,
        "id": "mE6tf91q7YI2"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=8, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:57:12.743951Z",
          "iopub.execute_input": "2023-07-02T07:57:12.744710Z",
          "iopub.status.idle": "2023-07-02T07:57:12.772950Z",
          "shell.execute_reply.started": "2023-07-02T07:57:12.744679Z",
          "shell.execute_reply": "2023-07-02T07:57:12.772081Z"
        },
        "trusted": true,
        "id": "khiNwj3P7YI2"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:57:12.776316Z",
          "iopub.execute_input": "2023-07-02T07:57:12.776597Z",
          "iopub.status.idle": "2023-07-02T07:58:46.351453Z",
          "shell.execute_reply.started": "2023-07-02T07:57:12.776574Z",
          "shell.execute_reply": "2023-07-02T07:58:46.350286Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvztbz5C7YI2",
        "outputId": "7729f61c-9753-4227-fe7c-4c3ede616cd7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.1503071784973145\n",
            "Iteration:  100 loss:  3.292788505554199\n",
            "Iteration:  200 loss:  3.6172516345977783\n",
            "Iteration:  300 loss:  3.282633066177368\n",
            "Iteration:  400 loss:  3.3533828258514404\n",
            "Iteration:  500 loss:  3.4132888317108154\n",
            "Iteration:  600 loss:  3.3911702632904053\n",
            "Iteration:  700 loss:  3.4006588459014893\n",
            "Iteration:  800 loss:  3.3593051433563232\n",
            "Iteration:  900 loss:  3.2086079120635986\n",
            "Iteration:  1000 loss:  2.9971401691436768\n",
            "Iteration:  1100 loss:  3.20408296585083\n",
            "Iteration:  1200 loss:  3.0765509605407715\n",
            "Iteration:  1300 loss:  2.9727540016174316\n",
            "Iteration:  1400 loss:  2.9958503246307373\n",
            "Iteration:  1500 loss:  3.3397715091705322\n",
            "Iteration:  1600 loss:  3.08198881149292\n",
            "Iteration:  1700 loss:  3.0465545654296875\n",
            "Iteration:  1800 loss:  3.390258312225342\n",
            "Iteration:  1900 loss:  3.151524782180786\n",
            "Iteration:  2000 loss:  3.6266586780548096\n",
            "Iteration:  2100 loss:  3.064724922180176\n",
            "Iteration:  2200 loss:  3.449143648147583\n",
            "Iteration:  2300 loss:  2.7588229179382324\n",
            "Iteration:  2400 loss:  2.9096670150756836\n",
            "Iteration:  2500 loss:  2.910416841506958\n",
            "Iteration:  2600 loss:  3.097141981124878\n",
            "Iteration:  2700 loss:  3.267282724380493\n",
            "Iteration:  2800 loss:  3.212775707244873\n",
            "Iteration:  2900 loss:  3.2138659954071045\n",
            "Iteration:  3000 loss:  3.2010440826416016\n",
            "Iteration:  3100 loss:  2.960890293121338\n",
            "Iteration:  3200 loss:  3.280336856842041\n",
            "Iteration:  3300 loss:  3.1465578079223633\n",
            "Iteration:  3400 loss:  3.267688512802124\n",
            "Iteration:  3500 loss:  3.500852584838867\n",
            "Iteration:  3600 loss:  3.268528699874878\n",
            "Iteration:  3700 loss:  3.057939291000366\n",
            "Iteration:  3800 loss:  3.0310862064361572\n",
            "Iteration:  3900 loss:  3.1679792404174805\n",
            "Iteration:  4000 loss:  3.0404345989227295\n",
            "Iteration:  4100 loss:  3.172600746154785\n",
            "Iteration:  4200 loss:  3.241060972213745\n",
            "Iteration:  4300 loss:  2.895282745361328\n",
            "Iteration:  4400 loss:  2.9541587829589844\n",
            "Iteration:  4500 loss:  2.977623224258423\n",
            "Iteration:  4600 loss:  3.127392292022705\n",
            "Iteration:  4700 loss:  3.314746856689453\n",
            "Iteration:  4800 loss:  3.2138946056365967\n",
            "Iteration:  4900 loss:  3.3603310585021973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increased loss significantly! But let's keep our faith on the transformer architecture and keep adding the components one by one."
      ],
      "metadata": {
        "id": "Zom5AoG3Ee1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=32, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device) # Increasing sequence length from 8 to 32"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:58:46.353014Z",
          "iopub.execute_input": "2023-07-02T07:58:46.353395Z",
          "iopub.status.idle": "2023-07-02T07:58:46.372874Z",
          "shell.execute_reply.started": "2023-07-02T07:58:46.353363Z",
          "shell.execute_reply": "2023-07-02T07:58:46.371999Z"
        },
        "trusted": true,
        "id": "HYW29CwJ7YI2"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    for iteration in range(5000):\n",
        "        data, labels = get_batch(32, 16)\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(data)\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        labels = labels.view(B*T)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        if iteration % 100 == 0:\n",
        "            print(\"Iteration: \", iteration, \"loss: \", loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:58:46.374129Z",
          "iopub.execute_input": "2023-07-02T07:58:46.374964Z",
          "iopub.status.idle": "2023-07-02T07:58:46.382448Z",
          "shell.execute_reply.started": "2023-07-02T07:58:46.374931Z",
          "shell.execute_reply": "2023-07-02T07:58:46.381470Z"
        },
        "trusted": true,
        "id": "gqCPc0pj7YI2"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T07:58:46.384073Z",
          "iopub.execute_input": "2023-07-02T07:58:46.385643Z",
          "iopub.status.idle": "2023-07-02T08:00:21.690016Z",
          "shell.execute_reply.started": "2023-07-02T07:58:46.385584Z",
          "shell.execute_reply": "2023-07-02T08:00:21.689044Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb23Oxo07YI3",
        "outputId": "df78a583-b4f0-4b27-a9b8-2927fc75aac5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.140641212463379\n",
            "Iteration:  100 loss:  3.35632061958313\n",
            "Iteration:  200 loss:  3.3990018367767334\n",
            "Iteration:  300 loss:  3.2380740642547607\n",
            "Iteration:  400 loss:  3.1847801208496094\n",
            "Iteration:  500 loss:  3.243692636489868\n",
            "Iteration:  600 loss:  3.20573091506958\n",
            "Iteration:  700 loss:  3.213630199432373\n",
            "Iteration:  800 loss:  3.1766393184661865\n",
            "Iteration:  900 loss:  3.2871124744415283\n",
            "Iteration:  1000 loss:  3.3521170616149902\n",
            "Iteration:  1100 loss:  3.2980587482452393\n",
            "Iteration:  1200 loss:  3.3027021884918213\n",
            "Iteration:  1300 loss:  3.3135128021240234\n",
            "Iteration:  1400 loss:  3.2884726524353027\n",
            "Iteration:  1500 loss:  3.367671251296997\n",
            "Iteration:  1600 loss:  3.3335530757904053\n",
            "Iteration:  1700 loss:  3.357274055480957\n",
            "Iteration:  1800 loss:  3.304220676422119\n",
            "Iteration:  1900 loss:  3.3521056175231934\n",
            "Iteration:  2000 loss:  3.281970977783203\n",
            "Iteration:  2100 loss:  3.4233953952789307\n",
            "Iteration:  2200 loss:  3.287477731704712\n",
            "Iteration:  2300 loss:  3.3061769008636475\n",
            "Iteration:  2400 loss:  3.2971906661987305\n",
            "Iteration:  2500 loss:  3.3387646675109863\n",
            "Iteration:  2600 loss:  3.384559392929077\n",
            "Iteration:  2700 loss:  3.330836296081543\n",
            "Iteration:  2800 loss:  3.215864896774292\n",
            "Iteration:  2900 loss:  3.4637577533721924\n",
            "Iteration:  3000 loss:  3.3607590198516846\n",
            "Iteration:  3100 loss:  3.4660162925720215\n",
            "Iteration:  3200 loss:  3.3198933601379395\n",
            "Iteration:  3300 loss:  3.3520405292510986\n",
            "Iteration:  3400 loss:  3.3609652519226074\n",
            "Iteration:  3500 loss:  3.3292903900146484\n",
            "Iteration:  3600 loss:  3.1424901485443115\n",
            "Iteration:  3700 loss:  3.259402275085449\n",
            "Iteration:  3800 loss:  3.307650566101074\n",
            "Iteration:  3900 loss:  3.343825578689575\n",
            "Iteration:  4000 loss:  3.3592045307159424\n",
            "Iteration:  4100 loss:  3.2141685485839844\n",
            "Iteration:  4200 loss:  3.28827166557312\n",
            "Iteration:  4300 loss:  3.323546886444092\n",
            "Iteration:  4400 loss:  3.5306689739227295\n",
            "Iteration:  4500 loss:  3.2643775939941406\n",
            "Iteration:  4600 loss:  3.3477463722229004\n",
            "Iteration:  4700 loss:  3.171347141265869\n",
            "Iteration:  4800 loss:  3.3993353843688965\n",
            "Iteration:  4900 loss:  3.255391836166382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance seems to have worsened ever since we made more deeper architecture. Now, since the model has become quite deeper, it's time to care about model optimization i.e skip connections for vanishing gradient and layer normalization"
      ],
      "metadata": {
        "id": "8Ecs5n8N7YI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Skip Connections and Layer Normalization"
      ],
      "metadata": {
        "id": "ilF32fMK7YI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.multi_head_self_attention(self.layer_norm(x))\n",
        "        x = x + self.mlp(self.layer_norm(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:00:21.691795Z",
          "iopub.execute_input": "2023-07-02T08:00:21.692175Z",
          "iopub.status.idle": "2023-07-02T08:00:21.701060Z",
          "shell.execute_reply.started": "2023-07-02T08:00:21.692145Z",
          "shell.execute_reply": "2023-07-02T08:00:21.700082Z"
        },
        "trusted": true,
        "id": "UyQYpphY7YI3"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=32, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:00:21.702862Z",
          "iopub.execute_input": "2023-07-02T08:00:21.703251Z",
          "iopub.status.idle": "2023-07-02T08:00:21.725861Z",
          "shell.execute_reply.started": "2023-07-02T08:00:21.703219Z",
          "shell.execute_reply": "2023-07-02T08:00:21.724983Z"
        },
        "trusted": true,
        "id": "Dmaef0aM7YI3"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:00:21.726997Z",
          "iopub.execute_input": "2023-07-02T08:00:21.727327Z",
          "iopub.status.idle": "2023-07-02T08:02:02.386814Z",
          "shell.execute_reply.started": "2023-07-02T08:00:21.727296Z",
          "shell.execute_reply": "2023-07-02T08:02:02.385834Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5cdXTd27YI3",
        "outputId": "4cb121a5-1951-44f1-aff7-ad91f1f7fa05"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.384712219238281\n",
            "Iteration:  100 loss:  2.4820759296417236\n",
            "Iteration:  200 loss:  2.526989459991455\n",
            "Iteration:  300 loss:  2.361846685409546\n",
            "Iteration:  400 loss:  2.443568468093872\n",
            "Iteration:  500 loss:  2.406647205352783\n",
            "Iteration:  600 loss:  2.289095640182495\n",
            "Iteration:  700 loss:  2.3551816940307617\n",
            "Iteration:  800 loss:  2.3736507892608643\n",
            "Iteration:  900 loss:  2.288585901260376\n",
            "Iteration:  1000 loss:  2.2525463104248047\n",
            "Iteration:  1100 loss:  2.3462138175964355\n",
            "Iteration:  1200 loss:  2.2804760932922363\n",
            "Iteration:  1300 loss:  2.296062707901001\n",
            "Iteration:  1400 loss:  2.2923262119293213\n",
            "Iteration:  1500 loss:  2.3139076232910156\n",
            "Iteration:  1600 loss:  2.2138051986694336\n",
            "Iteration:  1700 loss:  2.184255599975586\n",
            "Iteration:  1800 loss:  2.270749092102051\n",
            "Iteration:  1900 loss:  2.245482921600342\n",
            "Iteration:  2000 loss:  2.143630027770996\n",
            "Iteration:  2100 loss:  2.1034204959869385\n",
            "Iteration:  2200 loss:  2.2479631900787354\n",
            "Iteration:  2300 loss:  2.266583204269409\n",
            "Iteration:  2400 loss:  2.177739381790161\n",
            "Iteration:  2500 loss:  2.2543699741363525\n",
            "Iteration:  2600 loss:  2.0650784969329834\n",
            "Iteration:  2700 loss:  2.16170334815979\n",
            "Iteration:  2800 loss:  2.252370834350586\n",
            "Iteration:  2900 loss:  2.1695477962493896\n",
            "Iteration:  3000 loss:  2.054295778274536\n",
            "Iteration:  3100 loss:  2.154226779937744\n",
            "Iteration:  3200 loss:  2.1478919982910156\n",
            "Iteration:  3300 loss:  2.070403814315796\n",
            "Iteration:  3400 loss:  2.142730712890625\n",
            "Iteration:  3500 loss:  2.1676435470581055\n",
            "Iteration:  3600 loss:  2.1249866485595703\n",
            "Iteration:  3700 loss:  2.1958611011505127\n",
            "Iteration:  3800 loss:  2.1098031997680664\n",
            "Iteration:  3900 loss:  2.1165034770965576\n",
            "Iteration:  4000 loss:  2.0647835731506348\n",
            "Iteration:  4100 loss:  2.069084882736206\n",
            "Iteration:  4200 loss:  2.108017921447754\n",
            "Iteration:  4300 loss:  2.07259464263916\n",
            "Iteration:  4400 loss:  2.177293300628662\n",
            "Iteration:  4500 loss:  2.107799768447876\n",
            "Iteration:  4600 loss:  2.0945851802825928\n",
            "Iteration:  4700 loss:  2.131939172744751\n",
            "Iteration:  4800 loss:  2.1840708255767822\n",
            "Iteration:  4900 loss:  2.0312507152557373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last trick to do: Add positional embedding"
      ],
      "metadata": {
        "id": "9MZbOj5Q7YI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Positional Embedding"
      ],
      "metadata": {
        "id": "wLtP5xIs7YI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, sequence_length, embed_dim, n_blocks, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = torch.nn.Embedding(sequence_length, embed_dim)\n",
        "        self.encoder_blocks = nn.Sequential(*([EncoderBlock(sequence_length, embed_dim, n_heads, drop_p) for x in range(n_blocks)]))\n",
        "        self.linear_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "\n",
        "        token_embeddings = self.token_embedding(x) # (B, T, C)\n",
        "        positional_embeddings = self.pos_embedding(torch.arange(T, device=device)) # (T, C)\n",
        "        embeddings = token_embeddings + positional_embeddings # B, T, C cause broadcasting\n",
        "        feature_maps = self.encoder_blocks(embeddings)\n",
        "        out = self.linear_head(feature_maps)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:02:02.388505Z",
          "iopub.execute_input": "2023-07-02T08:02:02.388856Z",
          "iopub.status.idle": "2023-07-02T08:02:02.398534Z",
          "shell.execute_reply.started": "2023-07-02T08:02:02.388824Z",
          "shell.execute_reply": "2023-07-02T08:02:02.396415Z"
        },
        "trusted": true,
        "id": "9zh19asl7YI3"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=32, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:02:02.399828Z",
          "iopub.execute_input": "2023-07-02T08:02:02.401446Z",
          "iopub.status.idle": "2023-07-02T08:02:02.421335Z",
          "shell.execute_reply.started": "2023-07-02T08:02:02.401410Z",
          "shell.execute_reply": "2023-07-02T08:02:02.420513Z"
        },
        "trusted": true,
        "id": "__wmARNL7YI3"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:02:02.422847Z",
          "iopub.execute_input": "2023-07-02T08:02:02.423206Z",
          "iopub.status.idle": "2023-07-02T08:03:44.168527Z",
          "shell.execute_reply.started": "2023-07-02T08:02:02.423178Z",
          "shell.execute_reply": "2023-07-02T08:03:44.167534Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyBD4yJ77YI3",
        "outputId": "84d13c51-cf7a-4cd3-fc9d-725fbc1737b9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.535435199737549\n",
            "Iteration:  100 loss:  2.6046290397644043\n",
            "Iteration:  200 loss:  2.6095778942108154\n",
            "Iteration:  300 loss:  2.5166263580322266\n",
            "Iteration:  400 loss:  2.438232421875\n",
            "Iteration:  500 loss:  2.371281385421753\n",
            "Iteration:  600 loss:  2.3521194458007812\n",
            "Iteration:  700 loss:  2.2881627082824707\n",
            "Iteration:  800 loss:  2.2664847373962402\n",
            "Iteration:  900 loss:  2.252852201461792\n",
            "Iteration:  1000 loss:  2.092099189758301\n",
            "Iteration:  1100 loss:  2.164508819580078\n",
            "Iteration:  1200 loss:  2.1726717948913574\n",
            "Iteration:  1300 loss:  2.156724452972412\n",
            "Iteration:  1400 loss:  2.123415231704712\n",
            "Iteration:  1500 loss:  2.2109720706939697\n",
            "Iteration:  1600 loss:  2.04011869430542\n",
            "Iteration:  1700 loss:  1.9385355710983276\n",
            "Iteration:  1800 loss:  1.981712818145752\n",
            "Iteration:  1900 loss:  1.9419118165969849\n",
            "Iteration:  2000 loss:  1.9631212949752808\n",
            "Iteration:  2100 loss:  2.0360653400421143\n",
            "Iteration:  2200 loss:  2.014402389526367\n",
            "Iteration:  2300 loss:  1.9774655103683472\n",
            "Iteration:  2400 loss:  2.057332754135132\n",
            "Iteration:  2500 loss:  1.9748766422271729\n",
            "Iteration:  2600 loss:  2.034456253051758\n",
            "Iteration:  2700 loss:  1.9425424337387085\n",
            "Iteration:  2800 loss:  2.064265727996826\n",
            "Iteration:  2900 loss:  2.019362449645996\n",
            "Iteration:  3000 loss:  2.0676333904266357\n",
            "Iteration:  3100 loss:  2.028778314590454\n",
            "Iteration:  3200 loss:  1.8955750465393066\n",
            "Iteration:  3300 loss:  1.8884773254394531\n",
            "Iteration:  3400 loss:  1.9628589153289795\n",
            "Iteration:  3500 loss:  1.8708710670471191\n",
            "Iteration:  3600 loss:  1.867039442062378\n",
            "Iteration:  3700 loss:  1.8861221075057983\n",
            "Iteration:  3800 loss:  1.992639422416687\n",
            "Iteration:  3900 loss:  1.9603228569030762\n",
            "Iteration:  4000 loss:  1.8492456674575806\n",
            "Iteration:  4100 loss:  1.9452427625656128\n",
            "Iteration:  4200 loss:  1.8930240869522095\n",
            "Iteration:  4300 loss:  1.9203617572784424\n",
            "Iteration:  4400 loss:  2.0734944343566895\n",
            "Iteration:  4500 loss:  1.8848158121109009\n",
            "Iteration:  4600 loss:  1.8325635194778442\n",
            "Iteration:  4700 loss:  1.841210961341858\n",
            "Iteration:  4800 loss:  1.7005860805511475\n",
            "Iteration:  4900 loss:  1.8703484535217285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "sequence_length=32\n",
        "with torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate\n",
        "    start_char = \"\\n\"\n",
        "    start_token = torch.tensor(encode(start_char)).unsqueeze(0)\n",
        "    print(\"start_token shape: \", start_token.shape)\n",
        "    current_token = start_token.to(device) # (B, T)\n",
        "    generated_tokens = []\n",
        "    for x in range(1000):\n",
        "        idx = current_token[:, -sequence_length:]\n",
        "        logits = model(idx) # (B, T, C)\n",
        "        logits = logits[:, -1, :] # (B, C)\n",
        "        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n",
        "        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n",
        "        generated_tokens.append(next_token.item())\n",
        "        current_token = torch.cat((current_token, next_token), dim=1).to(device) # (B, T+1)\n",
        "    print(decode(generated_tokens))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:03:44.170160Z",
          "iopub.execute_input": "2023-07-02T08:03:44.170530Z",
          "iopub.status.idle": "2023-07-02T08:03:49.674050Z",
          "shell.execute_reply.started": "2023-07-02T08:03:44.170498Z",
          "shell.execute_reply": "2023-07-02T08:03:49.672924Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxKh-WSo7YI3",
        "outputId": "111cc417-daea-415a-9c92-52202084738d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_token shape:  torch.Size([1, 1])\n",
            "to oFfaice your und?\n",
            "\n",
            "MEY:\n",
            "Your thou not ways thy voolds: may shull by folle,\n",
            "If lifty more disertise of brown,\n",
            "To belted priower's my strict.\n",
            "\n",
            "JUEEN LERD Pernouts, lim, some you,\n",
            "Mave rociry hear for your with have curd;\n",
            "And are dlant, you\n",
            "I now speake adving be'll, and hall lian in ham; onds, sull is earther, I deation to't apond aster thou strock'd the upon liviles:\n",
            "To can thou cove wornd teare oft, and yearthing their creare,, uKy,\n",
            "Thil the were he woun your bege\n",
            "The morna grain to come wraitickah,\n",
            "Tyue, where roct is madidie\n",
            "Thy freasul gods with uneessed so foress for up as some with bish sir\n",
            "Tear rove kin revery dees have and beatius\n",
            "To theres berserver of the knands had.\n",
            "\n",
            "BUCKEN? YORKE VIO:\n",
            "He which upe, the sween heath strome be bave of in\n",
            "My stay drouse sloveing that is sir:\n",
            "Some of up my forduder\n",
            "As cryalt as iseet to downgbong wifes\n",
            "\n",
            "TORKE VINIUS:\n",
            "Are shall all of fiendstorrow in\n",
            "Ways more.\n",
            "\n",
            "AUTESS:\n",
            "That You a meay\n",
            "Whome him, you pot in as doe ell him.\n",
            "\n",
            "Fings We Ritherichme\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add LayerNorm"
      ],
      "metadata": {
        "id": "qHaIXGKEHF9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, sequence_length, embed_dim, n_blocks, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = torch.nn.Embedding(sequence_length, embed_dim)\n",
        "        self.encoder_blocks = nn.Sequential(*([EncoderBlock(sequence_length, embed_dim, n_heads, drop_p) for x in range(n_blocks)]))\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.linear_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "\n",
        "        token_embeddings = self.token_embedding(x) # (B, T, C)\n",
        "        positional_embeddings = self.pos_embedding(torch.arange(T, device=device)) # (T, C)\n",
        "        embeddings = token_embeddings + positional_embeddings # B, T, C cause broadcasting\n",
        "        feature_maps = self.encoder_blocks(embeddings)\n",
        "        x = self.layer_norm(feature_maps)\n",
        "        x = self.linear_head(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:03:49.679843Z",
          "iopub.execute_input": "2023-07-02T08:03:49.680158Z",
          "iopub.status.idle": "2023-07-02T08:03:49.688868Z",
          "shell.execute_reply.started": "2023-07-02T08:03:49.680134Z",
          "shell.execute_reply": "2023-07-02T08:03:49.687801Z"
        },
        "trusted": true,
        "id": "o9Uj9tBT7YI3"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, sequence_length=32, embed_dim=64, n_blocks = 4, n_heads=4, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:03:49.690384Z",
          "iopub.execute_input": "2023-07-02T08:03:49.691074Z",
          "iopub.status.idle": "2023-07-02T08:03:49.717051Z",
          "shell.execute_reply.started": "2023-07-02T08:03:49.691041Z",
          "shell.execute_reply": "2023-07-02T08:03:49.716132Z"
        },
        "trusted": true,
        "id": "porV56-W7YI3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:03:49.718502Z",
          "iopub.execute_input": "2023-07-02T08:03:49.718851Z",
          "iopub.status.idle": "2023-07-02T08:05:31.883568Z",
          "shell.execute_reply.started": "2023-07-02T08:03:49.718820Z",
          "shell.execute_reply": "2023-07-02T08:05:31.882445Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYH1EhUC7YI3",
        "outputId": "c72d858d-0720-419d-b597-ca2203088cfe"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  4.316089630126953\n",
            "Iteration:  100 loss:  2.807663679122925\n",
            "Iteration:  200 loss:  2.61960768699646\n",
            "Iteration:  300 loss:  2.4380409717559814\n",
            "Iteration:  400 loss:  2.454068183898926\n",
            "Iteration:  500 loss:  2.356218099594116\n",
            "Iteration:  600 loss:  2.2856268882751465\n",
            "Iteration:  700 loss:  2.264251232147217\n",
            "Iteration:  800 loss:  2.348238229751587\n",
            "Iteration:  900 loss:  2.209446907043457\n",
            "Iteration:  1000 loss:  2.1343467235565186\n",
            "Iteration:  1100 loss:  2.2199947834014893\n",
            "Iteration:  1200 loss:  2.1353843212127686\n",
            "Iteration:  1300 loss:  2.1119725704193115\n",
            "Iteration:  1400 loss:  2.100851535797119\n",
            "Iteration:  1500 loss:  2.1556591987609863\n",
            "Iteration:  1600 loss:  2.113408088684082\n",
            "Iteration:  1700 loss:  2.104343891143799\n",
            "Iteration:  1800 loss:  2.140507459640503\n",
            "Iteration:  1900 loss:  2.0200841426849365\n",
            "Iteration:  2000 loss:  2.0337531566619873\n",
            "Iteration:  2100 loss:  2.08750057220459\n",
            "Iteration:  2200 loss:  2.0656230449676514\n",
            "Iteration:  2300 loss:  2.0059564113616943\n",
            "Iteration:  2400 loss:  2.0139877796173096\n",
            "Iteration:  2500 loss:  1.9306896924972534\n",
            "Iteration:  2600 loss:  1.91768217086792\n",
            "Iteration:  2700 loss:  1.9595574140548706\n",
            "Iteration:  2800 loss:  2.0105738639831543\n",
            "Iteration:  2900 loss:  1.880821943283081\n",
            "Iteration:  3000 loss:  1.8583741188049316\n",
            "Iteration:  3100 loss:  1.8722116947174072\n",
            "Iteration:  3200 loss:  1.9216581583023071\n",
            "Iteration:  3300 loss:  1.9256072044372559\n",
            "Iteration:  3400 loss:  1.9056216478347778\n",
            "Iteration:  3500 loss:  1.9548001289367676\n",
            "Iteration:  3600 loss:  1.9315848350524902\n",
            "Iteration:  3700 loss:  1.8841224908828735\n",
            "Iteration:  3800 loss:  2.029266357421875\n",
            "Iteration:  3900 loss:  1.9270151853561401\n",
            "Iteration:  4000 loss:  1.9250569343566895\n",
            "Iteration:  4100 loss:  1.9081522226333618\n",
            "Iteration:  4200 loss:  1.9043231010437012\n",
            "Iteration:  4300 loss:  1.8716729879379272\n",
            "Iteration:  4400 loss:  1.945001244544983\n",
            "Iteration:  4500 loss:  1.8120222091674805\n",
            "Iteration:  4600 loss:  1.8631435632705688\n",
            "Iteration:  4700 loss:  1.8537908792495728\n",
            "Iteration:  4800 loss:  1.886850357055664\n",
            "Iteration:  4900 loss:  1.925036072731018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "sequence_length=32\n",
        "with torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate\n",
        "    start_char = \"\\n\"\n",
        "    start_token = torch.tensor(encode(start_char)).unsqueeze(0)\n",
        "    print(\"start_token shape: \", start_token.shape)\n",
        "    current_token = start_token.to(device) # (B, T)\n",
        "    generated_tokens = []\n",
        "    for x in range(1000):\n",
        "        idx = current_token[:, -sequence_length:]\n",
        "        logits = model(idx) # (B, T, C)\n",
        "        logits = logits[:, -1, :] # (B, C)\n",
        "        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n",
        "        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n",
        "        generated_tokens.append(next_token.item())\n",
        "        current_token = torch.cat((current_token, next_token), dim=1).to(device) # (B, T+1)\n",
        "    print(decode(generated_tokens))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:05:31.885019Z",
          "iopub.execute_input": "2023-07-02T08:05:31.885378Z",
          "iopub.status.idle": "2023-07-02T08:05:37.409843Z",
          "shell.execute_reply.started": "2023-07-02T08:05:31.885347Z",
          "shell.execute_reply": "2023-07-02T08:05:37.408888Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g9Xg0vq7YI4",
        "outputId": "95e2d04a-577b-4f5e-cbf4-b2d0031f1a11"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_token shape:  torch.Size([1, 1])\n",
            "Thesengre.\n",
            "\n",
            "DUKE VINCE:\n",
            "Them to mear it no's munk thre, I he pooter meak far he down: the cramenterfule,\n",
            "And meast'd tight Rided! with the and shall dam what of tat const. Thus erruin father, and mysenats set,\n",
            "Corce neve your calonsince, honou my is sith the all.\n",
            "\n",
            "KTHERTH:\n",
            "I praventle hus the kay, abosonglary plordfer,\n",
            "No when befea; Good slake the wordss their hy fracetuar's?\n",
            "I his seepts, from me thou facts decone.\n",
            "Ster! So to be\n",
            "Thot so saves mobe enewn: I your hine the knay?\n",
            "\n",
            "CLETBERLA:\n",
            "To be staught ven it the legwastilencen,\n",
            "As the the resed ye words, I lord now Rickings, what broth is:\n",
            "You have I ban blarke or in Yow. OF Of Thinks,\n",
            "And in hand which encesentence, and fexceight Of Rugherse in then?\n",
            "\n",
            "LEONS:\n",
            "Mas, If hight:\n",
            "That this douths, but not that niging as you be him,\n",
            "Reffeeld wither prittles of thats of so.\n",
            "That you have the but their such to they seeps,\n",
            "Sir wolds a Romemead is rowndd, or to is name besten,\n",
            "Thell? in frieve yought King, Worrly, gented\n",
            "A'll I wict, andsself,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use token embeddings from OpenAI: Tiktoken"
      ],
      "metadata": {
        "id": "rlVRf4GL7YI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content\"\n",
        "\n",
        "with open(os.path.join(data_dir, \"input.txt\"), encoding=\"utf8\") as f:\n",
        "    text = f.read()\n",
        "train_text = text[:int(len(text)*0.90)]\n",
        "val_text = text[int(len(text)*0.90):]\n",
        "print(\"chars in train_data: \", len(train_text))\n",
        "print(\"chars in val_data: \", len(val_text))\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_data = enc.encode_ordinary(train_text)\n",
        "val_data = enc.encode_ordinary(val_text)\n",
        "\n",
        "print(\"tokens in train dataset: \", len(train_data))\n",
        "print(\"tokens in validaiton dataset: \", len(val_data))\n",
        "\n",
        "train_set = set(train_data)\n",
        "val_set = set(val_data)\n",
        "\n",
        "vocab_size = len(train_set.union(val_set))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:05:37.411251Z",
          "iopub.execute_input": "2023-07-02T08:05:37.411704Z",
          "iopub.status.idle": "2023-07-02T08:05:41.378052Z",
          "shell.execute_reply.started": "2023-07-02T08:05:37.411668Z",
          "shell.execute_reply": "2023-07-02T08:05:41.377013Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BE5g-zP7YI4",
        "outputId": "a808f270-15ef-4f79-94ae-89d55bcb30db"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chars in train_data:  1003854\n",
            "chars in val_data:  111540\n",
            "tokens in train dataset:  301966\n",
            "tokens in validaiton dataset:  36059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = np.array(train_data), np.array(val_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:05:41.379530Z",
          "iopub.execute_input": "2023-07-02T08:05:41.380142Z",
          "iopub.status.idle": "2023-07-02T08:05:41.431745Z",
          "shell.execute_reply.started": "2023-07-02T08:05:41.380107Z",
          "shell.execute_reply": "2023-07-02T08:05:41.430498Z"
        },
        "trusted": true,
        "id": "wXhQcXss7YI4"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = max(train_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:05:41.433307Z",
          "iopub.execute_input": "2023-07-02T08:05:41.433725Z",
          "iopub.status.idle": "2023-07-02T08:05:41.485552Z",
          "shell.execute_reply.started": "2023-07-02T08:05:41.433692Z",
          "shell.execute_reply": "2023-07-02T08:05:41.484562Z"
        },
        "trusted": true,
        "id": "715qHKXR7YI4"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:05:41.487232Z",
          "iopub.execute_input": "2023-07-02T08:05:41.487562Z",
          "iopub.status.idle": "2023-07-02T08:05:41.499995Z",
          "shell.execute_reply.started": "2023-07-02T08:05:41.487532Z",
          "shell.execute_reply": "2023-07-02T08:05:41.499079Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIx3545s7YI4",
        "outputId": "ab5b23d2-0773-4155-8db4-ce3b4addfc89"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50255"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, embed_dim, drop_p):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:05:41.501575Z",
          "iopub.execute_input": "2023-07-02T08:05:41.502084Z",
          "iopub.status.idle": "2023-07-02T08:05:41.509778Z",
          "shell.execute_reply.started": "2023-07-02T08:05:41.502047Z",
          "shell.execute_reply": "2023-07-02T08:05:41.508829Z"
        },
        "trusted": true,
        "id": "QBrZ2MMQ7YI4"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=vocab_size + 1, sequence_length=32, embed_dim=64, n_blocks=4, n_heads=4, drop_p=0.2).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:05:41.511068Z",
          "iopub.execute_input": "2023-07-02T08:05:41.511625Z",
          "iopub.status.idle": "2023-07-02T08:05:41.599151Z",
          "shell.execute_reply.started": "2023-07-02T08:05:41.511592Z",
          "shell.execute_reply": "2023-07-02T08:05:41.598185Z"
        },
        "trusted": true,
        "id": "86bc_QGC7YI4"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:05:41.600679Z",
          "iopub.execute_input": "2023-07-02T08:05:41.601291Z",
          "iopub.status.idle": "2023-07-02T08:07:24.076049Z",
          "shell.execute_reply.started": "2023-07-02T08:05:41.601257Z",
          "shell.execute_reply": "2023-07-02T08:07:24.075085Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6mVtQHT7YI4",
        "outputId": "cd17725c-aee4-4491-db6d-f311e6bdd119"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0 loss:  11.005958557128906\n",
            "Iteration:  100 loss:  6.3544135093688965\n",
            "Iteration:  200 loss:  6.3163628578186035\n",
            "Iteration:  300 loss:  6.000838756561279\n",
            "Iteration:  400 loss:  5.658395767211914\n",
            "Iteration:  500 loss:  5.3386054039001465\n",
            "Iteration:  600 loss:  5.237395286560059\n",
            "Iteration:  700 loss:  4.751059055328369\n",
            "Iteration:  800 loss:  4.904850959777832\n",
            "Iteration:  900 loss:  4.998269081115723\n",
            "Iteration:  1000 loss:  4.507585525512695\n",
            "Iteration:  1100 loss:  4.641820907592773\n",
            "Iteration:  1200 loss:  4.9189043045043945\n",
            "Iteration:  1300 loss:  4.089478969573975\n",
            "Iteration:  1400 loss:  4.2778425216674805\n",
            "Iteration:  1500 loss:  4.599847793579102\n",
            "Iteration:  1600 loss:  4.3327741622924805\n",
            "Iteration:  1700 loss:  4.126469135284424\n",
            "Iteration:  1800 loss:  4.284380912780762\n",
            "Iteration:  1900 loss:  4.159511089324951\n",
            "Iteration:  2000 loss:  4.120541572570801\n",
            "Iteration:  2100 loss:  4.5922322273254395\n",
            "Iteration:  2200 loss:  4.311765193939209\n",
            "Iteration:  2300 loss:  4.060207366943359\n",
            "Iteration:  2400 loss:  4.430873870849609\n",
            "Iteration:  2500 loss:  4.124011993408203\n",
            "Iteration:  2600 loss:  4.362270832061768\n",
            "Iteration:  2700 loss:  4.212732791900635\n",
            "Iteration:  2800 loss:  4.158505916595459\n",
            "Iteration:  2900 loss:  4.143306732177734\n",
            "Iteration:  3000 loss:  3.891831874847412\n",
            "Iteration:  3100 loss:  4.348986625671387\n",
            "Iteration:  3200 loss:  3.9691357612609863\n",
            "Iteration:  3300 loss:  4.05063533782959\n",
            "Iteration:  3400 loss:  4.644815444946289\n",
            "Iteration:  3500 loss:  4.209781646728516\n",
            "Iteration:  3600 loss:  3.85026216506958\n",
            "Iteration:  3700 loss:  3.922175168991089\n",
            "Iteration:  3800 loss:  4.081613063812256\n",
            "Iteration:  3900 loss:  4.1871209144592285\n",
            "Iteration:  4000 loss:  3.4790396690368652\n",
            "Iteration:  4100 loss:  3.929513454437256\n",
            "Iteration:  4200 loss:  3.640573740005493\n",
            "Iteration:  4300 loss:  3.975574016571045\n",
            "Iteration:  4400 loss:  4.175961494445801\n",
            "Iteration:  4500 loss:  3.7911431789398193\n",
            "Iteration:  4600 loss:  4.37492036819458\n",
            "Iteration:  4700 loss:  3.9300360679626465\n",
            "Iteration:  4800 loss:  4.039916038513184\n",
            "Iteration:  4900 loss:  3.820528984069824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "sequence_length=32\n",
        "with torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate\n",
        "    start_char = \"\\n\"\n",
        "    start_token = torch.tensor(encode(start_char)).unsqueeze(0)\n",
        "    print(\"start_token shape: \", start_token.shape)\n",
        "    current_token = start_token.to(device) # (B, T)\n",
        "    generated_tokens = []\n",
        "    for x in range(1000):\n",
        "        idx = current_token[:, -sequence_length:]\n",
        "        logits = model(idx) # (B, T, C)\n",
        "        logits = logits[:, -1, :] # (B, C)\n",
        "        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n",
        "        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n",
        "        generated_tokens.append(next_token.item())\n",
        "        current_token = torch.cat((current_token, next_token), dim=1).to(device) # (B, T+1)\n",
        "    print(enc.decode(generated_tokens))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T08:07:24.077781Z",
          "iopub.execute_input": "2023-07-02T08:07:24.078164Z",
          "iopub.status.idle": "2023-07-02T08:07:29.686302Z",
          "shell.execute_reply.started": "2023-07-02T08:07:24.078133Z",
          "shell.execute_reply": "2023-07-02T08:07:29.685322Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP7lFu6T7YI4",
        "outputId": "28901de8-7301-4640-f4cf-80f6950d5bad"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_token shape:  torch.Size([1, 1])\n",
            " ho! What, go?\n",
            "\n",
            "CAPULET:\n",
            "Hang, at with all my lord of Hark;\n",
            "The pleasure breathe the love\n",
            "Thy school treading tears good Friends souls, so.\n",
            "\n",
            "First Citizen:\n",
            "Now, fair lady!\n",
            "Torily, and thy brother's life,\n",
            "That ever tongue that is full of my tent.\n",
            "\n",
            "First Servingman:\n",
            "By heaven with so.\n",
            "You know thy mother isle, if that may command so well: away,\n",
            "And undertake at the corse-cfold deathsimely,\n",
            "Or EthiopianTill to-off strokes: but I should kill\n",
            "GRE heard the gods, whose pardon\n",
            "Her knees, faith brokens with all the shade\n",
            "To cast the war revolar Laurence Tarpe: at cause\n",
            "There is the Capition.\n",
            "\n",
            "First Musician:\n",
            "Yet in this? and you may speak the king too dishonour away sin\n",
            "But es heart of man of our king's love\n",
            "Where: on her, do you deny no power.\n",
            "\n",
            "FROTH:\n",
            "She knew you treat: it be so much\n",
            "Two too much, as would show'd you give out.\n",
            "\n",
            "YORK:\n",
            "I have two, let the sacrament; and go so learned\n",
            "Of bread allied I make thy royal spring,\n",
            "As cries of that he has, I am cha our true drop\n",
            "The duke that reign great suspicion for the state.\n",
            "\n",
            "POLIXENES:\n",
            "Considerress!\n",
            "\n",
            "LUCIO:\n",
            "Then say comes the same man not them\n",
            "To win me all the veryful of lead-hearted part this\n",
            "one is coldwell of hearts from the king,\n",
            "In chron of thy present own friends, lewdors lament dignities of he:\n",
            "This is the poordone nobles and hear:\n",
            "And h irreest thou say\n",
            "They a little lunacy the chairs of me yet you\n",
            "Where home keep, Jamesench, as a thanks?\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "This is your nurse that holds my most contr broad;\n",
            "And have, as, honour of then whetted\n",
            "Wherewith the noble Lord Northumberland?\n",
            "\n",
            "DUKE OF SALISTRANIO:\n",
            "If seem my lord. Iniquowickily,\n",
            "Def'st thou shalt seize his old bench;\n",
            "For I might wish you left thee to sound, to forward no\n",
            "they Marcius with my happiness.\n",
            "But by him is he that came Caius.\n",
            "Ay, thou Hest not wed'd-- Your shadows's with youried;\n",
            "And with a bolted of this horses,\n",
            "Or, two appear fares your madness'st\n",
            "Will I be patient RICHARD CAPULET:\n",
            "Bid thy sovereign.\n",
            "\n",
            "PAULINA:\n",
            "I see but't, sea, honest,\n",
            "The kiss your dog-viols of a servant must-delate patience,\n",
            "And interchange to pray you: my lord,\n",
            "Whose sovereign, by his tied it as I am bound with Angelo.\n",
            "\n",
            "MENENIUS:\n",
            "When shall ne'Tis but Cain. Sw her hence thy sepulchre\n",
            "But the king o'er ere I have enrich'd't.\n",
            "\n",
            "GLOUCESTER:\n",
            "But I am for all for you: no;\n",
            "I think it care to say you, he I carry.\n",
            "\n",
            "LEONTES:\n",
            "Why, sweet honour an infected better suits, is\n",
            "Is against his own honour before your English five,\n",
            "Of my poor fellow's arguing hence,\n",
            "And I shall prove, I find their purpose:\n",
            "Besides on my father's house made laid,\n",
            "Without these many virtues is a fearful and honour as\n",
            "For all, till it seems as grieved swears.\n",
            "\n",
            "KING RICHARD II:\n",
            "And I might behold I cannot speak:\n",
            "I speak join'd up my sister hath subtle;\n",
            "That long were look'd, slight cheque the elderonawife;\n",
            "But to hold of Rome to\n",
            "Did we make King Edward thy or giant in vain.\n",
            "\n",
            "KING RICHARD III:\n",
            "That'st, I am heading is that Warwick.\n",
            "\n",
            "VOLUMIO:\n",
            "You wait Barnardine! Edward! yes, if you have deserved all\n",
            "When thou vain flourish, that gives love;\n",
            "For I yet cut up the teaching,\n",
            "Of the park picking of you.\n",
            "\n",
            "ROMEO:\n",
            "No, I come for't.\n",
            "Withdraw of my death, see thy base lord,\n",
            "I am so rich more, which successes:\n",
            "I would enjoy up it were sin lies,\n",
            "By a case and new bride pass,\n",
            "I should make't pity, still so, must I died the canopy.\n",
            "He was thy coal of our foes\n",
            "Of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Makes more meaningful sentences"
      ],
      "metadata": {
        "id": "pl-M400DIvaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train on a subset of OpenWebText"
      ],
      "metadata": {
        "id": "dkyTeEjc7YI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:01:29.719797Z",
          "iopub.execute_input": "2023-07-01T05:01:29.720211Z",
          "iopub.status.idle": "2023-07-01T05:01:42.212255Z",
          "shell.execute_reply.started": "2023-07-01T05:01:29.720180Z",
          "shell.execute_reply": "2023-07-01T05:01:42.211072Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV91hKw07YI4",
        "outputId": "40fcd929-433c-4275-a283-a357e992ecfc"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import os\n",
        "from torch import nn"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:01:42.215560Z",
          "iopub.execute_input": "2023-07-01T05:01:42.215982Z",
          "iopub.status.idle": "2023-07-01T05:01:43.877563Z",
          "shell.execute_reply.started": "2023-07-01T05:01:42.215940Z",
          "shell.execute_reply": "2023-07-01T05:01:43.876644Z"
        },
        "trusted": true,
        "id": "ZI0Japd37YI4"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:01:43.879034Z",
          "iopub.execute_input": "2023-07-01T05:01:43.879638Z",
          "iopub.status.idle": "2023-07-01T05:01:43.912704Z",
          "shell.execute_reply.started": "2023-07-01T05:01:43.879593Z",
          "shell.execute_reply": "2023-07-01T05:01:43.911716Z"
        },
        "trusted": true,
        "id": "tonDf4CC7YI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, head_size, drop_p):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.k = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.v = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(sequence_length, sequence_length)))\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "        self.head_size = head_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "        query = self.q(x)\n",
        "        key = self.k(x)\n",
        "        value = self.v(x)\n",
        "#         key.permute(0, -1, -2)\n",
        "        attention_map = query @ key.transpose(-2, -1) * self.head_size**-0.5 # (B, T, C) @ (B, C, T) = (B, T, T)\n",
        "        masked_attention_map = attention_map.masked_fill(self.tril[:T, :T] == 0, -np.Inf)\n",
        "        masked_attention_map = F.softmax(masked_attention_map, dim=-1)\n",
        "        attention_map = self.dropout(masked_attention_map)\n",
        "        feature_map = attention_map @ value # (B, T, T) @ (B, T, C) = (B, T, C)\n",
        "\n",
        "        return feature_map\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:01:43.915864Z",
          "iopub.execute_input": "2023-07-01T05:01:43.916216Z",
          "iopub.status.idle": "2023-07-01T05:01:43.926163Z",
          "shell.execute_reply.started": "2023-07-01T05:01:43.916190Z",
          "shell.execute_reply": "2023-07-01T05:01:43.925210Z"
        },
        "trusted": true,
        "id": "50BSACIB7YI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // n_heads\n",
        "        self.heads = nn.ModuleList([SelfAttention(sequence_length, embed_dim, head_size, drop_p) for x in range(n_heads)])\n",
        "        self.mlp = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.mlp(out)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:01:43.927605Z",
          "iopub.execute_input": "2023-07-01T05:01:43.928132Z",
          "iopub.status.idle": "2023-07-01T05:01:43.938361Z",
          "shell.execute_reply.started": "2023-07-01T05:01:43.928093Z",
          "shell.execute_reply": "2023-07-01T05:01:43.937485Z"
        },
        "trusted": true,
        "id": "13OQz1jT7YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, embed_dim, drop_p):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, 4 * embed_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(4 * embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "#         x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:01:43.939876Z",
          "iopub.execute_input": "2023-07-01T05:01:43.940227Z",
          "iopub.status.idle": "2023-07-01T05:01:43.948742Z",
          "shell.execute_reply.started": "2023-07-01T05:01:43.940196Z",
          "shell.execute_reply": "2023-07-01T05:01:43.947814Z"
        },
        "trusted": true,
        "id": "-MuifGfb7YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.multi_head_self_attention = MultiHeadSelfAttention(sequence_length, embed_dim, n_heads, drop_p)\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.multi_head_self_attention(self.layer_norm(x))\n",
        "        x = x + self.mlp(self.layer_norm(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:01:43.950324Z",
          "iopub.execute_input": "2023-07-01T05:01:43.950721Z",
          "iopub.status.idle": "2023-07-01T05:01:43.960951Z",
          "shell.execute_reply.started": "2023-07-01T05:01:43.950689Z",
          "shell.execute_reply": "2023-07-01T05:01:43.960154Z"
        },
        "trusted": true,
        "id": "C6t2Mlv97YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, sequence_length, embed_dim, n_blocks, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.token_embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = torch.nn.Embedding(sequence_length, embed_dim)\n",
        "        self.encoder_blocks = nn.Sequential(*([EncoderBlock(sequence_length, embed_dim, n_heads, drop_p) for x in range(n_blocks)]))\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.linear_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Attributes\n",
        "        __________________\n",
        "        x.shape: (B, T)\n",
        "\n",
        "        Returns\n",
        "        _________________\n",
        "        logits.shape: (B, T, C), where C= vocab size\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "\n",
        "        token_embeddings = self.token_embedding(x) # (B, T, C)\n",
        "        positional_embeddings = self.pos_embedding(torch.arange(T, device=device)) # (T, C)\n",
        "        embeddings = token_embeddings + positional_embeddings # B, T, C cause broadcasting\n",
        "        feature_maps = self.encoder_blocks(embeddings)\n",
        "        x = self.layer_norm(feature_maps)\n",
        "        x = self.linear_head(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:01:43.962444Z",
          "iopub.execute_input": "2023-07-01T05:01:43.962769Z",
          "iopub.status.idle": "2023-07-01T05:01:43.973047Z",
          "shell.execute_reply.started": "2023-07-01T05:01:43.962739Z",
          "shell.execute_reply": "2023-07-01T05:01:43.971922Z"
        },
        "trusted": true,
        "id": "kU85ibUs7YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/kaggle/input/openwebtext-subset-20\"\n",
        "\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "vocab_size = max(train_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:01:43.975107Z",
          "iopub.execute_input": "2023-07-01T05:01:43.975578Z",
          "iopub.status.idle": "2023-07-01T05:06:53.983917Z",
          "shell.execute_reply.started": "2023-07-01T05:01:43.975547Z",
          "shell.execute_reply": "2023-07-01T05:06:53.982637Z"
        },
        "trusted": true,
        "id": "1MyLlD-_7YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=vocab_size + 1, sequence_length=512, embed_dim=768, n_blocks=6, n_heads=6, drop_p=0.1).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:06:53.988600Z",
          "iopub.execute_input": "2023-07-01T05:06:53.989023Z",
          "iopub.status.idle": "2023-07-01T05:06:58.080895Z",
          "shell.execute_reply.started": "2023-07-01T05:06:53.988966Z",
          "shell.execute_reply": "2023-07-01T05:06:58.079872Z"
        },
        "trusted": true,
        "id": "xxWWn6eo7YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.join(data_dir, \"best_checkpoint.pth\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:08:01.459957Z",
          "iopub.execute_input": "2023-07-01T05:08:01.460356Z",
          "iopub.status.idle": "2023-07-01T05:08:01.470266Z",
          "shell.execute_reply.started": "2023-07-01T05:08:01.460326Z",
          "shell.execute_reply": "2023-07-01T05:08:01.469338Z"
        },
        "trusted": true,
        "id": "HZTTLqDo7YI5",
        "outputId": "738b4e4d-2523-499e-8372-19f50c7f7a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/kaggle/input/openwebtext-subset-20/best_checkpoint.pth'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(os.path.join(data_dir, \"best_checkpoint.pth\"))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:07:40.580375Z",
          "iopub.execute_input": "2023-07-01T05:07:40.580814Z",
          "iopub.status.idle": "2023-07-01T05:07:40.635174Z",
          "shell.execute_reply.started": "2023-07-01T05:07:40.580778Z",
          "shell.execute_reply": "2023-07-01T05:07:40.633990Z"
        },
        "trusted": true,
        "id": "t5fbEwMj7YI5",
        "outputId": "6d65f2a9-27e8-4974-9a04-a7a79b06b51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_checkpoint.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1994\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copies parameters and buffers from :attr:`state_dict` into\u001b[39;00m\n\u001b[1;32m   1972\u001b[0m \u001b[38;5;124;03mthis module and its descendants. If :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[1;32m   1973\u001b[0m \u001b[38;5;124;03mthe keys of :attr:`state_dict` must exactly match the keys returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 1994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(state_dict)))\n\u001b[1;32m   1996\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1997\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'str'>."
          ],
          "ename": "TypeError",
          "evalue": "Expected state_dict to be dict-like, got <class 'str'>.",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:08:07.402437Z",
          "iopub.execute_input": "2023-07-01T05:08:07.402809Z",
          "iopub.status.idle": "2023-07-01T05:08:07.408889Z",
          "shell.execute_reply.started": "2023-07-01T05:08:07.402777Z",
          "shell.execute_reply": "2023-07-01T05:08:07.407963Z"
        },
        "trusted": true,
        "id": "YkW9eKeV7YI5",
        "outputId": "0baa4a5c-4fbb-4166-d554-54a32bbd606b"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(12885552,)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split, sequence_length, batch_size):\n",
        "    if split == \"train\":\n",
        "        dataset = train_data\n",
        "    else:\n",
        "        dataset = val_data\n",
        "    random_numbers = torch.randint(0, len(dataset) - sequence_length, (batch_size,))\n",
        "    data = torch.stack([torch.from_numpy(dataset[random_number: random_number + sequence_length].astype(np.int64)) for random_number in random_numbers])\n",
        "    labels = torch.stack([torch.from_numpy(dataset[random_number + 1: random_number + sequence_length + 1].astype(np.int64)) for random_number in random_numbers])\n",
        "    return data, labels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:08:09.814298Z",
          "iopub.execute_input": "2023-07-01T05:08:09.814761Z",
          "iopub.status.idle": "2023-07-01T05:08:09.825513Z",
          "shell.execute_reply.started": "2023-07-01T05:08:09.814722Z",
          "shell.execute_reply": "2023-07-01T05:08:09.824521Z"
        },
        "trusted": true,
        "id": "qFl5tZ5j7YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 100\n",
        "def evaluate():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            data, labels = get_batch(split, 512, 16)\n",
        "            data = data.to(device)\n",
        "            labels = data.to(device)\n",
        "            logits = model(data)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            labels = labels.view(B*T)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:08:12.152297Z",
          "iopub.execute_input": "2023-07-01T05:08:12.152663Z",
          "iopub.status.idle": "2023-07-01T05:08:12.160211Z",
          "shell.execute_reply.started": "2023-07-01T05:08:12.152630Z",
          "shell.execute_reply": "2023-07-01T05:08:12.159040Z"
        },
        "trusted": true,
        "id": "05BTfxZc7YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    min_val_loss = np.Inf\n",
        "    for iteration in range(700000):\n",
        "        if iteration % 500 == 0:\n",
        "            losses = evaluate()\n",
        "            print(f\"step {iteration}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "            if losses[\"val\"] < min_val_loss:\n",
        "                min_val_loss = losses[\"val\"]\n",
        "                torch.save(model.state_dict(), os.path.join(\"/kaggle/working\", f\"best_checkpoint.pth\"))\n",
        "        data, labels = get_batch(\"train\", 512, 16)\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(data)\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        labels = labels.view(B*T)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:08:12.576609Z",
          "iopub.execute_input": "2023-07-01T05:08:12.576965Z",
          "iopub.status.idle": "2023-07-01T05:08:12.584774Z",
          "shell.execute_reply.started": "2023-07-01T05:08:12.576937Z",
          "shell.execute_reply": "2023-07-01T05:08:12.583778Z"
        },
        "trusted": true,
        "id": "yFYjIjwd7YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T05:08:13.031515Z",
          "iopub.execute_input": "2023-07-01T05:08:13.032241Z",
          "iopub.status.idle": "2023-07-01T15:56:08.745687Z",
          "shell.execute_reply.started": "2023-07-01T05:08:13.032194Z",
          "shell.execute_reply": "2023-07-01T15:56:08.744368Z"
        },
        "trusted": true,
        "id": "UQT1-ZJb7YI5",
        "outputId": "4ccecd23-7bf4-49fc-931f-58e5f8969ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "step 0: train loss 11.1205, val loss 11.1206\nstep 500: train loss 5.5682, val loss 5.6956\nstep 1000: train loss 5.8837, val loss 5.6302\nstep 1500: train loss 5.6791, val loss 5.7529\nstep 2000: train loss 5.7770, val loss 5.7086\nstep 2500: train loss 5.9406, val loss 5.9721\nstep 3000: train loss 5.7866, val loss 5.9718\nstep 3500: train loss 5.9370, val loss 5.9669\nstep 4000: train loss 6.0417, val loss 5.9601\nstep 4500: train loss 6.1591, val loss 6.0797\nstep 5000: train loss 6.0856, val loss 6.0366\nstep 5500: train loss 5.9664, val loss 6.0845\nstep 6000: train loss 6.0808, val loss 6.3655\nstep 6500: train loss 6.0679, val loss 6.1373\nstep 7000: train loss 6.0773, val loss 6.1683\nstep 7500: train loss 6.1455, val loss 6.1742\nstep 8000: train loss 6.0446, val loss 6.1076\nstep 8500: train loss 6.4296, val loss 6.1427\nstep 9000: train loss 6.1211, val loss 6.4866\nstep 9500: train loss 6.4034, val loss 6.2373\nstep 10000: train loss 6.4121, val loss 6.4991\nstep 10500: train loss 6.1764, val loss 6.5570\nstep 11000: train loss 6.3700, val loss 6.3286\nstep 11500: train loss 6.4545, val loss 6.3639\nstep 12000: train loss 6.4398, val loss 6.5670\nstep 12500: train loss 6.4841, val loss 6.4767\nstep 13000: train loss 6.4913, val loss 6.2875\nstep 13500: train loss 6.4920, val loss 6.5710\nstep 14000: train loss 6.3974, val loss 6.4338\nstep 14500: train loss 6.5943, val loss 6.3983\nstep 15000: train loss 6.4459, val loss 6.6690\nstep 15500: train loss 6.6768, val loss 6.2907\nstep 16000: train loss 6.3828, val loss 6.5992\nstep 16500: train loss 6.4810, val loss 6.4342\nstep 17000: train loss 6.4051, val loss 6.5527\nstep 17500: train loss 6.7868, val loss 6.5877\nstep 18000: train loss 6.5740, val loss 6.4397\nstep 18500: train loss 6.3169, val loss 6.3720\nstep 19000: train loss 6.5429, val loss 6.4375\nstep 19500: train loss 6.5489, val loss 6.5588\nstep 20000: train loss 6.7413, val loss 6.3575\nstep 20500: train loss 6.5188, val loss 6.4423\nstep 21000: train loss 6.4790, val loss 6.6569\nstep 21500: train loss 6.4286, val loss 6.6849\nstep 22000: train loss 6.5780, val loss 6.7038\nstep 22500: train loss 6.6107, val loss 6.5476\nstep 23000: train loss 6.5164, val loss 6.5398\nstep 23500: train loss 6.3558, val loss 6.4462\nstep 24000: train loss 6.5160, val loss 6.6539\nstep 24500: train loss 6.5674, val loss 6.7258\nstep 25000: train loss 6.5717, val loss 6.4969\nstep 25500: train loss 6.6170, val loss 6.5999\nstep 26000: train loss 6.5648, val loss 6.8594\nstep 26500: train loss 6.4947, val loss 6.6411\nstep 27000: train loss 6.7433, val loss 6.6552\nstep 27500: train loss 6.7092, val loss 6.5918\nstep 28000: train loss 6.5999, val loss 6.6540\nstep 28500: train loss 6.5533, val loss 6.7075\nstep 29000: train loss 6.5029, val loss 6.6908\nstep 29500: train loss 6.6680, val loss 6.5573\nstep 30000: train loss 6.7218, val loss 6.6501\nstep 30500: train loss 6.7028, val loss 6.4009\nstep 31000: train loss 6.5150, val loss 6.4720\nstep 31500: train loss 6.7446, val loss 6.4466\nstep 32000: train loss 6.4963, val loss 6.5445\nstep 32500: train loss 6.4879, val loss 6.5360\nstep 33000: train loss 6.7114, val loss 6.6882\nstep 33500: train loss 6.6246, val loss 6.7101\nstep 34000: train loss 6.6162, val loss 6.6953\nstep 34500: train loss 6.7844, val loss 6.7474\nstep 35000: train loss 6.8140, val loss 6.5698\nstep 35500: train loss 6.6491, val loss 6.6029\nstep 36000: train loss 6.6448, val loss 6.6241\nstep 36500: train loss 6.6383, val loss 6.4612\nstep 37000: train loss 6.6060, val loss 6.6481\nstep 37500: train loss 6.6730, val loss 6.6359\nstep 38000: train loss 6.7289, val loss 6.7214\nstep 38500: train loss 6.6134, val loss 6.7302\nstep 39000: train loss 6.7059, val loss 6.7744\nstep 39500: train loss 6.7824, val loss 6.6205\nstep 40000: train loss 6.7398, val loss 6.8232\nstep 40500: train loss 6.5742, val loss 6.6476\nstep 41000: train loss 6.5529, val loss 6.5688\nstep 41500: train loss 6.7225, val loss 6.6579\nstep 42000: train loss 6.6629, val loss 6.6912\nstep 42500: train loss 6.7417, val loss 6.7601\nstep 43000: train loss 6.6481, val loss 6.7588\nstep 43500: train loss 6.7580, val loss 6.5742\nstep 44000: train loss 6.7080, val loss 6.7364\nstep 44500: train loss 6.6401, val loss 6.6391\nstep 45000: train loss 6.5862, val loss 6.7942\nstep 45500: train loss 6.7585, val loss 6.8213\nstep 46000: train loss 6.8539, val loss 6.6330\nstep 46500: train loss 6.7505, val loss 6.6980\nstep 47000: train loss 6.6159, val loss 6.6612\nstep 47500: train loss 6.7725, val loss 6.6193\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m700000\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m min_val_loss:\n",
            "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT)\n\u001b[1;32m     15\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, labels)\n\u001b[0;32m---> 16\u001b[0m         losses[k] \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.training"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T15:56:18.232118Z",
          "iopub.execute_input": "2023-07-01T15:56:18.233150Z",
          "iopub.status.idle": "2023-07-01T15:56:18.241088Z",
          "shell.execute_reply.started": "2023-07-01T15:56:18.233107Z",
          "shell.execute_reply": "2023-07-01T15:56:18.239981Z"
        },
        "trusted": true,
        "id": "CP0xhyXp7YI5",
        "outputId": "c92f204b-b729-4408-f338-5bd00e6825b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "False"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T15:59:51.060802Z",
          "iopub.execute_input": "2023-07-01T15:59:51.061184Z",
          "iopub.status.idle": "2023-07-01T15:59:52.748871Z",
          "shell.execute_reply.started": "2023-07-01T15:59:51.061154Z",
          "shell.execute_reply": "2023-07-01T15:59:52.747873Z"
        },
        "trusted": true,
        "id": "p-AAtfTx7YI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "sequence_length=512\n",
        "with torch.no_grad(): # despite calling model.eval, it is a good idea to do torch.no_grad() to make sure no gradients are calculate\n",
        "    start_char = \"Once upon a time, there were a prince and a princess who loved eachother a lot, but\"\n",
        "    start_token = torch.tensor(enc.encode_ordinary(start_char)).unsqueeze(0)\n",
        "    print(\"start_token shape: \", start_token.shape)\n",
        "    current_token = start_token.to(device) # (B, T)\n",
        "    generated_tokens = []\n",
        "    for x in range(1000):\n",
        "        idx = current_token[:, -sequence_length:]\n",
        "        logits = model(idx) # (B, T, C)\n",
        "        logits = logits[:, -1, :] # (B, C)\n",
        "        preds = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n",
        "        next_token = torch.multinomial(preds, num_samples=1) # (B, 1)\n",
        "        generated_tokens.append(next_token.item())\n",
        "        current_token = torch.cat((current_token, next_token), dim=1).to(device) # (B, T+1)\n",
        "    print(enc.decode(generated_tokens))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T16:05:57.747730Z",
          "iopub.execute_input": "2023-07-01T16:05:57.748633Z",
          "iopub.status.idle": "2023-07-01T16:06:14.553926Z",
          "shell.execute_reply.started": "2023-07-01T16:05:57.748596Z",
          "shell.execute_reply": "2023-07-01T16:06:14.552278Z"
        },
        "trusted": true,
        "id": "NRWd4NXI7YI6",
        "outputId": "d777edee-9ab3-43ea-9f50-fe9473ae5d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "start_token shape:  torch.Size([1, 20])\n most of us couldn’t tell them A.) you were so much better?” Bloor said.\n\n“She emailed me with a shovel, and I could call my Queen” Bloor said. \"Being vice president of the Royal Family Air Brigade is perfect, and is my coach of the Royal Family Air Brigade and there are fantastic individuals in Charles. They are the primesocialist of the Navy who is a large industry right now.”\n\nGeorge forced Vietnam to escape dangerous pathogens in shipworm, Naroul, Prince Heinz, and the Marine Fisheries Service. Bloor was expecting what Trump encouraged them to report on what he called “murdily” articles defending “fake news.”\n\nA quick attack by the Navy and American vessels has proven what Trump has learned about as a base of sophisticated squid silk cocoons and has fuelled jobs, as an important symbol for helping the empire better. Trump has nothing to do with these wars. Cavesturas famously signed up to be prime minister in January.\n\nMcGillibrand,\n\n“Most Krhegan says get along, compartmentalise was coined to describe Asian-style Bing: Sarah Besick; Professor Rousse Pearson opens in paperback\n\n“It involves digging, staying somewhere,” said Michael Tapper, an odd critic of the courage of an of Trump’s reelection campaign. “It flows back and forth until people were dissatisfied, oppressive right now. Buying from an ivory tower like this?”\n\nThe president looked surprised at what he had called “She could say ‘Trust me, stay her.' (She ordered Trump to flee negotiations to enter office this year.) “Hit up how there can be a special relationship between America, an empathetic presence and civilisation,” said Michael Tapper, who even spoke earlier today in this week’s Atlantic.\n\nADVERTISEMENT\n\nThe president implied that he had helped avoid the background, and says Trump’s second wanted embracing “cannon cult residences.” Wall Street insiders, hardliners and trivial ones are rushed to the government. Trump has both scaled back toward submarines, compared to Cold War 160 years ago, but Trump also thinks China should have named one country, whether Iran was a egg egg or an alien? If there were any suggestions that might very much be had in a movie for Trump years incompetence could be truly repugnant.\n\nSome of the president anxiously profiled James Sutter in such a horrifying experience, which he found refreshing, high-stakes golfing alternative lessons learned. According to the president of Australia, African-American men tended to stick in the sweat not just in their shops but also in their living rooms if they were very awesome. The president made golf clothes or in the shoes before the rapid equeton and other procedures on a steel pedestal. But when he got a lousy golf course, he became a chicken helpermaker.\n\nTrump has struck a country that has historically been communed over the decades — women’s status as men would likely as many as 45, or so — with no threat. But the president always refused to uphold the sexist notions of masculinity or masculinity.\n\nWhile president-elect Trump may be the most ardent major geopolitical force behind Trump, with almost to learn about “whatever black guy is small.”\n\nLike Trump himself, Republicans have an uncanny knack for moral course because they describe women as politically powerful than they thinks they’ve produced. The driving culture used to be unpopular in 1953,-, and brought to America by the right-wing media.\n\nSome and other leftists tried to run for reelection but did so.\n\nDespite Trump’s certainly late-afternoon economic crisis, Alabama lawmakers won by nominating Republican Senate candidate Roy McCaul in a landslide victory in Arizona. Even young voters joined him in Ohio. In 2009, McCaul won Montana, the GOP candidate who tended to dominate Mississippi.\n\nPresident Trump often pushed economic skills. In the former United States, he heard a sound cocky joke used by Liberals rallying to find someone she knew of during the same time in the Democratic National Convention.\n\nFor all of us, these callouts have allowed us to marathon economic activity, and Trump is ambivalent about partisan incumbency. These aren’t the only themes Trump surely made — women’s political leaders seem to be seen musical femas — even than women, but Obama is a surest Republican.\n\nWorkston responded on a Saturday afternoon, pulling a rally on NBC backlit, flat blocks with bump-in cups and Too often a simple elective governing strategy Trump relayed was his win probability — which will show up just four points — but the equation is more difficult.\n\nIs it any coincidence that both Trump\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), os.path.join(\"/kaggle/working\", f\"last_checkpoint.pth\"))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T16:03:48.328244Z",
          "iopub.execute_input": "2023-07-01T16:03:48.328620Z",
          "iopub.status.idle": "2023-07-01T16:03:49.226902Z",
          "shell.execute_reply.started": "2023-07-01T16:03:48.328591Z",
          "shell.execute_reply": "2023-07-01T16:03:49.225919Z"
        },
        "trusted": true,
        "id": "Nq9WPqsD7YI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GESLG2EE7YI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxzdkVIS7YI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsgbLjxD7YI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ap4QXY9F7YI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KljEWEgM7YI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MQCDaDeQ7YI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GRixLa5Q7YI6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}